{
  "hash": "4bc77022aa5a4119ecd851cbb1fbd7ed",
  "result": {
    "markdown": "---\ntitle: \"Chapter : Resampling methods\"\n\nbibliography: references.bib\nexecute: \n  freeze: auto\noutput: \n  html_document:\n   toc: true\n   toc_float: true\n---\n\n\n# General introduction\n\n  While studying random variables, it is useful to know which statistical distribution they are following. Such knowledge allows to make inferences about the statistical population when only sub-samples are available. For instance, it is needed in order to calculate confidence intervals around an estimated statistic or to calculate p-values to test hypothesis. In other words, recognizing a statistical distribution of the sampled data is essential to estimate the reliability of estimations.  \n  Many well-studied statistical distributions can be useful in this situation, some of the most famous being the Normal distribution, the Poisson distribution or the Binomial distribution. Despite the diversity of studied distribution, sampled data distribution often differ, whether because they do not follow any studied statistical distribution or because too few data are available making it difficult to recognize any distribution.  \n  In this situation, resampling procedures become interesting. It is a non-parametric statistic which has many usages, such as calculating confidence intervals or estimating p-values. The principle of resampling is to \"draw samples from the observed data to draw certain conclusions about the population of interest\" [@sinharay_jackknife_2010]. This chapter will discuss three resampling techniques: the jackknife, the bootstrap and the permutation (with a focus on the mantel test). \n  \nBefore diving into the subject, let's load some packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'gridExtra' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(bootstrap)\n```\n:::\n\n\n\n# Confidence interval estimation: Jackknife and Bootstrap methods\n\nTo illustrate the following methods, we'll use the Iris RBase dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Original dataset\ndata <- iris\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nSuppose we're interested in determining the average width of the sepals in a wild population of Iris flowers. However, it's practically impossible to measure the sepals of every single flower in the population. Instead, we've sampled and measured a specific number of individuals in the field. To simplify, let's consider our Iris dataset as a representation of the complete wild population (which, in reality, is inaccessible) and select only a small number of individuals to mimic our field sampling.\n\n### Normally distributed sample\n\nLet's first imagine that we only sampled 100 individuals within our population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our real population\nirisPopulation <- iris$Sepal.Width\n\n# Randomly sample 100 individuals\nset.seed(42)\ndata_sample <- irisPopulation[sample(1:150,100)]\n\n# Histogram\nplot1 <- ggplot()+\n  geom_histogram(aes(x = data_sample),colour = \"black\", fill = \"white\", bins = 8)+\n  xlab(\"Sepal width\")+\n  labs(title = \"Sepal width distribution (n=100)\") +\n  theme_classic()\n\n# QQ-plot\nplot2 <- ggplot()+\n  stat_qq(aes(sample=data_sample))+\n  stat_qq_line(aes(sample=data_sample), color=\"red\")+\n  xlab(\"Normal theoretical quantiles\")+\n  ylab(\"Sub-sample quantiles\")+\n  labs(title = \" Sepal width QQ-plot (n=100)\") +\n  theme_classic()\n\n# Print\ngrid.arrange(plot1,plot2,ncol=2)\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-3-1.png){width=50%}\n:::\n:::\n\n\nThe sample being normally distributed, we can compute its mean ($\\mu$), as well as its confidence interval as below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute sample mean\ncat(\"mu = \",mean(data_sample),\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmu =  3.08 \n```\n:::\n\n```{.r .cell-code}\n# Compute confidence interval\nlow <- mean(data_sample)-1.96*sd(data_sample)/sqrt(length(data_sample))\nhigh <- mean(data_sample)+1.96*sd(data_sample)/sqrt(length(data_sample))\ncat(\"Lower bound : \",round(low,2), \"    Upper bound = \",round(high,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLower bound :  2.99     Upper bound =  3.17\n```\n:::\n:::\n\n> Note: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\nThen, we can easily compare this result with the actual mean of our wild population, that theoretically we do not know:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute population mean\ncat(\"mean = \",mean(irisPopulation))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmean =  3.057333\n```\n:::\n:::\n\n\nThe mean of our population falls within our confidence interval!\n\n### Non-normally distributed sample\n\nNow, let's consider that we were only able to sample 15 individuals within our wild population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Randomly sample 15 individuals\ndata_sample <- irisPopulation[sample(1:150,15)]\n\n# Histogram\nplot1 <- ggplot()+\n  geom_histogram(aes(x = data_sample),colour = \"black\", fill = \"white\", bins = 8)+\n  xlab(\"Sepal width\")+\n  labs(title = \" Sepal width distribution (n=15)\") +\n  theme_classic()\n\n# QQ-plot\nplot2 <- ggplot()+\n  stat_qq(aes(sample=data_sample))+\n  stat_qq_line(aes(sample=data_sample), color=\"red\")+\n  xlab(\"Normal theoretical quantiles\")+\n  ylab(\"Sub-sample quantiles\")+\n  labs(title = \" Sepal width QQ-plot (n=15)\") +\n  theme_classic()\n\n# Print\ngrid.arrange(plot1,plot2,ncol=2)\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-6-1.png){width=50%}\n:::\n:::\n\n\nThis time, our sample size is too small, and its distribution is no longer normal. In this very common scenario, what options do we have to estimate a confidence interval around any of our parameters of interest, like the mean? This is where resampling methods get interesting.\n\n## The Jackknife resampling method\n\nThis method was first proposed by @quenouille_approximate_1949. The name \"Jackknife\" comes from the fact that it is often referenced as a \"quick and dirty\" tool of statistics [@abdi_jackknife_2010]. It means that it is usable in many situations but it is often not the best tool. The technique allows us to estimate a confidence interval for some statistics when the dataset is too small and/or when it does not follow a known distribution.\n\n### Principle of the leave-one-out Jackknife [@sinharay_jackknife_2010 ; @petit_techniques_2022]\n\nThe main goal of the jackknife method is to calculate a confidence interval around a statistic when classical statistics can not apply to the data.The principle is to create subsamples of the initial sample by successively removing some of the observations. Then, on each subsample, a pseudo-value will be calculated. With all the pseudo-values, it is possible to calculate an estimator of the statistic and to estimate its confidence interval.\n\nThe most famous jackknife is the the leave-one-out jackknife (or order 1 jackknife) for which all the subsamples contain all the observations except one. Concretely, for a sample of $n$ observations, there will be $n$ subsamples of size $(n-1)$. The $i^{th}$ subsample will be composed of observations from $1$ to $n$ minus the $i^{th}$ observation.\n\nTo illustrate, here is our sample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2.8 2.8 3.2 3.2 2.3 3.0 2.9 3.4 2.8 3.4 2.4 3.0 3.2 3.9 2.5\n```\n:::\n:::\n\n\nAnd we can represent some subsamples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"1st subsample : \\n\",data_sample[-1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1st subsample : \n 2.8 3.2 3.2 2.3 3 2.9 3.4 2.8 3.4 2.4 3 3.2 3.9 2.5 \n```\n:::\n\n```{.r .cell-code}\ncat(\"4th subsample : \\n\",data_sample[-4], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4th subsample : \n 2.8 2.8 3.2 2.3 3 2.9 3.4 2.8 3.4 2.4 3 3.2 3.9 2.5 \n```\n:::\n:::\n\n\n### Calculation of pseudo-values\n\nAfter creating the subsamples, the next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:\n\n$$\nv_{i} = n\\overline{X} - (n-1)\\overline{X}_{-i}\n$$\n\nwith the following variables:\n\n|      Variable       | Meaning                                                                              |\n|:------------------:|----------------------------------------------------|\n|       $v_{i}$       | Pseudo value of the $i^{th}$ subsample                                               |\n|         $n$         | Total number of observations                                                         |\n|   $\\overline{X}$    | Mean of the initial sample                                                           |\n| $\\overline{X}_{-i}$ | Mean of the $i^{th}$ subsample, corresponding to all observations except the $i{th}$ |\n\nLet's write a function which create the subsamples and calculate their pseudo values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function for pseudo-values\npseudo_val <- function(data, theta){\n  # entry : data = the vector of data to which we want to apply the Jackknife\n  # entry : theta = function for the statistic of interest\n  # output : a vector of pseudo values for each subsample\n  n <- length(data)\n  mean_init <- theta(data)\n  pv <- rep(NA,n) #to keep in memory each pseudo value\n  for (i in 1:n) {\n    pv[i] <- n*mean_init - (n-1)*theta(data[-i])\n  }\n  return(pv)\n}\n```\n:::\n\n\nTo try the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function test\npv <- pseudo_val(data =  data_sample,\n                  theta = mean)\nprint(pv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2.8 2.8 3.2 3.2 2.3 3.0 2.9 3.4 2.8 3.4 2.4 3.0 3.2 3.9 2.5\n```\n:::\n:::\n\n\n### Statistical test\n\nThe jackknife estimator $\\theta$ of the mean will then be calculated as follow:\n\n$$\n\\begin{align}\n\\hat{\\theta} & = \\frac{\\sum_{i=1}^{n}v_{i}}{n} \\\\\n\\hat{\\theta} & = \\overline{v}\n\\end{align}\n$$\n\nIt corresponds to the mean of the pseudo values $v_i$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute mean of the pseudo-values\nmean_pv <- mean(pv)\nmean_pv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.986667\n```\n:::\n:::\n\n\nThe Jackknife estimator $\\theta$ obtained is 3.15. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:\n\n$$\n\\begin{align}\nSE_\\hat{X} & = \\sqrt{\\frac{\\sum_{i=1}^{n}(v_{i}-\\overline{v})}{n(n-1)}} \\\\\nSE_\\hat{X} & = \\sqrt{\\frac{\\sigma_{v}^{2}}{n}}\n\\end{align}\n$$\n\nWith $\\overline{v}$ being the mean of the pseudo values (and the jackknife estimator).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute standard error\nSE <- sqrt(var(pv)/length(pv))\nSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1086132\n```\n:::\n:::\n\n\n### Confidence interval\n\nFrom these we can calculate a confidence interval:\n\n$$\n[\\, \\overline{v} - 1.96 \\, SE_\\hat{X} ; \\overline{v} + 1.96 \\, SE_\\hat{X} \\,]\n$$\n\n> Note: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n Lower bound :  2.77 \n Higher bound :  3.2\n```\n:::\n:::\n\n\n\nThe estimated Jackknife mean is $3.15$ with the following confidence interval : $[2.94\\:;\\:3.35]$. The real value $\\mu = 3.05$ is captured within the bounds of the confidence interval which indicate the robustness of the estimation process.\n\n### R package \n\nOn r, functions already exist to automatically execute the Jackknife. For instance, in the package `bootstrap` [@tibshirani_bootstrap_2019], there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Package function\nbootstrap :: jackknife(x= data_sample,\n                theta = function(x) mean(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$jack.se\n[1] 0.1086132\n\n$jack.bias\n[1] 0\n\n$jack.values\n [1] 3.000000 3.000000 2.971429 2.971429 3.035714 2.985714 2.992857 2.957143\n [9] 3.000000 2.957143 3.028571 2.985714 2.971429 2.921429 3.021429\n\n$call\nbootstrap::jackknife(x = data_sample, theta = function(x) mean(x))\n```\n:::\n:::\n\n\nThe output of the function include the standard error (`$jack.se`) as describe above. It also include the bias (`$jack.bias`) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output `$jack.values` does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In our example, it correspond to the mean of each subsample.\n\n### Strengths of the method\n\nAs seen precedently, the Jackknife allows to calculate a confidence intervall when data are not following a normal distribution and to estimate the bias induced when only a sample of the statistical population is observed. These two characteristic does not only apply to univariate estimators such as the mean. It can be used on correlation coefficients and regression parameters for instance [@sinharay_jackknife_2010]. In that case, each subsample leave one observations out with all its associated variables. The jackknife method is peticularly interesting when it comes to make inferences about variance ratio. It can perform as good as the Fisher test if variables are normally distributed and better if they are not [@miller_jackknife--review_1974].  \nThe Jackknife method was a good tool in the last century as it was easy to apply manually but it is nowaday surpassed by other methods which emerged thanks to the evolution of computers.\n\n### Weaknesses of the method\n\nThis method is not always efficient, it is not recomanded for time series analysis where some time period are successively removed to create the subsample. Moreover it has little succes when it comes to the estimation of single order statistics (specific values in an ordered set of observations or data points) such as the median or the maximum [@miller_jackknife--review_1974]. But for this last point, other jackknife can be used such as the deleted-d jackknife which perform better for the median. In this method, subsamples are of size $(n-d)$ and there are $\\binom{n}{d}$ subsamples.  \n\nFor more informations about applications of the Jackknife you can read the paper: Jackknife a Review by Rupert G. Miller @miller_jackknife--review_1974.\n\n## The bootstrap re-sampling method\n\n### Introduction and Principle\n\nNow that we've thoroughly explored jackknife resampling in the previous section, let's delve into another resampling technique: the bootstrap method.\n\nSimilar to jackknife, bootstrap aims to estimate descriptive parameters of a sample and assess the accuracy of these estimates for making inferences about the actual population. It is yet another statistical inference method that involves generating multiple datasets through resampling from the original dataset. Essentially, the concept revolves around using resampling techniques to create a probability distribution for a chosen parameter. This distribution, known as the empirical distribution function, serves as an estimation of the true probability distribution of our parameter in the population. In practice, similar to the earlier section, our objective here is to compute parameters of interest from our sample (such as mean, median, or even the $R^2$ of a regression, among others) and establish confidence intervals around these estimates.\n\nThe fundamental difference between jackknife and bootstrap lies in their resampling methodologies. In each of its $i$ iterations, bootstrap randomly resamples $n$ elements from an initial sample of $n$ data points with replacement. This leads to two crucial distinctions from the resampling method employed in jackknife: i) The new samples generated through bootstrap maintain the same size $n$ as the original dataset. ii) Given the sampling with replacement approach, a particular element can occur multiple times in a new sample or might not appear at all during the resampling process.\n\nAfter generating these $i$ bootstrap samples, the intended statistical parameters are computed for each of these samples. As a result, we obtain a distribution of $i$ data points derived from these samples, forming what is known as our empirical distribution function. Subsequently, the analysis of this distribution allows us to estimate precision, particularly in establishing the confidence interval for our statistical parameters.\n\n### Practice and Application\n\nIf the previous introduction seemed a bit perplexing, don't worry---we'll use an example to illustrate this concept.\n\nFor this demonstration, we'll once again consider our non-normally distributed sampling of 15 individuals. Let's proceed with the calculation of the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Field sampling mean\nmean(data_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.986667\n```\n:::\n:::\n\n\nSo, we've obtained a mean value of 3.15, which is great. However, this mean value can't be reliably generalized to the entire population because we lack information about the variability induced by the specific individuals we chose to sample. As highlighted in our previous code, we only sampled $n=15$ individuals in the field. Clearly, this limited sampling isn't sufficient to confidently assert that our estimated mean is truly representative of the entire population. Hence, it becomes essential to gain insights into the variability of the mean we've just calculated. This is precisely where the bootstrap method comes into play.\n\nWe'll now systematically apply our algorithm to resample the initial dataset obtained from our field sampling. This resampling process will be utilized to construct the probability distribution of our mean.\n\n1.  **bootstrap resampling iterations**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix our number of bootstrap iterations\nB = 1000\n\n# Create an empty list to stock our resamplings\nbootstrapSamples <- vector(\"list\",B)\n\n# bootstrap resampling algorithm\nfor (i in 1:B) {\n  # Randomly sample n elements with replacement\n  bootstrapSamples[[i]] <- sample(data_sample, replace = TRUE)\n}\n```\n:::\n\n\nFor pedagogical purposes, let's examine a few resamples while comparing them to our initial field sampling dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our original field sampling\nsort(data_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2.3 2.4 2.5 2.8 2.8 2.8 2.9 3.0 3.0 3.2 3.2 3.2 3.4 3.4 3.9\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# A few resampling\nsort(bootstrapSamples[[5]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2.4 2.4 2.4 2.4 2.5 2.5 2.8 2.8 3.2 3.2 3.2 3.2 3.2 3.4 3.9\n```\n:::\n\n```{.r .cell-code}\nsort(bootstrapSamples[[777]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2.3 2.3 2.4 2.5 2.8 2.8 2.8 2.8 2.8 3.0 3.2 3.2 3.4 3.4 3.4\n```\n:::\n:::\n\n\nObserve how certain values are repeated more frequently than in our initial dataset. Additionally, note that some values aren't sampled at all. For instance, the value 2.5 appeared only once in our initial set and consequently was seldom or perhaps never resampled. Conversely, the value 2.8 was more prevalent in our initial dataset and consequently was sampled more frequently\n\n2.  **Compute mean for every resampled datasets**\n\nNow, we'll compute the parameter of interest for each of our resampled datasets. This process will yield our set of $i=1000$ mean values, which we'll utilize to construct our empirical distribution in the subsequent step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute mean for each bootstrap samples\niterationMeans <- sapply(bootstrapSamples, mean)\n```\n:::\n\n\n3.  **Plot the probability distribution**\n\nThe computation of the distribution for our parameter of interest is complete. We can now proceed to plot it, providing us with an understanding of the variability surrounding our estimated mean value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"black\", aes(y =after_stat(density))) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(xintercept = mean(data_sample), color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nHence, this probability distribution provides us with insights into the variability around the mean value estimated from our field sampling. As anticipated, and as indicated by the red dashed line, this distribution is centered around the estimated mean value.\n\n4.  **Compute Confidence Interval around our mean estimate**\n\nRecall the primary objective of the bootstrap method: to assess the precision of our parameter estimation to make inferences about the broader population. To accomplish this, we'll utilize the bootstrap percentiles. For instance, suppose we aim to calculate the 95% Confidence Interval. In this case, we'll allocate 5% of the error evenly on both sides of our distribution, corresponding to the values at the 2.5th and 97.5th percentiles in our empirical distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get percentile 2.5%\nquantile(iterationMeans, probs = c(0.025,0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%    97.5% \n2.793333 3.186833 \n```\n:::\n:::\n\n\nAnd thus, we've successfully obtained our confidence interval for the mean estimation: $2.97 \\leq \\overline{x} \\leq 3.33$.\n\nFor educational purposes, let's proceed to plot these percentiles in green.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"black\", aes(y =after_stat(density))) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(xintercept = mean(data_sample), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quantile(iterationMeans, probs = 0.025), color = \"darkolivegreen4\", linetype = \"dashed\", linewidth = 1.5) +\n  geom_vline(xintercept = quantile(iterationMeans, probs = 0.975), color = \"darkolivegreen4\", linetype = \"dashed\", linewidth = 1.5) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n5.  **Compare with population mean value**\n\nThe bootstrap resampling method has facilitated the construction of a confidence interval around our mean estimate with a 5% margin of error. Now, let's compare this interval with the actual value of our Iris population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(irisPopulation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.057333\n```\n:::\n:::\n\n\nThe actual sepal width mean of our population falls within the bounds of our confidence interval, which is highly encouraging! Comparing it to the interval estimated using the jackknife method, we notice that this interval is narrower and more precise. Consequently, we've effectively inferred information about the population mean in a scenario where we had limited data, made no normality assumptions, and where the standard computation of confidence intervals might not have been sufficiently robust and a\n\n### Strengths and Weaknesses\n\nAs we've seen, bootstrap is a powerful resampling technique used in statistics for estimating the precision of parameters. One of its primary advantages lies in its versatility across a wide spectrum of statistical estimations. Indeed, this resampling technique is applicable to various parameters, such as mean, variance, regression coefficients, and more. Furthermore, this method is particularly robust in handling complex data structure without strict distributional assumptions, meaning that it can easily works with non-normally distributed datasets with little observations, which is really relevant in real-word scenarios. In those cases, bootstrap ends up being more accurate than the standard intervals obtained under normality assumption.\n\nHowever, one significant concern is its computational intensity, especially noticeable with larger datasets or complex models. Generating numerous bootstrap samples can demand, in certain cases, substantial computing resources. Additionally, bootstrap might display biased estimation of the accuracy of estimates, depending on whether or not the sampling dataset is representative of the actual population.\n\nOverall, bootstrap's major advantages lie in its broader applicability across various statistical estimations, but which comes with higher computational demands. In contrast, Jackknife might be less versatile, but tends to offer a better computational efficiency and less biased parameters, making it a favorable choice under certain conditions.\n\n### To go further\n\nThis section on the bootstrap method offers a fundamental understanding, providing explicit R scripts for pedagogical purposes. If you find yourself needing to utilize bootstrap, we highly recommend exploring the capabilities of the precedently presented `bootstrap` R package, which streamlines bootstrap implementation. Moreover, the bootstrap method extends beyond basic parameter estimation, offering various derivatives and applications catering to diverse needs:\n\n-   **Parametric bootstrap:** Assumes a specific parametric distribution, generating bootstrap samples from this fitted distribution and allowing inference within a particular statistical model.\n-   **Smooth bootstrap:** Incorporates smoothing techniques to reduce variability in resampling, ideal for noisy or irregular data.\n-   **Bayesian bootstrap:** Generates bootstrap samples from posterior distributions, useful for uncertainty estimation in Bayesian analysis (see more in Bayesian chapter of this book).\n\nLastly, bootstrap resampling isn't limited to parameter estimation. It can also be employed to assess statistical tests, a topic we'll delve into in our upcoming section.\n",
    "supporting": [
      "r_chapter_boostrapping_resampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}