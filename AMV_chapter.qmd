---
title: "AMV_chapter"
format: html
editor: visual
---

Biologists' data, whether on individuals, populations or communities, are usually presented in the form of rectangular tables, with observations (n) in the rows and variables (p) in the columns.

The graphical representation of these n observations and p variables is easily achieved when there are only 2 or 3 variables (dimension). However, when the number of variables increases, the graphical representation becomes complicated, and Multivariate Analyses come into their own!

The aim of these analyses is to reduce the number of dimensions and examine the structure of the data by answering the following questions:

-   Which observations are similar?

-   Are there observations that stand out? Subgroups?

-   Which variables are correlated?

-   Are there particular links between certain observations and/or variables?

Multivariate analysis methods are therefore used to describe the data and generate hypotheses that can then be tested.

# PART 1: PCA

## Introduction :

PCA can be used to process a measurement table with :

-   In rows: the **n observations**.

-   In column: the **p quantitative variables**.

The 2 sets (observations and variables) are **totally distinct and non-interchangeable**. In other words, if you interchange the rows and columns (with the variables in the rows and the observations in the columns), the table no longer has the same meaning.

In PCA tables, the mean of a column has a meaning, while the mean of a row does not.

The aim of PCA is to achieve the best geometric representation of individuals and variables. To achieve this, we seek to reduce the dimension by finding the best projection plane (subspace) for "best" visualization of the point cloud in reduced space.

However, this reduction must :

-   Preserve distances between individuals:
    -   Two individuals who resemble each other must be close in the representation space.
-   Preserve correlations between variables:
    -   Two variables that are correlated must be represented by vectors forming an acute angle.
    -   Two independent variables are represented by orthogonal vectors.

## Mathematics

## Interpretation

Here's an example of how PCA can be applied. *dataset presentation* *package needed*

### a) Importing the dataset :

```{r}
# Loading the "iris" dataset available on R :
data("decathlon2") 
data=decathlon2[decathlon2$Competition=="OlympicG",c(1:10,12)]
summary(data)
```

In this data set, we have 14 individuals (athletes) on whom 10 quantitative variables (performances in different sports disciplines) have been recorded: - X100m - Long.jump - Shot.put - X400m - X110m.hurdle - Discus - Pole.vault - Javelin - X1500m

If NAs are present in the data. In order to carry out the PCA without any problems with NAs, they can be omitted from the analysis with the following command:

```{r}
# data=na.omit(object = data)
```

We want to find out whether particular links between certain observations and/or variables can be observed in this data set.

### b) Study of correlations :

Obtain the correlation matrix between variables using the **cor()** function:

```{r}
cor(data)
```

Variables are correlated if their value is greater than 0.9 (as a general rule). Here, we can see that no variables are correlated.

Here's another way of visualizing correlations between variables: (useful when you have a lot of variables, as here):

```{r}
abs(cor(data))>0.9
```

If TRUE (outside the diagonal), then both variables are correct. If two variables are correct, one must be removed. The choice of deleting one of the two correct variables is arbitrary and depends on the question being asked.

To remove a variable from the data set, use the following function: **data=data\[,-(column_of_the_variable_to_remove)}\]**

### c) Performing PCA :

To run the PCA, you need to load the following two packages: - **factoextra** - **FactoMineR**

Then, to perform the PCA on R, you can use the function : **PCA()**. Remember to store the result of this PCA in a new variable, so that you can easily retrieve the PCA information you need for subsequent interpretation.

```{r, echo=FALSE, hide=TRUE}
library(FactoMineR)
library(factoextra)
PCA=PCA(data)
```

### d) Interpretation of outputs :

#### 1) Inertia and choice of axes :

```{r}
# Recuperer les valeurs propres de chaque axes et l'inerties portees par les composantes principales
PCA$eig

# Visualisation graphique de l'inertie de chaque axe : 
fviz_eig(PCA, addlabels = TRUE)
```

To determine the number of axes used in the PCA analysis, we need to identify the jump in variance explained by the different axes. Here we can see that the first axis represents 43.2% of the variance, the second axis 22%, the third axis 14.1%, ... We can therefore see that the jump in variance explained by the different axes is between the first and second axes. The difference between the variance explained by the axes other than the first is negligible compared to the difference between the first axis and the others.

For the PCA interpretation, we therefore retain the first two axes, which together explain 65.19% of the variance.

#### 2) Interpretation of the biological meaning of the axes :

In PCA, the axes are interpreted according to the columns (\<=\> variables) via the correlation circle and the absolute contributions of the variables.

To obtain this information in R, use the following commands:

```{r}
# Obtention des contributions absolues des variables :
PCA$var$contrib

# Obtention du cercle des correlations :
fviz_pca_var(PCA)

# Representation graphique des valeurs de contributions absolues des variables pour un axe : 
# Cas axe 1 : 
fviz_contrib(PCA, choice = "var", axes = 1)

# Cas axe 2: 
fviz_contrib(PCA, choice = "var", axes = 2)
```

The absolute contribution shows how the initial variable contributes to the formation of the axis. To find out from these results which variables contribute to the formation of the synthetic axis, we use the threshold of \*1/number_of_variables)\*100\*\* (in general). For a variable to contribute, its contribution value must be greater than this threshold. This threshold value corresponds to the red dotted line on the graphs showing the absolute contributions of the variables along the axes.

From these results, we can see that axis 1 is mainly represented by the following variables: - Points - Discus - Shotput - Long.jump - X100m - High.jump

Axis 2 is represented by the variables: - X1500m - Pole.vault - X110m.hurdle - X100m

The correlation circle also shows that some variables are related and others independent. Two related variables are represented by vectors forming an acute angle. Two independent variables are represented by orthogonal vectors.

#### 3) Projection of individuals to create a typology based on the axes:

For this, we use the relative contributions of the individuals. These relative contributions can be used to create a typology of individuals based on the axes. The relative contribution of an individual corresponds to the share of information on the axis explained by the point and supported on the axis (in percentage).

To obtain the relative contributions of individuals on R, use the following commands:

```{r}
# Obtention des contributions relatives des individus :
PCA$ind$cos2

# Obtention de la projection des individus : 
fviz_pca_ind(PCA)



```

The individuals who most explain Axis 1 are : - Sebrle - Clay - Karpov - Schwarzl - Drews - Schoenbeck

Those who most explain axis 2 are : - Macey - Warners - Zsivoczky - Barras

### e) Biological conclusion :

Thus, thanks to the results of the absolute contributions of the variables and the relative contributions of the individuals, we can represent the projection of the individuals and variables as follows:

```{r}
fviz_pca_biplot(PCA)
```

Knowing the biological interpretation of the axes : *IMAGE*

We can therefore see that the individuals with the highest scores at this tournament are those who had the best results in the discus, shot put, long jump and high jump events. In addition, having good results in the 100m race doesn't give you a good ranking.

Thanks to PCA, we can see that the final score is more linked to the score of certain disciplines than others. And therefore, that links between certain variables and individuals can be observed.

# PART 2: CA

## Mathematiques

## Interpretation

This chapter is a simple example using R

You can import R package using the code

## Mathématiques

## Interprétation

# PART 2: MCA

## Mathématiques

## Interpretation

Let's have to look to fictive the data set we will be working on:

```{r}
library(tidyverse)
hippo <- read.table(file = "https://raw.githubusercontent.com/AnsaldiL/MODE_reproduciblescience/master/hp.csv", sep=';', header=TRUE)
hippo=hippo[,-1]
str(hippo)
```

Water consumption behaviour of Hippopotamus was observed in Penjari National Park, Benin. Drinking frequency was evaluated by a technician and rated "rarely" or "regularly". Sex is indicated with F for female and M for male. As there are 3 lakes in the park, the tree of them are written "G1", "G2" and "G3".

We are performing MCA with the package FactoMineR.

```{r, include=FALSE}
library(FactoMineR)
res.mca = MCA(hippo, quanti.sup=1)
```

Let's have a look to Eigen Values:

```{r}
res.mca$eig
```

The total variance of the data set is divided between four dimensions. The first dimension concentrate 32.6% of the total variance. The first plan gather 58% of the variance.

Here, you can see the results for the individuals, their coordinates ($coord$), contribution ($contrib$) and the quality of their projection ($cos2$ )

```{r}
head(res.mca$ind$coord)
head(res.mca$ind$cos2)
head(res.mca$ind$contrib)
```

Coordonates of the individuals are their position on the first plan. $cos2$ represents the quality of the representation of the individuals on the first plan. Contribution is how the point contribute to the creation of different axis.

You can access the same information for the variable (instead of the individuals): \>\>\>\>\>\>\> Lucile

```{r}
res.mca$var$coord
res.mca$var$contrib
res.mca$var$cos2
```

As a supplementary quantitative variable was added, MCA gives its coordinates with the following code:

```{r}
res.mca$quanti.sup$coord
```

Let's have a look to the plot of this MCA analysis:

```{r}
plot(res.mca)
```

As this is a factice data set, some individuals are overlapping. We will not focus on this artefact of the data set.

**How to interprete these graphs?**

***General description*** : The individuals are in black. The variables are in red. The percentage of variance of the first two axis is written on them. Individuals in the center of the cloud are individuals taking a mean value for all of their caracteristics, unlikely individuals far from the middle which are specific individuals, very different from others. Close individuals present close characteristics.

***Interpretation***: Here, we can see that the first axis separates individuals drinking regularly from individuals drinking rarely. The second axis separates individuals drinking at G2 from individuals drinking at G3. Male and female seamed to be separated by the first axis. As we can gather individuals sharing close properties, females seams to drink regularly and male more rarely. It is a bit less clear but male are drinking preferably in pound 1.

## Take Home Message

MCA is a statistical method adapted to table of type "individuals x quatitative variable". Eingen Values correspond to means of squered correlation ratios. It could be used a pre-processing before a classification or a coinertia analysis on tables with quantitative.

# PART 3: RDA

## Method principle

La République démocratique allemande, en abrégé RDA, est un État qui a existé de 1949 jusqu'à 1990. Elle a été fondée à l'issue de la Seconde Guerre mondiale (en réaction à la fondation quelques mois plus tôt de la République fédérale d'Allemagne sur les zones d'occupation américaine, anglaise et française) par les communistes allemands sur la zone d'occupation soviétique, partie de l'Allemagne occupée par l'Armée rouge.

La RDA était un État socialiste qui se comprenait comme un exemple de « socialisme réel » fondé sur l'idéologie marxiste-léniniste. Les structures politiques du pays (notamment le gouvernement et le parlement) suivent les directives du principal parti du pays, le SED. Suite à l'effondrement du Bloc de l'Est, la RDA disparaît et est intégrée à la République fédérale d'Allemagne, le 3 octobre 1990.

État européen de taille moyenne (108 333 km²), la RDA était divisée en 15 Bezirke (districts). Sa population est passée de 19 millions de personnes après la Seconde Guerre mondiale à 16 millions au moment de la réunification en 1990.

Two table analysis is used when the scientist possesses two data set with expecting the second to explain the first. The first one is called Y, the response variable and the second one is called X, the explanatory variable. For example :

| Table Y                                           | Table X                                                                                              |
|---------------------------------------------------|------------------------------------------------------------------------------------------------------|
| a data set with temperature data                  | a data set with cities caracteristics such as heigth of buildings, concreted area, number of cars... |
| a data set with the quantity of chemicals in soil | a data set with % of pesticide applied                                                               |
| a data set with functional traits of an animal    | a data set with the quantities of different food given to the animal                                 |

RDA is used when expecting a linear response from X to Y. Only the variables of X explaining Y would be kept. The canonical axis are a linear combinaison of the explicative variable (Y).

## Interprétation

This chapter is a simple example using R

You can import R package using the code

```{r}
library(tidyverse)
```

and then describe the purpose of your chapter as well as executing R command.

For example a basic summary of a dataset is given by

```{r}
df <- read.table("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv", sep = "," , header = TRUE)
```

and produce a graph

```{r}
df %>% ggplot() +
	aes(x=species, y = body_mass_g) +
	geom_boxplot()  
```

A citation @bauer2023writing

# PART 4: RLQ

## Mathématiques

## Interprétation

The use of RLQ analysis is important in ecology to integrate the traits of species with the environmental variables. So here, we don't have 2 tables (environment & specie) as RDA part but 3 tables:

-   environmental variables by sites (R)

-   abundance of species by sites (L)

-   trait values by species (Q)

To perform the RLQ, we need to decompose the analyse by three type of analyses already done in this chapter:

-   we will use a CA analyse on the abundance of species

-   we will use a MCA on the environmental table by taking the sites weight on the CA

-   we will use a MCA on the trait table by taking the species weight on the CA

Here, we use MCA for R and Q because our variables are factors but you can perform a PCA if your variables are quantitatives. Warning, for R and Q you have the obligation to weight by the L table (see below).

You can import R package using the code

```{r}
library(tidyverse)
library(ade4)
library(vegan)
library(ggplot2)
library(factoextra)
library(corrplot)
library(RVAideMemoire)
library(PerformanceAnalytics)
```

Here, we work with a dataset of "*ade4*" package

```{r}
#import the dataset
data(aviurba)

#create the three tables
summary(aviurba$mil)    #(R)
R<-aviurba$mil

summary(aviurba$fau)    #(L)
L<-aviurba$fau

summary(aviurba$traits) #(Q)
Q<-aviurba$traits
```

and explore the tables

```{r}
head(R)  
head(L)
head(Q)
```

The first part is to perform our CA on specie table

```{r}
afcL <- dudi.coa(log(L+1), scannf = FALSE) 
afcL  

```

The first CA is done. We use log transformation because the abundance of species has a large range and we add "+1" to avoid the log(0) for some species.

Now, we can perform the two MCA analysis on the trait table and environmental table

```{r}
acmR <- dudi.acm(R, row.w = afcL$lw, scannf = FALSE,nf = 4)
scatter(acmR)

acmQ <- dudi.acm(Q, row.w = afcL$cw, scannf = FALSE,nf = 4)
scatter(acmQ)

```

The scatterplot allows to see the ordination of each table and the repartition of factor on the simple axe of MCA.

But now, we will use the RLQ analyse that creates three co-inertia (R-L, L-Q, R-Q), assembles and compares the co-inertia. We use the rlq function for that.

```{r}
rlq <- rlq(acmR, afcL, acmQ, scannf = FALSE)
rlq
axe=c(1:8)
print(paste("The contribution of axe n°",axe, "are", rlq$eig/(sum(rlq$eig))*100,"%"))
#randtest(rlq)
#summary(rlq)
#plot(rlq)

```

Here, the output of the RLQ is complex but only few information are, at this point important. We see that we have 8 eigenvalues and we have their values. All the differents compounds of the output will be used after in representations or analysis.

Nevertheless, we can calculate the contribution of each axis of the RLQ by performing the formule below: METTRE LA FORMULE AU PROPRE rlq$eig/(sum(rlq$eig))\*100

After that, and before to plot and analyse the result, it is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between are different tables. To produce this, we perform a permutation test with the function *randtest*.

```{r}
randtest(rlq)
plot(randtest(rlq))


```

The outputs above corresponds to the permutation test. We see that the number of permutation of columns and rows was to 999 (default value).

The results of this test shows that the permutation of sites (rows) is the first result (p_value=0.1%) and the permutation of species (columns) is the second result (p_value=1.9%) So each result is significant (5% threshold) and we can conclude that our RLQ result is not linked to random effect.

Finally, the results of the RLQ are plot in the distribution law calculated by the permutation.

A citation @bauer2023writing
