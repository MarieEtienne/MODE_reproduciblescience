---
title: "Partie 4 - L'éternel déménagement"
author: "Quentin"
date: "2023-11-15"
output: html_document
---

a- the Markov Chain

![Changement de colonie](C:/Users/qlamb/Documents/R/Schema_suricates.png)

After this latest blow and epidemic, the survivors of our meerkat family are once again forced to look for a habitat and join a new colony. There are several options and colonies to choose from, each with a different probability of being reached depending on the difficulty of the journey and the resources it contains. In other words, the arrows linking the habitats represent the probability of reaching them. When the habitats are reached, a new arrow comes into play to determine the probability of being accepted into the colony; when this is the case, the meerkats settle permanently and stop moving. On the other hand, if they are not accepted in a colony, they can try their luck in a new one or try again to be accepted in a previous colony. This information is used to construct the diagram above. 

For example, our poor meerkats are initially in position O and cannot stay there. However, they can move to a small nearby colony (colony A) with a probability of 0.9 of arriving and 0.6 of being accepted, or to a more distant colony (colony C) with a probability of 0.1 of arriving and 0.8 of being accepted. 

The diagram above is what is known as a "Markov Chain" in which the various probabilities are numerically anotated. A Markov chain can be defined as a discrete-time or continuous-time stochastic process with a discrete state space, defined as being "memoryless" because it depends only on the present state and not on previous states. 

b- The transition Matrix 

These different elements and this "Markov Chain" allow us to produce a rather special matrix T, which we call the transition matrix of a "Markov Chain". This defines the transition probabilities from each state at time (t) vertical to each state at time (t+1) horizontal and is shown below: 

$$
T=
\left(\begin{array}{cc} 
P(O|O) & P(A|O) & P(A1|O) & P(B|O) & P(B2|O) & P(C|O) & P(C3|O) & P(D|O) & P(D4|O)\\
P(O|A) & P(A|A) & P(A1|A) & P(B|A) & P(B2|A) & P(C|A) & P(C3|A) & P(D|A) & P(D4|A)\\
P(O|A1) & P(A|A1) & P(A1|A1) & P(B|A1) & P(B2|A1) & P(C|A1) & P(C3|A1) & P(D|A1) & P(D4|A1)\\
P(O|B) & P(A|B) & P(A1|B) & P(B|B) & P(B2|B) & P(C|B) & P(C3|B) & P(D|B) & P(D4|B)\\
P(O|B2) & P(A|B2) & P(A1|B2) & P(B|B2) & P(B2|B2) & P(C|B2) & P(C3|B2) & P(D|B2) & P(D4|B2)\\
P(O|C) & P(A|C) & P(A1|C) & P(B|C) & P(B2|C) & P(C|C) & P(C3|C) & P(D|C) & P(D4|C)\\
P(O|C3) & P(A|C3) & P(A1|C3) & P(B|C3) & P(B2|C3) & P(C|C3) & P(C3|C3) & P(D|C3) & P(D4|C3)\\
P(O|D) & P(A|D) & P(A1|D) & P(B|D) & P(B2|D) & P(C|D) & P(C3|D) & P(D|D) & P(D4|D)\\
P(O|D4) & P(A|D4) & P(A1|D4) & P(B|D4) & P(B2|D4) & P(C|D4) & P(C3|D4) & P(D|D4) & P(D4|D4)\\
\end{array}\right)
$$

By replacing the previous algebraic matrix with the values from the Markov chain diagram, we obtain the following matrix T :

$$ T=
\left(\begin{array}{cc} 
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\
0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{array}\right)
$$

As we can see, this matrix works in discrete time because of the transition from time t to time t+1 and the absence of a transition period between the different compartments.
However, it could also work in continuous time with discrete state spaces, but for the sake of simplicity we will work in discrete time.
For example, in the matrix above, we can read that the probability of moving from position O (the poor camping tent of our family of beloved meerkats) to colony A is 0.9, or 90%.

c- Evolution of the system

If we take into account the fact that at time 0, the meerkats are in their makeshift tent, here is the initial condition:

$$
T_0 =\left(\begin{array}{cc} 
1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\
\end{array}\right)
$$

At the end of the first time step, we will have :

$$
T_1 = \left(\begin{array}{cc}
1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\
\end{array}\right).\left(\begin{array}{cc} 
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\
0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{array}\right) = T_0.T
$$
So :

$$ 
T_1 =\left(\begin{array}{cc} 
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0\\
\end{array}\right)
$$

In other words, at the end of the first time step the meerkats will have a 90% probability of being in colony A and a 10% probability of being in colony C.

At the end of the second time step, we will have :

$$
T_2 = \left(\begin{array}{cc}
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0\\
\end{array}\right).\left(\begin{array}{cc} 
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\
0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{array}\right)=T_1.T
$$
So :

$$ 
T_2 =\left(\begin{array}{cc} 
0.0025 & 0 & 0.54 & 0.3625 & 0 & 0 & 0.08 & 0.015 & 0\\
\end{array}\right)
$$

At the end of the second time step, the meerkat family will have a 0.25% probability of having returned to its initial camp (position 0), a 54% probability of having been accepted by colony A, a 36.25% probability of having ended up in colony B, an 8% probability of having been accepted in colony C and a 1.5% probability of having reached colony D with the largest quantities of resources.

Generally speaking, we can agree on the following matrix product:

$$
T_t = \left(\begin{array}{cc}
1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\
\end{array}\right).\left(\begin{array}{cc} 
0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\
0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{array}\right)^t=T_0.T^t
$$

d- Eigenvalues and eigenvectors

Markov chain theory tells us that, after a certain time, the probability distribution will be independent of the initial distribution and we will see convergence towards different points in the chain.

Reminder: The properties of a matrix stipulate the presence of eigenvalues and eigenvectors. We say that λ is an eigenvalue of the matrix T if there is an eigenvector V such that T.V = λ.V. We then say that λ and V are an eigenpair of the matrix. 
The eigenvalues are obtained by solving the characteristic polynomial of the matrix T, which is calculated as follows:

$$ P(λ) = det|T-λI| $$
In which I is the "Identity" matrix of the T matrix :

$$
I =
\left(\begin{array}{cc} 
1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{array}\right)
$$

and λ is an eigenvalue of the matrix T. P(λ) will therefore be the determinant of the matrix resulting from the difference between the matrix T and the product of λ and the identity matrix of T, i.e. :

$$
P(λ)=det|\left(\begin{array}{cc} 
-λ & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\
0 & -λ & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1-λ & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0.1 & -λ & 0.7 & 0.2 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & -λ & 0 & 0 & 0 & 0\\
0.025 & 0 & 0 & 0.025 & 0 & -λ & 0.8 & 0.15 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1-λ & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0.1 & 0 & -λ & 0.9\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1-λ\\
\end{array}\right)|
$$

By solving the equation P(λ) = 0, the solutions λ will be the different eigenvalues associated with the matrix T. Since the determinant of a 9*9 matrix is particularly long and difficult to calculate manually, we will determine the eigenvalues and their associated eigenvectors directly on R after creating the matrix and using the following commands:

```{r}
T<-matrix(c(0,0.9,0,0,0,0.1,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0.1,0,0.7,0.2,0,0,0,0,0,0,0,1,0,0,0,0,0.025,0,0,0.025,0,0,0.8,0.15,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.1,0,0,0.9,0,0,0,0,0,0,0,0,1),ncol=9,nrow=9,byrow=TRUE)

VP<-eigen(T)

VP
```

The "eigen" function therefore returns the eigenvalues and eigenvectors of the matrix T. Note that the eigenvalues of a matrix can also contain an imaginary part and be defined by complex numbers, but in this case we will only be interested in the 'Real' part, which can be obtained using the Re function.

The particularity of the eigenvalues of a transition matrix is that they are linked to the eigenvectors associated with the stationary distribution, corresponding to the limit probability of occupying each state in a Markov chain after a large number of iterations. When eigenvalues are equal to 1, their associated eigenvectors are the so-called "stationary distributions" indicating the probabilities of being in each state at equilibrium.

The eigenvalues of a transition matrix are related to the eigenvectors associated with the stationary distribution. The stationary distribution is the limiting probability of occupying each state in a Markov chain after a large number of iterations. The eigenvectors corresponding to the eigenvalues of 1 are the stationary distributions. They indicate the probabilities of being in each state at equilibrium.
