[
  {
    "objectID": "r_chapter.html",
    "href": "r_chapter.html",
    "title": "A simple chapter using R",
    "section": "",
    "text": "This chapter is a simple example using R\nYou can import R package using the code\n\nlibrary(tidyverse)\n\nWarning: le package 'tidyverse' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'ggplot2' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'tibble' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'tidyr' a été compilé avec la version R 4.2.2\n\n\nWarning: le package 'readr' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'purrr' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'dplyr' a été compilé avec la version R 4.2.3\n\n\nWarning: le package 'stringr' a été compilé avec la version R 4.2.2\n\n\nWarning: le package 'forcats' a été compilé avec la version R 4.2.2\n\n\nWarning: le package 'lubridate' a été compilé avec la version R 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nand then describe the purpose of your chapter as well as executing R command.\nFor example a basic summary of a dataset is given by\n\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n\nand produce a graph\n\ndf %&gt;% ggplot() +\n    aes(x=species, y = body_mass_g) +\n    geom_boxplot()  \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nA citation Bauer and Landesvatter (2023)\n\n\n\n\nReferences\n\nBauer, Paul Cornelius, and Camille Landesvatter. 2023. “Writing a Reproducible Paper with RStudio and Quarto.”"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html",
    "href": "r_chapter_boostrapping_resampling.html",
    "title": "Chapter : Resampling methods",
    "section": "",
    "text": "While studying random variables, it is useful to know which statistical distribution they are following. Such knowledge allows to make inferences about the statistical population when only sub-samples are available. For instance, it is needed in order to calculate confidence intervals around an estimated statistic or to calculate p-values to test hypothesis. In other words, recognizing a statistical distribution of the sampled data is essential to estimate the reliability of estimations.\nMany well-studied statistical distributions can be useful in this situation, some of the most famous being the Normal distribution, the Poisson distribution or the Binomial distribution. Despite the diversity of studied distribution, sampled data distribution often differ, whether because they do not follow any studied statistical distribution or because too few data are available making it difficult to recognize any distribution.\nIn this situation, resampling procedures become interesting. It is a non-parametric statistic which has many usages, such as calculating confidence intervals or estimating p-values. The principle of resampling is to “draw samples from the observed data to draw certain conclusions about the population of interest” (Sinharay 2010). This chapter will discuss three resampling techniques: the jackknife, the bootstrap and the permutation (with a focus on the mantel test).\nBefore diving into the subject, let’s load some packages:\n\n# Load packages\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWarning: le package 'gridExtra' a été compilé avec la version R 4.2.3\n\nlibrary(bootstrap)"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#confidence-interval-on-a-sample",
    "href": "r_chapter_boostrapping_resampling.html#confidence-interval-on-a-sample",
    "title": "Chapter : Bootstrapping and Resampling",
    "section": "",
    "text": "hist(data$Sepal.Width)\n\n\n\nshapiro.test(data$Sepal.Width)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$Sepal.Width\nW = 0.98492, p-value = 0.1012\n\nqqnorm(data$Sepal.Width)\nqqline(data$Sepal.Width)\n\n\n\n\nThe sample being normally distributed, we can estimate a confidence interval as below:\n\nlow &lt;- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))\nhigh &lt;- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))\nprint(c(low,high))\n\n[1] 2.987580 3.127086"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#confidence-interval-on-multiple-sub-samples-means",
    "href": "r_chapter_boostrapping_resampling.html#confidence-interval-on-multiple-sub-samples-means",
    "title": "General introduction",
    "section": "Confidence interval on multiple sub-samples means",
    "text": "Confidence interval on multiple sub-samples means\nSi on veut désormais estimer \\(\\mu\\), la taille moyenne des étudiants sur les trois promotions du jeu de données.\n\n(mu &lt;- mean(size$Etudiant_e))\n\n[1] 170.5755\n\n\nmu correspond à la taille moyenne qu’on veut estimer mais dont on ne connait pas la véritable valeur.\nImaginons que nous ayons seulement accès à des moyennes de sous échantillons et qu’on veuille estimer la taille moyenne des étudiants de ces trois promotions.\n\ngroup &lt;- c()\nsampled_data &lt;- c()\nfor (i in 1:20) { #au total, 100 individus auront été échantillonnés\n  set.seed(i)\n  sample_temp &lt;- sample(setdiff(1:139,sampled_data),5)\n  sampled_data &lt;- c(sampled_data,sample_temp)\n  group &lt;- c(group,mean(size[sample_temp,\"Etudiant_e\"]))\n  \n}\n\nLe vecteur group contient les moyennes de plusieurs sous échantillons de 5 étudiants\n\nhist(group)\n\n\n\nqqnorm(group)\nqqline(group)\n\n\n\nshapiro.test(group)\n\n\n    Shapiro-Wilk normality test\n\ndata:  group\nW = 0.98028, p-value = 0.9377\n\n\nLa distribution des moyennes suit une loi normale -&gt; TCL ????\nOn peut définir l’intervalle de confiance suivant:\n\nmean(group)\n\n[1] 171.12\n\nlow &lt;- mean(group)-1.96*sd(group)\nhigh &lt;- mean(group)+1.96*sd(group)\nprint(c(low,high))\n\n[1] 162.6425 179.5975\n\n\nLa moyenne est dans intervalle, mais intervalle grand !!!!!!!!!!!! JCP si l’exemple est ok. !!!!!!!!!!!!!!"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#a---principe",
    "href": "r_chapter_boostrapping_resampling.html#a---principe",
    "title": "General introduction",
    "section": "a - Principe",
    "text": "a - Principe"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#b---exemple-dimplémentation",
    "href": "r_chapter_boostrapping_resampling.html#b---exemple-dimplémentation",
    "title": "General introduction",
    "section": "b - exemple d’implémentation",
    "text": "b - exemple d’implémentation"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#c---avantages-et-inconvénients-de-la-méthode",
    "href": "r_chapter_boostrapping_resampling.html#c---avantages-et-inconvénients-de-la-méthode",
    "title": "General introduction",
    "section": "c - Avantages et inconvénients de la méthode",
    "text": "c - Avantages et inconvénients de la méthode"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#confidence-interval-on-a-sub-sample",
    "href": "r_chapter_boostrapping_resampling.html#confidence-interval-on-a-sub-sample",
    "title": "Chapter : Bootstrapping and Resampling",
    "section": "",
    "text": "Let’s imagine now that we only have access to a smaller data set. The full iris dataset is the population from which we want to estimate a statistical parameter for instance the mean.\n\nmu &lt;- mean(data$Sepal.Width)\nprint(mu)\n\n[1] 3.057333\n\n\nHere \\(\\mu\\) is the “real” mean of the population that theoretically we do not know.\nWe only have access to the following sub-sample of 15 individuals:\n\nset.seed(42)\ndata_sample &lt;- data[sample(1:150,15),]\npar(mfrow=c(2,1))\nhist(data_sample$Sepal.Width)\nqqnorm(data_sample$Sepal.Width)\nqqline(data_sample$Sepal.Width)\n\n\n\n\nThis sample is not normally distributed.\n\nmean(data_sample$Sepal.Width)\n\n[1] 3.146667\n\n\nThe mean is 3.15, the problem is that we can not estimate if the mean that we observe is far from the true mean. We don’t have any method with classical statistics to estimate a confidence interval. This is where resampling methods can become interesting."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#a---the-leave-one-out-jackknife",
    "href": "r_chapter_boostrapping_resampling.html#a---the-leave-one-out-jackknife",
    "title": "Chapter : Bootstrapping and Resampling",
    "section": "a - The leave-one-out Jackknife",
    "text": "a - The leave-one-out Jackknife\nThe principle of the jackknife is to create subsamples of the initial sample by successively removing one of the individuals. This is the leave-one-out technique.\nConcretely, for a sample of \\(n\\) individuals, there will be \\(n\\) subsamples of size \\((n-1)\\). The \\(i^{th}\\) subsample will be composed of individuals from \\(1\\) to \\(n\\) minus the \\(i^{th}\\) individual. A simple representation of all the subsamples is proposed below with the subsamples in lines and the individual numbers in columns.\n\n\n\n\n\nThe next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:\n\\[\nv_{i} = n\\overline{X} - (n-1)\\overline{X}_{-i}\n\\]\nwith the following variables:\n\n\n\n\n\n\n\nVariable\nMeaning\n\n\n\n\n\\(v_{i}\\)\nPseudo value of the \\(i^{th}\\) subsample\n\n\n\\(n\\)\nTotal number of individuals\n\n\n\\(\\overline{X}\\)\nMean of the initial sample\n\n\n\\(\\overline{X}_{-i}\\)\nMean of the \\(i^{th}\\) subsample, corresponding to all individuals except the \\(i{th}\\)\n\n\n\nLet’s write a function which create the subsamples and calculate their pseudo values:\n\npseudo_val &lt;- function(data, theta){\n  #entry : data = the vector of data to which we want to apply the Jackknife\n  #entry : theta = function for the statistic of interest\n  #output : a vector of pseudo values for each subsample\n  n &lt;- length(data)\n  mean_init &lt;- theta(data)\n  pv &lt;- rep(NA,n) #to keep in memory each pseudo value\n  for (i in 1:n) {\n    pv[i] &lt;- n*mean_init - (n-1)*theta(data[-i])\n  }\n  return(pv)\n}\n\nTo try the function:\n\n(pv &lt;- pseudo_val(data =  data_sample$Sepal.Width, \n                  theta = mean))\n\n [1] 3.7 2.9 2.8 3.0 2.8 3.0 3.0 3.8 3.3 3.2 2.8 3.0 3.6 3.8 2.5\n\n\nThe jackknife estimator of the mean will then be calculated as follow:\n\\[\n\\begin{align}\n\\hat{\\theta} & = \\frac{\\sum_{i=1}^{n}v_{i}}{n} \\\\\n\\hat{\\theta} & = \\overline{v}\n\\end{align}\n\\]\nIt corresponds to the mean of the pseudo values \\(v_i\\).\n\n(mean_pv &lt;- mean(pv))\n\n[1] 3.146667\n\n\nThe Jackknife estimator obtained is 3.15. Note that here it is the same as the initial sample mean but it is not always the case. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:\n\\[\n\\begin{align}\nSE_\\hat{X} & = \\sqrt{\\frac{\\sum_{i=1}^{n}(v_{i}-\\overline{v})}{n(n-1)}} \\\\\nSE_\\hat{X} & = \\sqrt{\\frac{\\sigma_{v}^{2}}{n}}\n\\end{align}\n\\]\nWith \\(\\overline{v}\\) being the mean of the pseudo values (and the jackknife estimator).\n\n(SE &lt;- sqrt(var(pv)/length(pv)))\n\n[1] 0.1050472\n\n\nFrom these we can calculate a confidence interval:\n\\[\n[\\, \\overline{v} - 1.96 \\, SE_\\hat{X} ; \\overline{v} + 1.96 \\, SE_\\hat{X} \\,]\n\\]\nNote: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\ncat(\" Lower bound : \",\n    as.character(mean_pv - 1.96*SE),\n    \"\\n\",\n    \"Higher bound : \",\n    as.character(mean_pv + 1.96*SE))\n\n Lower bound :  2.94077409489412 \n Higher bound :  3.35255923843921\n\n\nOn r, functions already exist to automatically execute the Jackknife. For instance, in the package bootstrap, there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).\n\nlibrary(\"bootstrap\")\nprint(jackknife(x= data_sample$Sepal.Width,\n                theta = function(x) mean(x)))\n\n$jack.se\n[1] 0.1050472\n\n$jack.bias\n[1] 0\n\n$jack.values\n [1] 3.107143 3.164286 3.171429 3.157143 3.171429 3.157143 3.157143 3.100000\n [9] 3.135714 3.142857 3.171429 3.157143 3.114286 3.100000 3.192857\n\n$call\njackknife(x = data_sample$Sepal.Width, theta = function(x) mean(x))\n\n\nThe output of the function include the standard error ($jack.se) as describe above. It also include the bias ($jack.bias) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output $jack.values does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In out example, it correspond to th mean of each subsample."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#b---the-grouped-jackknife",
    "href": "r_chapter_boostrapping_resampling.html#b---the-grouped-jackknife",
    "title": "Chapter : Bootstrapping and Resampling",
    "section": "b - The grouped Jackknife",
    "text": "b - The grouped Jackknife"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#c--",
    "href": "r_chapter_boostrapping_resampling.html#c--",
    "title": "Chapter : Bootstrapping and Resampling",
    "section": "c -",
    "text": "c -"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#the-jackknife-resampling-method",
    "href": "r_chapter_boostrapping_resampling.html#the-jackknife-resampling-method",
    "title": "Chapter : Resampling methods",
    "section": "The Jackknife resampling method",
    "text": "The Jackknife resampling method\nThis method was first proposed by Quenouille (1949). The name “Jackknife” comes from the fact that it is often referenced as a “quick and dirty” tool of statistics (Abdi and Williams 2010). It means that it is usable in many situations but it is often not the best tool. The technique allows us to estimate a confidence interval for some statistics when the dataset is too small and/or when it does not follow a known distribution.\n\nPrinciple of the leave-one-out Jackknife (Sinharay 2010 ; Petit 2022)\nThe main goal of the jackknife method is to calculate a confidence interval around a statistic when classical statistics can not apply to the data.The principle is to create subsamples of the initial sample by successively removing some of the observations. Then, on each subsample, a pseudo-value will be calculated. With all the pseudo-values, it is possible to calculate an estimator of the statistic and to estimate its confidence interval.\nThe most famous jackknife is the the leave-one-out jackknife (or order 1 jackknife) for which all the subsamples contain all the observations except one. Concretely, for a sample of \\(n\\) observations, there will be \\(n\\) subsamples of size \\((n-1)\\). The \\(i^{th}\\) subsample will be composed of observations from \\(1\\) to \\(n\\) minus the \\(i^{th}\\) observation.\nTo illustrate, here is our sample:\n\ndata_sample\n\n [1] 2.8 2.8 3.2 3.2 2.3 3.0 2.9 3.4 2.8 3.4 2.4 3.0 3.2 3.9 2.5\n\n\nAnd we can represent some subsamples:\n\ncat(\"1st subsample : \\n\",data_sample[-1], \"\\n\")\n\n1st subsample : \n 2.8 3.2 3.2 2.3 3 2.9 3.4 2.8 3.4 2.4 3 3.2 3.9 2.5 \n\ncat(\"4th subsample : \\n\",data_sample[-4], \"\\n\")\n\n4th subsample : \n 2.8 2.8 3.2 2.3 3 2.9 3.4 2.8 3.4 2.4 3 3.2 3.9 2.5 \n\n\n\n\nCalculation of pseudo-values\nAfter creating the subsamples, the next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:\n\\[\nv_{i} = n\\overline{X} - (n-1)\\overline{X}_{-i}\n\\]\nwith the following variables:\n\n\n\n\n\n\n\nVariable\nMeaning\n\n\n\n\n\\(v_{i}\\)\nPseudo value of the \\(i^{th}\\) subsample\n\n\n\\(n\\)\nTotal number of observations\n\n\n\\(\\overline{X}\\)\nMean of the initial sample\n\n\n\\(\\overline{X}_{-i}\\)\nMean of the \\(i^{th}\\) subsample, corresponding to all observations except the \\(i{th}\\)\n\n\n\nLet’s write a function which create the subsamples and calculate their pseudo values:\n\n# Function for pseudo-values\npseudo_val &lt;- function(data, theta){\n  # entry : data = the vector of data to which we want to apply the Jackknife\n  # entry : theta = function for the statistic of interest\n  # output : a vector of pseudo values for each subsample\n  n &lt;- length(data)\n  mean_init &lt;- theta(data)\n  pv &lt;- rep(NA,n) #to keep in memory each pseudo value\n  for (i in 1:n) {\n    pv[i] &lt;- n*mean_init - (n-1)*theta(data[-i])\n  }\n  return(pv)\n}\n\nTo try the function:\n\n# Function test\npv &lt;- pseudo_val(data =  data_sample,\n                  theta = mean)\nprint(pv)\n\n [1] 2.8 2.8 3.2 3.2 2.3 3.0 2.9 3.4 2.8 3.4 2.4 3.0 3.2 3.9 2.5\n\n\n\n\nStatistical test\nThe jackknife estimator \\(\\theta\\) of the mean will then be calculated as follow:\n\\[\n\\begin{align}\n\\hat{\\theta} & = \\frac{\\sum_{i=1}^{n}v_{i}}{n} \\\\\n\\hat{\\theta} & = \\overline{v}\n\\end{align}\n\\]\nIt corresponds to the mean of the pseudo values \\(v_i\\).\n\n# Compute mean of the pseudo-values\nmean_pv &lt;- mean(pv)\nmean_pv\n\n[1] 2.986667\n\n\nThe Jackknife estimator \\(\\theta\\) obtained is 3.15. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:\n\\[\n\\begin{align}\nSE_\\hat{X} & = \\sqrt{\\frac{\\sum_{i=1}^{n}(v_{i}-\\overline{v})}{n(n-1)}} \\\\\nSE_\\hat{X} & = \\sqrt{\\frac{\\sigma_{v}^{2}}{n}}\n\\end{align}\n\\]\nWith \\(\\overline{v}\\) being the mean of the pseudo values (and the jackknife estimator).\n\n# Compute standard error\nSE &lt;- sqrt(var(pv)/length(pv))\nSE\n\n[1] 0.1086132\n\n\n\n\nConfidence interval\nFrom these we can calculate a confidence interval:\n\\[\n[\\, \\overline{v} - 1.96 \\, SE_\\hat{X} ; \\overline{v} + 1.96 \\, SE_\\hat{X} \\,]\n\\]\n\nNote: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\n\n\n Lower bound :  2.77 \n Higher bound :  3.2\n\n\nThe estimated Jackknife mean is \\(3.15\\) with the following confidence interval : \\([2.94\\:;\\:3.35]\\). The real value \\(\\mu = 3.05\\) is captured within the bounds of the confidence interval which indicate the robustness of the estimation process.\n\n\nR package\nOn r, functions already exist to automatically execute the Jackknife. For instance, in the package bootstrap (Tibshirani 2019), there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).\n\n# Package function\nbootstrap :: jackknife(x= data_sample,\n                theta = function(x) mean(x))\n\n$jack.se\n[1] 0.1086132\n\n$jack.bias\n[1] 0\n\n$jack.values\n [1] 3.000000 3.000000 2.971429 2.971429 3.035714 2.985714 2.992857 2.957143\n [9] 3.000000 2.957143 3.028571 2.985714 2.971429 2.921429 3.021429\n\n$call\nbootstrap::jackknife(x = data_sample, theta = function(x) mean(x))\n\n\nThe output of the function include the standard error ($jack.se) as describe above. It also include the bias ($jack.bias) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output $jack.values does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In our example, it correspond to the mean of each subsample.\n\n\nStrengths of the method\nAs seen precedently, the Jackknife allows to calculate a confidence intervall when data are not following a normal distribution and to estimate the bias induced when only a sample of the statistical population is observed. These two characteristic does not only apply to univariate estimators such as the mean. It can be used on correlation coefficients and regression parameters for instance (Sinharay 2010). In that case, each subsample leave one observations out with all its associated variables. The jackknife method is peticularly interesting when it comes to make inferences about variance ratio. It can perform as good as the Fisher test if variables are normally distributed and better if they are not (miller_jackknife?–review_1974).\nThe Jackknife method was a good tool in the last century as it was easy to apply manually but it is nowaday surpassed by other methods which emerged thanks to the evolution of computers.\n\n\nWeaknesses of the method\nThis method is not always efficient, it is not recomanded for time series analysis where some time period are successively removed to create the subsample. Moreover it has little succes when it comes to the estimation of single order statistics (specific values in an ordered set of observations or data points) such as the median or the maximum (miller_jackknife?–review_1974). But for this last point, other jackknife can be used such as the deleted-d jackknife which perform better for the median. In this method, subsamples are of size \\((n-d)\\) and there are \\(\\binom{n}{d}\\) subsamples.\nFor more informations about applications of the Jackknife you can read the paper: Jackknife a Review by Rupert G. Miller (miller_jackknife?)–review_1974."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#the-bootstrap-re-sampling-method",
    "href": "r_chapter_boostrapping_resampling.html#the-bootstrap-re-sampling-method",
    "title": "Chapter : Resampling methods",
    "section": "The bootstrap re-sampling method",
    "text": "The bootstrap re-sampling method\n\nIntroduction and Principle\nNow that we’ve thoroughly explored jackknife resampling in the previous section, let’s delve into another resampling technique: the bootstrap method.\nSimilar to jackknife, bootstrap aims to estimate descriptive parameters of a sample and assess the accuracy of these estimates for making inferences about the actual population. It is yet another statistical inference method that involves generating multiple datasets through resampling from the original dataset. Essentially, the concept revolves around using resampling techniques to create a probability distribution for a chosen parameter. This distribution, known as the empirical distribution function, serves as an estimation of the true probability distribution of our parameter in the population. In practice, similar to the earlier section, our objective here is to compute parameters of interest from our sample (such as mean, median, or even the \\(R^2\\) of a regression, among others) and establish confidence intervals around these estimates.\nThe fundamental difference between jackknife and bootstrap lies in their resampling methodologies. In each of its \\(i\\) iterations, bootstrap randomly resamples \\(n\\) elements from an initial sample of \\(n\\) data points with replacement. This leads to two crucial distinctions from the resampling method employed in jackknife: i) The new samples generated through bootstrap maintain the same size \\(n\\) as the original dataset. ii) Given the sampling with replacement approach, a particular element can occur multiple times in a new sample or might not appear at all during the resampling process.\nAfter generating these \\(i\\) bootstrap samples, the intended statistical parameters are computed for each of these samples. As a result, we obtain a distribution of \\(i\\) data points derived from these samples, forming what is known as our empirical distribution function. Subsequently, the analysis of this distribution allows us to estimate precision, particularly in establishing the confidence interval for our statistical parameters.\n\n\nPractice and Application\nIf the previous introduction seemed a bit perplexing, don’t worry—we’ll use an example to illustrate this concept.\nFor this demonstration, we’ll once again consider our non-normally distributed sampling of 15 individuals. Let’s proceed with the calculation of the mean.\n\n# Field sampling mean\nmean(data_sample)\n\n[1] 2.986667\n\n\nSo, we’ve obtained a mean value of 3.15, which is great. However, this mean value can’t be reliably generalized to the entire population because we lack information about the variability induced by the specific individuals we chose to sample. As highlighted in our previous code, we only sampled \\(n=15\\) individuals in the field. Clearly, this limited sampling isn’t sufficient to confidently assert that our estimated mean is truly representative of the entire population. Hence, it becomes essential to gain insights into the variability of the mean we’ve just calculated. This is precisely where the bootstrap method comes into play.\nWe’ll now systematically apply our algorithm to resample the initial dataset obtained from our field sampling. This resampling process will be utilized to construct the probability distribution of our mean.\n\nbootstrap resampling iterations\n\n\n# Fix our number of bootstrap iterations\nB = 1000\n\n# Create an empty list to stock our resamplings\nbootstrapSamples &lt;- vector(\"list\",B)\n\n# bootstrap resampling algorithm\nfor (i in 1:B) {\n  # Randomly sample n elements with replacement\n  bootstrapSamples[[i]] &lt;- sample(data_sample, replace = TRUE)\n}\n\nFor pedagogical purposes, let’s examine a few resamples while comparing them to our initial field sampling dataset.\n\n# Our original field sampling\nsort(data_sample)\n\n [1] 2.3 2.4 2.5 2.8 2.8 2.8 2.9 3.0 3.0 3.2 3.2 3.2 3.4 3.4 3.9\n\n\n\n# A few resampling\nsort(bootstrapSamples[[5]])\n\n [1] 2.4 2.4 2.4 2.4 2.5 2.5 2.8 2.8 3.2 3.2 3.2 3.2 3.2 3.4 3.9\n\nsort(bootstrapSamples[[777]])\n\n [1] 2.3 2.3 2.4 2.5 2.8 2.8 2.8 2.8 2.8 3.0 3.2 3.2 3.4 3.4 3.4\n\n\nObserve how certain values are repeated more frequently than in our initial dataset. Additionally, note that some values aren’t sampled at all. For instance, the value 2.5 appeared only once in our initial set and consequently was seldom or perhaps never resampled. Conversely, the value 2.8 was more prevalent in our initial dataset and consequently was sampled more frequently\n\nCompute mean for every resampled datasets\n\nNow, we’ll compute the parameter of interest for each of our resampled datasets. This process will yield our set of \\(i=1000\\) mean values, which we’ll utilize to construct our empirical distribution in the subsequent step.\n\n# Compute mean for each bootstrap samples\niterationMeans &lt;- sapply(bootstrapSamples, mean)\n\n\nPlot the probability distribution\n\nThe computation of the distribution for our parameter of interest is complete. We can now proceed to plot it, providing us with an understanding of the variability surrounding our estimated mean value.\n\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"black\", aes(y =after_stat(density))) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(xintercept = mean(data_sample), color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic()\n\n\n\n\nHence, this probability distribution provides us with insights into the variability around the mean value estimated from our field sampling. As anticipated, and as indicated by the red dashed line, this distribution is centered around the estimated mean value.\n\nCompute Confidence Interval around our mean estimate\n\nRecall the primary objective of the bootstrap method: to assess the precision of our parameter estimation to make inferences about the broader population. To accomplish this, we’ll utilize the bootstrap percentiles. For instance, suppose we aim to calculate the 95% Confidence Interval. In this case, we’ll allocate 5% of the error evenly on both sides of our distribution, corresponding to the values at the 2.5th and 97.5th percentiles in our empirical distribution.\n\n# Get percentile 2.5%\nquantile(iterationMeans, probs = c(0.025,0.975))\n\n    2.5%    97.5% \n2.793333 3.186833 \n\n\nAnd thus, we’ve successfully obtained our confidence interval for the mean estimation: \\(2.97 \\leq \\overline{x} \\leq 3.33\\).\nFor educational purposes, let’s proceed to plot these percentiles in green.\n\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"black\", aes(y =after_stat(density))) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(xintercept = mean(data_sample), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quantile(iterationMeans, probs = 0.025), color = \"darkolivegreen4\", linetype = \"dashed\", linewidth = 1.5) +\n  geom_vline(xintercept = quantile(iterationMeans, probs = 0.975), color = \"darkolivegreen4\", linetype = \"dashed\", linewidth = 1.5) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic()\n\n\n\n\n\nCompare with population mean value\n\nThe bootstrap resampling method has facilitated the construction of a confidence interval around our mean estimate with a 5% margin of error. Now, let’s compare this interval with the actual value of our Iris population.\n\nmean(irisPopulation)\n\n[1] 3.057333\n\n\nThe actual sepal width mean of our population falls within the bounds of our confidence interval, which is highly encouraging! Comparing it to the interval estimated using the jackknife method, we notice that this interval is narrower and more precise. Consequently, we’ve effectively inferred information about the population mean in a scenario where we had limited data, made no normality assumptions, and where the standard computation of confidence intervals might not have been sufficiently robust and a\n\n\nStrengths and Weaknesses\nAs we’ve seen, bootstrap is a powerful resampling technique used in statistics for estimating the precision of parameters. One of its primary advantages lies in its versatility across a wide spectrum of statistical estimations. Indeed, this resampling technique is applicable to various parameters, such as mean, variance, regression coefficients, and more. Furthermore, this method is particularly robust in handling complex data structure without strict distributional assumptions, meaning that it can easily works with non-normally distributed datasets with little observations, which is really relevant in real-word scenarios. In those cases, bootstrap ends up being more accurate than the standard intervals obtained under normality assumption.\nHowever, one significant concern is its computational intensity, especially noticeable with larger datasets or complex models. Generating numerous bootstrap samples can demand, in certain cases, substantial computing resources. Additionally, bootstrap might display biased estimation of the accuracy of estimates, depending on whether or not the sampling dataset is representative of the actual population.\nOverall, bootstrap’s major advantages lie in its broader applicability across various statistical estimations, but which comes with higher computational demands. In contrast, Jackknife might be less versatile, but tends to offer a better computational efficiency and less biased parameters, making it a favorable choice under certain conditions.\n\n\nTo go further\nThis section on the bootstrap method offers a fundamental understanding, providing explicit R scripts for pedagogical purposes. If you find yourself needing to utilize bootstrap, we highly recommend exploring the capabilities of the precedently presented bootstrap R package, which streamlines bootstrap implementation. Moreover, the bootstrap method extends beyond basic parameter estimation, offering various derivatives and applications catering to diverse needs:\n\nParametric bootstrap: Assumes a specific parametric distribution, generating bootstrap samples from this fitted distribution and allowing inference within a particular statistical model.\nSmooth bootstrap: Incorporates smoothing techniques to reduce variability in resampling, ideal for noisy or irregular data.\nBayesian bootstrap: Generates bootstrap samples from posterior distributions, useful for uncertainty estimation in Bayesian analysis (see more in Bayesian chapter of this book).\n\nLastly, bootstrap resampling isn’t limited to parameter estimation. It can also be employed to assess statistical tests, a topic we’ll delve into in our upcoming section."
  }
]