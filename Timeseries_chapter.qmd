---
title: "Timeseries"
bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

Temporal dependency means that values sampled at a given time can be related to the value sampled before. Therefore, the variable can be correlated to itself and create **pseudoreplication**. 


Diverse packages on R are useful to manage time series, such as TSA (Time Series Analysis). 
```{r, echo = F, error=FALSE}
## Install and load the packages required

library(tidyverse) # Data cleaning, ordering, etc.
library(lubridate) # Helps manage the date format data
library(ade4)      # Study ecological data
library(nlme)      # glm and gls
library(car)       # Type II anova.

#To install older version of a package to use the arimax function.
require(devtools)
install_version("TSA", version = "1.2.1", repos = "http://cran.us.r-project.org")
install_version("TSA", version = "1.2.1", repos = "https://pbil.univ-lyon1.fr/CRAN/")
library(TSA)       # Time series analysis (autocorrelation tests)
```

### The dataset

To understand better how to handle time series, data are used to illustrate the process from an experiment on vultures in time. 

```{r}
## Load the data
#setwd("C:/Users/User/Documents/GitHub/MODE_reproduciblescience")
dataT=read.table("vautour.txt",header=T,dec=",", stringsAsFactors = T)

head(dataT)       # What are the data
summary(dataT)    # Summary of the spread of each variable
```

Two colonies 'C' and 'O' are studied from 1972 to 2003. In particular, the reproductive success (success), the growth rate (growth), the density of population (denstot) and the resource availability (ratio) of the colonies. 

```{r}
## Arrange the dataset
dataT_ordered=dataT %>% arrange(col,year)
## Create 2 sub-datasets for the 2 colonies
C=dataT[dataT$col=="C",]
O=dataT[dataT$col=="O",]
```

### Data exploration

```{r}
## Data representation
# Window size
par(mfrow=c(2,3),mar=c(2,4,2.5,0),oma=c(2,2,2,4)) 

## Colony C
plot(C[,2],C[,3], xlab="year", ylab="mean reproductive success")
lines(C[,2],C[,3])
plot(C[,2],C[,4], pch=17, xlab="year", ylab="Colony growth rate")
lines(C[,2],C[,4])
plot(C[,2],C[,5],pch=16, xlab="year", ylab="Number of pairs")
lines(C[,2],C[,5])
mtext(text="Parameter for Colony C",side=3,line=-1,outer=TRUE)

## Colony O
plot(O[,2],O[,3],  xlab="year", ylab="mean reproductive success")
lines(O[,2],O[,3])
plot(O[,2],O[,4],pch=17, xlab="year", ylab="Colony growth rate")
lines(O[,2],O[,4])
plot(O[,2],O[,5], pch=16, xlab="year", ylab="Number of pairs")
lines(O[,2],O[,5])
mtext(text="Parameter for Colony O",side=1,line=-14.5,outer=TRUE)
```
Through time, all the variables do not have the same behaviour for both of the colonies. 
Some variables fluctuate in time like the O colony growth rate, or can evolve gradually, like the number of pairs for the O colony.

The irregularity in time of the variables can mean that the data are correlated to the precedent ones.
A key parameter to know if there really is a correlation of the variable to itself is **autocorrelation**.


### Autocorrelation

Autocorrelation is given by the expression: $$\rho(k) = \frac{E(X_tX_{t+1}) - E(X_t)E(X{t+1})}{\sqrt{E(X_t^2)E(X_{t+1}^2)}}$$

With the discrete sequence $\{X_1, ... X_T\}$ with $X = \{X_t\}_{t\in\mathbb{N}}$ as $E[X_t^2] < \infty$ for $t\in\mathbb{Z}$. 

In other words, the values are compared to their neighbors to conclude if they are correlated or not.

The autocorrelation can be detected in a **non-stationary process**.

**Stationnarity** is the constistency of the statistical parameters of a series such as the mean and the variance (Figure). 

- In non-stationary processes, these parameters vary in time.
- In stationary processes, the values gravitate around a mean value within the same range. 

![Stationary and non-stationary processes](C:/Users\User\OneDrive\Bureau\MODE\M2\OCR\Stationarycomparison.png)

Given the precedent plots displayed, one would think there are both stationary and non-stationary processes within the different variables.
Tests will allow us to know with more details if there is a dependency. 

On R, autocorrelation can be known thanks to the function acf (Auto- and Cross- Covariance and -Correlation Function Estimation) of the TSA package. 

**Important note:** In order for the afc() function to work, the data have to be sorted according to time with one value per time step. To do so, *NA* or mean values can be added if necessary to fill the dataset accordingly. The package lubridate can be useful to manage dates data.

```{r}
par(mfrow=c(2,3),mar=c(2,4,4,2))
acf(C[,3], main="Reproductive success for C")
acf(C[,4], main="Growth rate for C")
acf(C[,5], main="Density for C")
acf(O[,3], main="Reproductive success for O")
acf(O[,4], main="Growth rate for O")
acf(O[,5], main="Density for O")
```
The plots display the correlation coefficient at the associated time step with the neighbors (lag). The blue dotted lines represent the significance threshold of correlation. 

These plots allow us to know if the time dependency exists and what profile it follows.

- Growth rate of O and reproductive success of C : **White noise**. There is no significant value and particular pattern. Therefore, no time dependency.
- Density of C and reproductive success of O : **Short positive dependency**. Some values are significantly positively correlated and a pattern emerges (cyclic here). 
- Growth rate of C : **Short negative dependency**. Some values are significantly negatively correlated and a pattern emerges (cyclic here).
- Density of O : **Long term dependency**. Several values are significantly correlated and a pattern appears (linear here).

For the following analyses, we will focus on the Density of the O colony.
The time is considered an **autocovariable**.

### Models

Now that the time dependency have been proved to be true thanks to autocorrelation test, it is possible to proceed with adapted statistical tests. 

Different models can be used according to the type of dependency found in the data. 
The goal will be to apply the model and verify that there is no time dependency left on the **residuals**.

#### Least square ordinary regression

Least square ordinary regression methods are used during the process of selection of the model.

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2+...+\beta_nX_n + \epsilon$$
- $Y$ is the dependent variable
- $X_n$ are the explanatory variables 
- $\beta_n$ are the coefficients of $X$
- $\epsilon$ is the random error term or **residuals**

To select a relevant model is the same as selecting a model that maximizes **likelihood** and minimize the **number of parameters**. 


#### Weak time dependency

If the data display a weak time dependency, it is possible to simply add the time as a random variable to the model. 

$$Y\sim\alpha.X + time$$ 

**Important note:** If the variance of $Y$ increases beforehand, you will need to perform a **logarithmic** transformation. If the variance of $Y$ decreases beforehand, one would use an **exponential** transformation. Here, the variance increases. 

A useful package on R to manage glm and gls is be nlme.

```{r}
par(mfrow=c(1,2))
hist(O$denstot, main = paste("Histogram of total density", "\n" ,"of O colony "), xlab = "Density")
hist(log(O$denstot), main = paste("Histogram of total density", "\n" ,"of colony O (log transformation)"), xlab = "Log(Density)")
```
The plots indicates that a log transformation could help better manage the data.

A simple lme model can be used (Linear Mixed-Effects Model) with X = log(total density) and Y the ressource availability.

We can check if the model is satisfactory if the residuals are not significant.

```{r}
## Model construction
modlme=lme(log(denstot) ~ ratio, data=O, random=~1|year) 
## Residuals
par(mfrow=c(1,1))
acf(modlme$residuals[,1], main = "Residuals of the lme model")
```

```{r}
## Residuals
summary(modlme)
```


The plot shows there is still a time dependency in this kind of model. 
The correlation value is too strong, the model is not adapted to this case. 

### Medium to strong time dependency

If a medium or a strong time dependency are proven, adding time as a random effect will not be effective enough. 
It will be necessary to use **Time Models**.

#### Stationary process

To deal with stationary processes, it is possible to use GLS (Generalized Leat Square) models. 
Three major GLS models can be used. 

- **AR** or **Autoregressive Model** states that a value $Y_{(t)}$ will depend on the values $Y$ at $p$ the precedent times. The model adds a an autoregressive coefficient $\phi_p$ to each $Y_{(t-1)}$ value. In summary, this model explains the current value as a linear combination of the precedent values with $\epsilon_t$ the random component or white noise. 
$$Y_{(t)}=\mu+\phi_1 \times Y_{(t-1)}+\phi_2 \times Y_{(t-2)} + ...+\phi_q \times Y_{(t-p)}+\epsilon_{(t)}$$

- **MA** or **Moving Average model** functions in the same way as the AR model. However, it will consider that the current value of a series is the weighted linear combination of the previous $q$ errors. 

$$Y_{(t)}= \mu+ \theta_1 \times \epsilon_{(t-1)}+\theta_2 \times \epsilon_{(t-2)}) + ...+\theta_q \times \epsilon_{(t-q)} $$

- **ARMA** is a combination of **AR** and **MA** models and is a linear combination of the $p$ past values and $q$ past residual errors.

$$Y_{(t)}=\mu+\phi_1 \times Y_{(t-1)}+\phi_2 \times Y_{(t-2)} + ...+\phi_q \times Y_{(t-p)}+\epsilon_{(t)} + \theta_1 \times \epsilon_{(t-1)}+\theta_2 \times \epsilon_{(t-2)}) + ...+\theta_q \times \epsilon_{(t-q)} $$
The gls() function covers the three types of model depending on the correlation structure of the data. 


```{r}
## Model construction
modgls=gls(log(denstot) ~  year + ratio, data=O, correlation = corAR1(value=0.9,form = ~ year))
acf(modgls$residuals, main = "Residuals of the gls model")
```

```{r}
summary(modgls)
```

The autoregressive coefficient Psi is equal to 0.72. The autocorrelation is not completely removed. Still, the variable 'ratio' does not have a significant effect on $Y$ anymore.


We can compare the two previous models thanks to an analysis of their variance (anova).

```{r}
anova(modgls,modlme)
Anova(modgls)
Anova(modlme)
```

```{r}
modgls2=gls(log(denstot) ~   ratio, O, correlation = corAR1(value=0.9,form = ~ year))
anova(modgls2,modlme)
anova(modgls2,modgls)

anova(modgls2)
```

The second models fits the data better than the first. We can conclude that the autocorrelation made the ratio variable effect wrongly significant.

In order to chose the range of application of the parameters $p$ and $q$, it is possible to use a **partial correlation function** that will display the direct correlation link between the time steps. 

```{r}
par(mfrow=c(1,2))
acf(log(O$denstot), main="Density for O")
pacf(log(O$denstot), main="Density for O")
```
So, the order $p$ (AR) is generally chosen based on significant lags in the PACF, and the order $q$ (MA) is generally chosen based on significant lags in the ACF.


#### Non-stationary process

To manage non-stationary processes, another model called **ARIMA** is used. A new parameter $d$ is introduced with $p$ and $q$. 

 **ARIMA(p,d,q)** models are a combination of **AR** models with $p$, **MA** models with $q$, and finally includes difference of order $d$. 

This difference helps make the series stationary by subtracting from the current value the values from the previous $d$ time points.

In general, we will choose an order $d=1$ when the series shows a **constant trend**, and an order $d=2$ when the series shows a **fluctuating trend**.

On R, ARIMA models can be used with the function arima(). The argument 'order'contains the values of $p$, $d$ and $q$, in that order. 

```{r}
arima1=stats::arima(log(O$denstot),order=c(1,1,1),xreg=O$ratio)
arima1
```
Different values of these parameters can be tested to find the best model with the AIC criteria displayed in the results. $d$ must always be equal to 1. 

The ARIMA function can help make **predictions**.

- prediction: now we will use this statistical model to make prediction under a specific scenario
- scenario: constant and maximum availability in resource (ratio=1) during 20 years

```{r}
newxreg=data.frame(O.ratio=rep(1,20))
predict(arima1, n.ahead = 20,newxreg)
pred=predict(arima1, n.ahead = 20,newxreg)
par(mfrow=c(1,1))
obspred=rbind (data.frame(denstot=O$denstot), data.frame(denstot=exp(predict(arima1, n.ahead = 20,newxreg)$pred)))
plot(obspred$denstot, main = "Prediction on a 20 years scenario", ylab ="Predicted observations for the total density of O")
lines(obspred$denstot)
```


### To go further

The **ARIMAX** model, which stands for **AutoRegressive Integrated Moving Average with eXogenous inputs**, is an extension of the ARIMA model that incorporates additional exogenous (external) variables. The ARIMAX model is particularly useful when there are external factors that can influence the time series to model.

The eXogenous (X) Inputs are additional external variables that are not part of the time series being modeled but may influence it.
These variables are included in the model to capture their impact on the dependent variable.

The ARIMAX model allows to account for the influence of external factors when modeling time series data, making it more flexible and potentially improving the accuracy of predictions. 

