[
  {
    "objectID": "prior_course.html",
    "href": "prior_course.html",
    "title": "Be ready for the course",
    "section": "",
    "text": "The Online Collaborative Resources (OCR) class will make intensive use of the Reproducible Science tools. For this class and as an investment for the future, you will need to:"
  },
  {
    "objectID": "prior_course.html#git-on-your-computer",
    "href": "prior_course.html#git-on-your-computer",
    "title": "Be ready for the course",
    "section": "Git on your computer",
    "text": "Git on your computer\nInstall Git on your personal computer: Go to the Git website and click on the computer screen on the right, which should offer you the version suitable for your operating system."
  },
  {
    "objectID": "prior_course.html#github-account",
    "href": "prior_course.html#github-account",
    "title": "Be ready for the course",
    "section": "GitHub account",
    "text": "GitHub account\nIf you don’t have one, you have to create a GitHub account and set up the link between your computer and the GitHub. Be careful with the login you choose, this account will be used professionally and you may want to avoid pseudo like toto2024, or ladybug288.\nGitHub is just one solution to share git repositories online, you may work later with gitlab with is very similar to GitHub."
  },
  {
    "objectID": "prior_course.html#a-ssh-connection",
    "href": "prior_course.html#a-ssh-connection",
    "title": "Be ready for the course",
    "section": "A SSH connection",
    "text": "A SSH connection\nTo have smooth interactions with Github, you have to use a SSH and to do so, you will to generate SSH key on your computer and copy paste the public SSH key id_rsa.pub on your GitHub account.\n\nOpen a terminal (any terminal on Mac and Linux, the Git bash program you have installed with git if you are using Windows)\ntype the command\nssh-keygen\nDO NOT ENTER ANY PASSPHRASE while asked, simply press enter (twice generally)\nYou should have a directory named .ssh in your main personnal folder. Open the file id_rsa.pub with any text basic editor (like notepad, gedit …) and copy the key.\nGo to the settings of your GitHub account, choose the SSH and GPG keys, then press New SSH key and paste the previously copied key."
  },
  {
    "objectID": "prior_course.html#let-me-know-who-you-are",
    "href": "prior_course.html#let-me-know-who-you-are",
    "title": "Be ready for the course",
    "section": "Let me know who you are",
    "text": "Let me know who you are\nPlease enter your name and GitHub login on this spreadsheet and indicate in the last column whether you are already familiar with some markup languages (markdown, HTML) and if you have prior experience with Git or any version control system.\nPlease try to install all of this as soon as possible, and if you encounter any difficulties, don’t hesitate to contact me by email or come to me at the beginning of the class.\nIt will be easier if you can follow this procedure on your laptop and bring it to the class, but if you don’t have any laptop we will able to use the computers in the classroom (however this tedious installation process will have to be repeated on your personal computer)"
  },
  {
    "objectID": "python_chapter.html",
    "href": "python_chapter.html",
    "title": "Chapter : statistical tests with dependent data",
    "section": "",
    "text": "This chapter is a simple example using python\nYou can import python libraries using the code\n\nimport pandas as pd\nimport numpy as np\n\nand then describe the purpose of your chapter as well as executing python command.\nFor example a basic summary of a dataset is given by\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\")\n\nand produce a graph\n\ndf.boxplot(by = 'species', column = 'body_mass_g')  \n\n&lt;Axes: title={'center': 'body_mass_g'}, xlabel='species'&gt;"
  },
  {
    "objectID": "simple_chapter.html",
    "href": "simple_chapter.html",
    "title": "A simple chapter with no code",
    "section": "",
    "text": "This chapter is a simple example of qmd format"
  },
  {
    "objectID": "simple_chapter.html#subsection",
    "href": "simple_chapter.html#subsection",
    "title": "A simple chapter with no code",
    "section": "subsection",
    "text": "subsection\na list\n\nblabla\nblabla 2\nblabla 3"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "How to contribute",
    "section": "",
    "text": "Create a specific branch for your chapter named an_explicit_name_for_the_branch and switch on this branch\n\ngit checkout -b an_explicit_name_for_the_branch\nIn the development of the project, you might find necessary to have several branches per chapter.\n\nCreate a quarto file for the chapter chapter0.qmd in your branch\n\ngit add chapter0.qmd\ngit commit -m \"first commit in the chapter branch\"\n\nPush everything on the remote repo on Github\n\nFor the first push after the branch creation, you have to specify the name of the branch on the remote repo and you can use git  push --set-upstream origin an_explicit_name_for_the_branch or\nFor the push to come after the first one, you will simply push by\ngit  push\nOnce you are quite happy with your production, you will be willing to integrate your production on the main branch. A good practice is to ask the permission to push on the main branch, which is named a pull request (PR).\n\nAsk for a PR on Github\nIf needed, specify in the PR message the package you need and mention MarieEtienne as reviewer of the PR.\n\nA mock rendering of the qmd file will start when you request a PR using the Github Action mechanism (called runner on gitlab). If the action passes (green signal), you can go to the next step. If not, you will have to fix the issue (again you can ask assistance if you don’t understand the error).\n\nOnce the PR is checked, mention one of your colleage as reviewer."
  },
  {
    "objectID": "instructions.html#one-chapter-corresponds-at-least-to-one-branch",
    "href": "instructions.html#one-chapter-corresponds-at-least-to-one-branch",
    "title": "How to contribute",
    "section": "",
    "text": "Create a specific branch for your chapter named an_explicit_name_for_the_branch and switch on this branch\n\ngit checkout -b an_explicit_name_for_the_branch\nIn the development of the project, you might find necessary to have several branches per chapter.\n\nCreate a quarto file for the chapter chapter0.qmd in your branch\n\ngit add chapter0.qmd\ngit commit -m \"first commit in the chapter branch\"\n\nPush everything on the remote repo on Github\n\nFor the first push after the branch creation, you have to specify the name of the branch on the remote repo and you can use git  push --set-upstream origin an_explicit_name_for_the_branch or\nFor the push to come after the first one, you will simply push by\ngit  push\nOnce you are quite happy with your production, you will be willing to integrate your production on the main branch. A good practice is to ask the permission to push on the main branch, which is named a pull request (PR).\n\nAsk for a PR on Github\nIf needed, specify in the PR message the package you need and mention MarieEtienne as reviewer of the PR.\n\nA mock rendering of the qmd file will start when you request a PR using the Github Action mechanism (called runner on gitlab). If the action passes (green signal), you can go to the next step. If not, you will have to fix the issue (again you can ask assistance if you don’t understand the error).\n\nOnce the PR is checked, mention one of your colleage as reviewer."
  },
  {
    "objectID": "instructions.html#as-a-reviewer",
    "href": "instructions.html#as-a-reviewer",
    "title": "How to contribute",
    "section": "As a reviewer",
    "text": "As a reviewer\nYour role is essential as you are responsible for the quality of the submission you were assigned to. Read carefully the production and ask for correction/clarification if needed. One you are happy with the correction you can accept the PR."
  },
  {
    "objectID": "linear_modelling.html",
    "href": "linear_modelling.html",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "The purpose of statistics is to test a hypothesis. You would need to follow these steps:\n\n\nThey define different proposed relationships between dependent and independent variable(s).\nNull hypothesis H0 = no relationship between dependent and independent variables.\nAlternative hypothesis H1 = some expected relationship between the variables.\n\n\n\nThe observed value Tobs of the test statistic T is calculated from data given that the T variable has a probability distribution known under H0.\n\n\n\nTo get this value, you need to calculate the probability of observing Tobs value in this T distribution knowing the distribution of the test statistic T under H0.\n\n\n\nIf the P-value is less than (or equal to) \\(\\alpha\\), then the null hypothesis is rejected in favor of the alternative hypothesis and if the P-value is greater than \\(\\alpha\\) then the null hypothesis is not rejected. WARNING : never say that the null hypothesis is validated. You just know that you cannot reject it with the information you have. There could be relationships that have not been detected.\n\n\n\n\nLinear modelling is widely used in statistics to model observed data, by considering their random nature. It explains one dependent variable, noted Y (random variable, also called the response variable) in function to independent variables (also called predictors or explanatory variables), also observed/measured on statistical units of the sample. In this chapter, you will see different models corresponding to different statistical tests. It happens that several tests can be conducted for the same dataset and hypothesis ; in this case, you will select the most powerful test. It is the one with the lowest \\(\\beta\\)-error (given at the end)."
  },
  {
    "objectID": "linear_modelling.html#the-statistical-test",
    "href": "linear_modelling.html#the-statistical-test",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "The purpose of statistics is to test a hypothesis. You would need to follow these steps:\n\n\nThey define different proposed relationships between dependent and independent variable(s).\nNull hypothesis H0 = no relationship between dependent and independent variables.\nAlternative hypothesis H1 = some expected relationship between the variables.\n\n\n\nThe observed value Tobs of the test statistic T is calculated from data given that the T variable has a probability distribution known under H0.\n\n\n\nTo get this value, you need to calculate the probability of observing Tobs value in this T distribution knowing the distribution of the test statistic T under H0.\n\n\n\nIf the P-value is less than (or equal to) \\(\\alpha\\), then the null hypothesis is rejected in favor of the alternative hypothesis and if the P-value is greater than \\(\\alpha\\) then the null hypothesis is not rejected. WARNING : never say that the null hypothesis is validated. You just know that you cannot reject it with the information you have. There could be relationships that have not been detected."
  },
  {
    "objectID": "linear_modelling.html#statistical-modelling",
    "href": "linear_modelling.html#statistical-modelling",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "Linear modelling is widely used in statistics to model observed data, by considering their random nature. It explains one dependent variable, noted Y (random variable, also called the response variable) in function to independent variables (also called predictors or explanatory variables), also observed/measured on statistical units of the sample. In this chapter, you will see different models corresponding to different statistical tests. It happens that several tests can be conducted for the same dataset and hypothesis ; in this case, you will select the most powerful test. It is the one with the lowest \\(\\beta\\)-error (given at the end)."
  },
  {
    "objectID": "linear_modelling.html#anova",
    "href": "linear_modelling.html#anova",
    "title": "Linear modelling in ecology",
    "section": "1) ANOVA",
    "text": "1) ANOVA\n\nINTRODUCTION\nANOVA (Analysis of variance) is one of the most widespread techniques in data analysis. We use it to test the effect of one or more independent quantitative variables (Xs) on a dependent qualitative variable (Y). The categorical qualitative variables are named ‘factors’, et each factor has different levels that are chosen and fixed.\nWe consider 2 types of ANOVA: in the presence of a single variable X in the analysis, we follow a simple factor ANOVA; in the presence of several variables X, we follow a multiple factor ANOVA.\n\n\nSimple factor ANOVA\nThe model takes the following form: \\[ Y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith level of the single factor and \\(\\epsilon\\) is the error term (i.e. residuals).\n\n\nMultiple factor ANOVA\nThe ANOVA model depends on the experimental design: factorial or nested.\nFull factorial design This design studies the influence of multiple factors and of their interactions on the variable of interest. We frequently want to test for differences in the response variable due to the multiple factors, called ‘main effects’. What we do is test the effects of each main effect separately, then whether or not these effects interact with each other (‘factor interactions’). Considering a factorial design with two factors, the model takes the form: \\[ Y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + \\gamma_{ij} +\\epsilon_{ijk}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith group of the first factor, and \\(\\beta_{j}\\) is the effect of the jth group of the second factor, \\(\\gamma_{ij}\\) is the interaction between both factors and \\(\\epsilon\\) is the error term (i.e. residuals).\nNested ANOVA In this design, the levels of a factor are hierarchically nested within the levels of another factor. Considering a nested design with two factors in which B factor is nested in A factor, the model takes the form: \\[ Y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j/i} +\\epsilon_{ijk}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith group of the first factor, and \\(\\beta_{j/i}\\) is the effect of the jth group of the second factor nested in the ith group of the first factor and \\(\\epsilon\\) is the error term (i.e. residuals).\n\n\nANOVA EXAMPLE\nLet’s consider an experimental data originated to an ANOVA example developed by James Lavender & Alistair Poore - 2016 (https://environmentalcomputing.net/statistics/linear-models/anova/anova-factorial/).\n\n\nDataset presentation and objectives of the analysis\nIn this data analysis, we will focus on experimental data with two factors that are both applied to all statistical individuals. An ecologist wants to test the effects of metal contamination on the number of species found in sessile marine invertebrates (i.e. sponges). This ecologist would precisely like to know whether copper enrichment reduces species richness, but also know that the richness of invertebrates can depend on whether the substrate is vertical or horizontal. In order to do this, they made an experiment where species richness was recorded in replicate samples in each of the six combinations of copper enrichment (\\(“None”\\),\\(“Low”\\),\\(“High”\\)) and orientation (\\(“Vertical”\\),\\(“Horizontal”\\)). The experimental design is factorial because all levels of one treatment are represented in all levels of the other treatment (i.e. crossed factors).\nIn consequence, the factorial ANOVA will test whether there are:\n\nany differences in species richness among the three levels of copper enrichment\nany differences in species richness among the two levels of substrate orientation\nany interactions between copper and orientation (i.e. the effect of the copper enrichment depends on the substrate orientation and reciprocally).\n\nWe have three null hypotheses:\n\nthere is no difference between the means for each level of copper enrichment, H0: \\(\\mu_{None}\\)=\\(\\mu_{Low}\\)=\\(\\mu_{High}\\)\nthere is no difference between the means for each level of orientation, H0: \\(\\mu_{Vertical}\\)=\\(\\mu_{Horizontal}\\)\nthere is no interaction between both factors (i.e. if factor effects exist, the factors do not interact)\n\nLet’s perform a two-factor ANOVA, something far better than running two separate single factor ANOVAs that contrast copper effects for each level of the substrate orientation, for 3 reasons:\n\n\nwe have more statistical power (higher degrees of freedom)\n\n\nwe can test whether the main effects interact or not\n\n\nwe reduce the risk of statistical error (i.e. we can’t forget that each time we perform a separate statistical analysis, we get \\(\\alpha\\) and \\(\\beta\\) risks)\n\n\n\n# Dataset import\ndatasessile &lt;- read.table(\"sessile.txt\", dec=\".\", header = TRUE)\ndatasessile$Copper&lt;-as.factor(datasessile$Copper)\ndatasessile$Orientation&lt;-as.factor(datasessile$Orientation)\nstr(datasessile)\n\n'data.frame':   60 obs. of  3 variables:\n $ Copper     : Factor w/ 3 levels \"High\",\"Low\",\"None\": 3 3 3 3 3 3 3 3 3 3 ...\n $ Orientation: Factor w/ 2 levels \"Horizontal\",\"vertical\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Richness   : int  68 64 64 63 69 63 70 68 68 62 ...\n\n# Check for presence of missing values\ncolSums(is.na(datasessile))\n\n     Copper Orientation    Richness \n          0           0           0 \n\n# There is no missing value.\n\n\n\nData exploration\nBefore any statistical analysis, we MUST explore the data in order to prevent any error. Here is the list of explorations to perform before modelling:\n\nCheck presence of outliers in \\(Y\\) and distribution of \\(Y\\) values\nIf \\(X\\) is a quantitative independent variable, check presence of outliers in X and distribution of X values\n\n2b. If \\(X\\) is a qualitative independent variable, analyse the number of levels and the number of individuals per level\n\nAnalyse the potential relationships between \\(Y\\) and the \\(X_{s}\\)\nCheck presence of interactions between \\(X_{s}\\)\nCheck presence of collinearity between \\(X_{s}\\)\n\n\n\nStatistical analysis\nModel building\nFor the statistical modelling, we first analyse the full model (model containing all independent variables and interactions to test).\n\n# Model formulation\nmod1&lt;-lm(Richness~Copper+Orientation+Copper:Orientation,data=datasessile)\n# Comment : a simplest way to write this\nmod1&lt;-lm(Richness~Copper*Orientation,data=datasessile)\n# Then we check for significance\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nRichness ~ Copper * Orientation\n                   Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                           467.0 135.12                      \nCopper:Orientation  2     570.7 1037.7 179.03  32.995 4.341e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe here have a significant interaction between COPPER and ORIENTATION, shown by the test statistic, F value and its associated p-value (Pr(&gt;F)). This means that the effect of one factor (COPPER) depends upon the other (ORIENTATION). In this example, it would mean that the effect of copper enrichment is not consistent between the vertical and horizontal habitats. This complexifies the interpretation of the main effects as a consequence. As the interaction is significant, the full model is the candidate model (i.e. the model containing only significant terms). To understand how factors and their interaction influence the species richness, we must analyse the coefficients of the model.\nModel’s coefficients analysis\n\n# Candidate model formulation\nmod1&lt;-lm(Richness~Copper*Orientation,data=datasessile)\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Richness ~ Copper * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      55.400      0.930  59.573  &lt; 2e-16 ***\nCopperLow                         0.400      1.315   0.304    0.762    \nCopperNone                       14.200      1.315  10.797 4.22e-15 ***\nOrientationvertical             -11.700      1.315  -8.896 3.63e-12 ***\nCopperLow:Orientationvertical    15.100      1.860   8.119 6.35e-11 ***\nCopperNone:Orientationvertical    8.000      1.860   4.301 7.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nThis table detailed the coefficients of the model with coefficients associated with each level of the significant fixed factor. For each factor, one level is called ‘the baseline’, meaning that its coefficient is 0 (also called the reference level).\nFrom this table, coefficients are : COPPER FACTOR\n\n\\(Copper_{High}\\) = 0 (the baseline of the factor COPPER)\n\\(Copper_{Low}\\) = \\(0.4^{NS}\\)\n\\(Copper_{None}\\) = \\(14.2^{***}\\)\n\nORIENTATION FACTOR\n\n\\(Orientation_{Horizontal}\\) = 0 (the baseline of the factor ORIENTATION)\n\\(Orientation_{Vertical}\\)= \\(-11.7^{***}\\)\n\nORIENTATION:COPPER INTERACTION\n\n\\(Copper_{Low}\\) : \\(Orientation_{Vertical}\\) = \\(15.1^{***}\\)\n\\(Copper_{None}\\) : \\(Orientation_{Vertical}\\) = \\(8^{***}\\)\n\nSo, the candidate model is: \\[ Species\\:Richness = 55.4  \\] \\[+ [\\:Copper_{High}=0;\\:Copper_{Low}=0.4^{NS}\\:,\\:Copper_{None}=14.2^{***} ]  \\] \\[ +[Orientation_{Horizontal}=0.0; \\:Orientation_{Vertical}=-11.7^{***}]  \\] \\[ +[Copper_{Low} : Orientation_{Vertical} = 15.1^{***};\\:Copper_{None} : Orientation_{Vertical} = 8^{***}]\\]\nA quick way to help understand an interaction, if we get one, is to examine the interaction plot.\n\n# Interactions graphic\nboxplot(datasessile$Richness~datasessile$Copper*datasessile$Orientation, varwidth = TRUE, ylab = \"Species Richness\", col='blue', main = \"\",cex.axis=0.7)\n\n\n\n\nMultiple comparisons\nIf we’re able to detect any significant differences in the ANOVA, we are then interested in knowing exactly which levels of a given factor differ from one another, and which do not. Remember that a significant p value in the F-test we just ran would reject the null hypothesis where the means were the same across all factor levels, but not identify which were different from each other. Here, we have two factors with their own coefficients:\nORIENTATION FACTOR\n\n\\(Orientation_{Horizontal}\\) = 0 (the baseline of the factor ORIENTATION)\n\\(Orientation_{Vertical}\\)= \\(-11.7^{***}\\)\n\nThose coefficients suggest that the species richness is lowest in vertical habitats.\nCOPPER FACTOR\n\n\\(Copper_{High}\\) = 0 (the baseline of the factor COPPER)\n\\(Copper_{Low}\\) = \\(0.4^{NS}\\)\n\n\\(Copper_{None}\\) = \\(14.2^{***}\\)\n\nThose coefficients suggest that the species richness is highest in absence of Copper enrichment (level \\(None\\) &gt; \\(High\\)). But as the level \\(High\\) is the baseline, we can’t detect whether the levels \\(None\\) and \\(Low\\) are different or not. So, we must change the level baseline and re-analyse the coefficients of the model to detect difference or not between those two factor levels.\n\n# Change the COPPER factor baseline: put 'Low' level as the baseline\ndatasessile$Copper2&lt;-relevel(datasessile$Copper,ref=\"Low\")\n# New model formulation\nmod2&lt;-lm(Richness~Copper2*Orientation,data=datasessile)\n# Coefficients of the model\nsummary(mod2)\n\n\nCall:\nlm(formula = Richness ~ Copper2 * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       55.800      0.930  60.003  &lt; 2e-16 ***\nCopper2High                       -0.400      1.315  -0.304  0.76218    \nCopper2None                       13.800      1.315  10.493 1.21e-14 ***\nOrientationvertical                3.400      1.315   2.585  0.01246 *  \nCopper2High:Orientationvertical  -15.100      1.860  -8.119 6.35e-11 ***\nCopper2None:Orientationvertical   -7.100      1.860  -3.817  0.00035 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nNow, the coefficients of the COPPER factor are: COPPER FACTOR\n\n\\(Copper_{Low}\\) = 0 (the new baseline of the factor COPPER)\n\\(Copper_{High}\\) = \\(-0.4^{NS}\\)\n\\(Copper_{None}\\) = \\(13.8^{***}\\)\n\nThose coefficients suggest that the species richness is highest in absence of Copper enrichment (\\(None\\) &gt; \\(Low\\)). In conclusion, \\(Richness_{Copper_{High}}\\) = \\(Richness_{Copper_{Low}}\\)&lt; \\(Richness_{Copper_{None}}\\)\nModel explanation: R²\nLet’s determine the part of the \\(Y\\) variation explained by the model.\n\n# R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Richness ~ Copper * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      55.400      0.930  59.573  &lt; 2e-16 ***\nCopperLow                         0.400      1.315   0.304    0.762    \nCopperNone                       14.200      1.315  10.797 4.22e-15 ***\nOrientationvertical             -11.700      1.315  -8.896 3.63e-12 ***\nCopperLow:Orientationvertical    15.100      1.860   8.119 6.35e-11 ***\nCopperNone:Orientationvertical    8.000      1.860   4.301 7.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nIn this output, the adjusted R² is equal to 0.8893, which means that about 89% of the variance of species richness is explained by the model.\n\n\nModel validation: Check the assumptions\nSee part III/."
  },
  {
    "objectID": "linear_modelling.html#ancova",
    "href": "linear_modelling.html#ancova",
    "title": "Linear modelling in ecology",
    "section": "2) ANCOVA",
    "text": "2) ANCOVA\n\nINTRODUCTION\nANCOVA (i.e. Analysis of Covariance) is a statistical technique widely used in various disciplines like the previous analysis presented, ANOVA and the linear regression . It is used to model the relationship between a quantitative dependent variable Y, called the response variable, and several independant variable X1, X2… called explanatory variables or predictors). The difference between ANOVA and ANCOVA, is that the predictors can be quantitative and qualitative. For example, we could use an ANCOVA to test whether the size (continuous), sex (categorical), reproductive status (categorical), habitat (categorical), age (continuous) of a sperm whale are good predictors of its weight (the response variable). As the ANCOVA includes quantitative and categorical independent variables, this technique represents between linear regression and ANOVA.\nWe can give an example and write the model with one continuous and one categorical independent variables, the model takes the form: \\[ Y_{ij} = \\mu + \\alpha_{i}+ \\beta.X_{ij}+\\gamma_{i}.X_{ij}+\\epsilon_{ij}\\]\n\n\\(\\mu\\) is the overall mean, the intercept\n\\(\\alpha_{i}\\) is the effect of the modality i of the categorical variable)\n\\(\\beta\\) is the slope (amount of change in Y for each unit of the quantitative covariate), it is the effect of the quantitative variable\\(X_{j}\\)\n\\(\\gamma_{i}\\) is the interactive coefficient between the modality i of the factor and the quantitative covariate\n\\(\\epsilon_{ij}\\) is the error term (residuals). The inclusion of the error term \\(\\epsilon_{ij}\\), also called the stochastic part of the model, that makes the model statistical rather than mathematical. The error term is drawn from a statistical distribution that integers the random variability in the response. In standard linear model, this is assumed to be a normal (Gaussian) distribution with parameter 0 et \\(\\sigma^2\\), avec \\(\\sigma\\) l’écart type.\n\n\n\nANCOVA EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nFor this example, we will use data from a study performed in 1994 that concerns the fly, Anatalanta aptera, a wingless fly living in subantarctic islands, particularly in sea bird colonies. 320 individuals has been collected in the Crozet Island, living either on the coast or inland (i.e. mountain landscape). The goal of that study is to determine which variables may explain the dry weight of Anatalanta aptera. Dry weight represents total organic and inorganic matter in the tissue and is more accurate than measuring wet weight (using dry weight as a measure of animal growth tends to be more reliable). Then, the following life-history traits have been measured to try and explain the response variable Y= Dry weight. Those are the predictors.\n\nSex = the sex of a given individual, categorical variable\n\nHabitat = the habitat of a given individual(‘Coast’ or ‘Inland’), categorical variable\n\nLength = the length of a given individual, continuous variable\n\nWidth = the width of a given individual, continuous variable\n\nWaterContent = the water content of a given individual, continuous variable\n\nFatContent = the fat content of a given individual, continuous variable\n\nDryWeight = the dry weight of a given individual, continuous variable\n\nThe underlying question for this research is simple; do variables drive the dry weight in Anatalanta aptera? Predictors being continuous and categorical, we are gonna perform an ANCOVA.\n\n# Dataset import\ndataFly &lt;- read.table(\"FlyWeight.txt\", dec=\".\", header = TRUE)\n\nWe must check whether the variables are correctly imported, i.e. they are of the right type. Sex and Habitat are ‘chr’, so we must transform them into factors.\n\ndataFly$Sex&lt;-as.factor(dataFly$Sex)\ndataFly$Habitat&lt;-as.factor(dataFly$Habitat)\nstr(dataFly)\n\n'data.frame':   320 obs. of  8 variables:\n $ FlyNumber   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Habitat     : Factor w/ 2 levels \"Coast\",\"Inland\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Length      : num  1.4 1.55 1.52 1.44 1.77 1.69 1.79 1.65 1.58 1.62 ...\n $ Width       : num  1.24 1.39 1.26 1.18 1.46 1.41 1.46 1.41 1.32 1.4 ...\n $ WaterContent: num  5.9 7.1 6.8 5.3 8.2 7.6 8.2 7.2 6.5 8.8 ...\n $ FatContent  : num  0.3 0.2 0 0.2 0.1 0 0 0.3 0.1 0.2 ...\n $ DryWeight   : num  2.2 3 2.2 2 3.1 2.6 2.8 3.1 2.2 2.6 ...\n\n\n\n\nData exploration\nSee part 1).\n\n\nStatistical analysis\nModel building\nFor the statistical modelling, we first analyse the full model (model containing all independent variables to test). We will test the effects of the main effects (3 continuous and 2 categorical independent variables) and their interactions (by excluding interactions between quantitative independent variables). To get the candidate model we will perform a backward selection. It consist in testing the full model first, then we drop the least significant interaction, we perform this step until all the remaining interaction effect are significant. We do the qsame approach for the main effect. By following these two steps, candidate model is found. We use the function drop (F test) to test the significant at each step and choose the lest significant effect. Keep in mind that if a variable is involved in an interaction effect you must keep it as a main effect in the model.\nBackward selection\n\n# Model formulation\n#Full model\nmod1&lt;-lm(DryWeight~ Sex + Habitat + Width + WaterContent + FatContent + Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + Habitat:WaterContent + Sex:FatContent + Habitat:FatContent \n        ,data=dataFly)\n# Then we check for significance\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + \n    Habitat:WaterContent + Sex:FatContent + Habitat:FatContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            83.710 -403.11                      \nSex:Habitat           1    0.1683 83.878 -404.47  0.6172 0.4326813    \nSex:Width             1    0.0076 83.717 -405.08  0.0277 0.8678741    \nHabitat:Width         1    3.0401 86.750 -393.69 11.1492 0.0009443 ***\nSex:WaterContent      1    0.0105 83.720 -405.07  0.0385 0.8445863    \nHabitat:WaterContent  1    3.3319 87.042 -392.62 12.2195 0.0005426 ***\nSex:FatContent        1    0.0033 83.713 -405.10  0.0122 0.9122382    \nHabitat:FatContent    1    0.9549 84.665 -401.48  3.5022 0.0622392 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this significance output, we will exclude the interaction that is ‘the less significant’ = ‘Sex:Fat Content’\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + \n    Habitat:WaterContent + Habitat:FatContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            83.713 -405.10                      \nSex:Habitat           1    0.1673 83.880 -406.46  0.6156 0.4332877    \nSex:Width             1    0.0061 83.719 -407.07  0.0226 0.8805490    \nHabitat:Width         1    3.0926 86.806 -395.49 11.3783 0.0008380 ***\nSex:WaterContent      1    0.0095 83.722 -407.06  0.0349 0.8518861    \nHabitat:WaterContent  1    3.4666 87.180 -394.11 12.7546 0.0004119 ***\nHabitat:FatContent    1    0.9516 84.665 -403.48  3.5013 0.0622693 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this new significance list, we will exclude the interaction that is ‘the less significant’… and so on.\nAfter the non significant interaction deletions, the model contains only 2 significant interactions = ‘Habitat:Width’ and ‘Habitat:WaterContent’. That means that the main effects ‘Habitat’, ‘Width’ and ‘WaterContent’ are maintained in the model as included in significant interactions.\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Habitat:Width + Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value   Pr(&gt;F)    \n&lt;none&gt;                            85.017 -408.15                     \nSex                   1    0.5309 85.547 -408.16  1.9482 0.163775    \nFatContent            1    0.7347 85.751 -407.40  2.6961 0.101602    \nHabitat:Width         1    2.5775 87.594 -400.59  9.4592 0.002287 ** \nHabitat:WaterContent  1    3.5343 88.551 -397.12 12.9705 0.000368 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the previous listing, we decide to first exclude the ‘Sex’ factor.\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Habitat + Width + WaterContent + FatContent + Habitat:Width + \n    Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            85.547 -408.16                      \nFatContent            1    0.6398 86.187 -407.78  2.3409 0.1270272    \nHabitat:Width         1    2.6522 88.200 -400.39  9.7040 0.0020089 ** \nHabitat:WaterContent  1    4.0859 89.633 -395.23 14.9495 0.0001343 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs the ‘FatContent’ is still non significant, we exclude this main effect and we obtain the ‘Candidate Model’. This candidate model contains three main effects (we are keeping the main effect because the variables have interaction effects) and two interactions.\nCandidate model:\n\n# Model formulation\nmod1&lt;-lm(DryWeight~ Habitat+ Width + WaterContent + Habitat:Width + Habitat:WaterContent,data=dataFly)\n\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nDryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            86.187 -407.78                      \nHabitat:Width         1    2.7963 88.983 -399.56  10.188 0.0015570 ** \nHabitat:WaterContent  1    4.2441 90.431 -394.39  15.462 0.0001036 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo understand how main effects and interactions influence the dry weight of the flies, we will analyse the coefficients of the model.\nModel’s coefficients analysis\n\nsummary(mod1)\n\n\nCall:\nlm(formula = DryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent, data = dataFly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.86592 -0.25639 -0.01119  0.23329  1.91099 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.05228    0.66459   0.079 0.937354    \nHabitatInland              -2.43270    0.78331  -3.106 0.002072 ** \nWidth                       0.21761    0.64964   0.335 0.737865    \nWaterContent                0.31332    0.04925   6.362 7.02e-10 ***\nHabitatInland:Width         2.34365    0.73427   3.192 0.001557 ** \nHabitatInland:WaterContent -0.21378    0.05437  -3.932 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 314 degrees of freedom\nMultiple R-squared:  0.6129,    Adjusted R-squared:  0.6067 \nF-statistic: 99.42 on 5 and 314 DF,  p-value: &lt; 2.2e-16\n\n\nThis output presents a table detailing the coefficients of the model with coefficients associated with each significant main effect and interaction. Remind that for a factor, one level is called ‘the baseline’ meaning that its coefficient is 0 (also called the reference level). From this table, coefficients are:\nHABITAT FACTOR\n\n\\(Habitat_{Coast}\\) = 0 (the baseline of the factor Habitat)\n\\(Habitat_{Inland}\\) = \\(-2.43^{**}\\)\n\nWIDTH COVARIATE\n\n\\(\\beta_{Width}\\) = \\(0.21^{NS}\\)\n\nWATER CONTENT COVARIATE\n\n\\(\\beta_{WaterContent}\\) = \\(0.31^{***}\\)\n\nHABITAT:WIDTH INTERACTION\n\n\\(\\beta_{Width_{Inland}}\\) = \\(2.34^{***}\\)\n\nHABITAT:WATER CONTENT INTERACTION\n\n\\(\\beta_{WaterContent_{Inland}}\\) = \\(- 0.21^{***}\\)\n\nModel explanation: R²\nYou can determine the part of the \\(Y\\) variation explained by your model. See the output of the model summary.\n\n#to get R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = DryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent, data = dataFly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.86592 -0.25639 -0.01119  0.23329  1.91099 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.05228    0.66459   0.079 0.937354    \nHabitatInland              -2.43270    0.78331  -3.106 0.002072 ** \nWidth                       0.21761    0.64964   0.335 0.737865    \nWaterContent                0.31332    0.04925   6.362 7.02e-10 ***\nHabitatInland:Width         2.34365    0.73427   3.192 0.001557 ** \nHabitatInland:WaterContent -0.21378    0.05437  -3.932 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 314 degrees of freedom\nMultiple R-squared:  0.6129,    Adjusted R-squared:  0.6067 \nF-statistic: 99.42 on 5 and 314 DF,  p-value: &lt; 2.2e-16\n\n\nIn this output, you get that the adjusted R² = 0.6067. That means that about 61% of the variance of the dry weight of the flies is explained by its relationship with their habitat, water content and width and the interactions between these predictors.\nModel expression\nYou can write the model with the coefficient. Be careful: you have a different expression for each modality. \\[ Dry\\:Weight = 0.05\\:+\\:(Habitat_{Coast} = 0 \\:;\\:Habitat_{Inland} = -2.43^{**}) \\] \\[ +\\:0.21^{NS}. Width\\:+\\:0.31^{***}. Water\\:Content \\] \\[ + (if\\:Habitat=Inland:\\:+ 2.34^{***}. Width\\: -\\: 0.21^{***}.Water\\:Content)\\]\nBiological interpretation\nBefore drawing any conclusions from your result, you have to validate the model (part model validation). The assumptions of ANCOVA are the same as for all general linear models (i.e. regressions, ANOVAs), you have to check independence of residuals, normality of residuals and homogeneity of variances."
  },
  {
    "objectID": "linear_modelling.html#regression",
    "href": "linear_modelling.html#regression",
    "title": "Linear modelling in ecology",
    "section": "3) REGRESSION",
    "text": "3) REGRESSION\n\nINTRODUCTION\nYou have certainly used the technique of regression many times in various exercises. It is used to model the relationship between a quantitative dependent variable \\(Y\\) (the response, that is continuous, and one or several explanatory quantitative variable(s) (the predictors, that are independant). \\(X_{1}\\),\\(X_{2}\\)…\\(X_{p}\\). For example, we could use a linear regression to test whether the weight of a dog (i.e. the explanatory variable) is a good predictor of its lifespan (the response variable).\n\n\nMODEL WRITING\nThe general model is written: \\[ Y_{i} = \\alpha + \\beta_{j}.X{ij}+ \\epsilon_{i}\\] \\[ i=1,...,n\\] \\[ j=1,...,p\\] The inclusion of the error term \\(\\epsilon\\), also called the stochastic part of the model, makes the model statistical rather than mathematical. The error term is drawn from a statistical distribution that integers the random variability in the response. In standard linear regression, this is assumed to be a normal (Gaussian) distribution.\nThere are two different regression types:\n• the simple linear regression includes a single \\(X\\) in your analysis. The model takes the form: \\[ Y = \\alpha + \\beta.X + \\epsilon \\] where \\(\\alpha\\) is the intercept (value of \\(Y\\) when \\(X\\) = 0), \\(\\beta\\) is the regression slope (amount of change in \\(Y\\) for each unit of \\(X\\)), and \\(\\epsilon\\) is the error term (i.e. residuals).\n• the multiple linear regression includes several \\(X_{s}\\) in your analysis. The model takes the form: \\[ Y_{i} = \\alpha + \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}+ \\epsilon_{i}\\]\n\n\nREGRESSION EXAMPLE:\n\n\nDataset presentation and objectives of the analysis\nThis data was published in ‘Latitudinal variation in light levels drives human visual system size’. Eiluned Pearce and Robin Dunbar. Biol. Lett. published online 27 July 2011 (http://doi:10.1098/rsbl.2011.0570). Ambient light levels influence visual system size in birds and primates and Pearce and Dunbar (2011) argue that the same is true for humans. Using linear regression techniques, they want to test the relationship between (absolute) latitude and human orbital volume, an index of eyeball size. Pearce and Dunbar (2011) measured cranial capacity (CC), orbital volume and foramen magnum (FM) dimensions for 73 healthy adult crania from the Oxford University Museum of Natural History and Duckworth Collection, University of Cambridge.\nList of the variables: - MeanOrbitalVolume = Index for eyeball size, continuous variable (the response)\n\nCranialCapacity = measure of the volume of the interior of the cranium, continuous variable\nMinimum_Illuminance = log scale of the minimum of illuminance of the sample site, continuous variable\nMinimum_Temperature = Minimum temperature of the sample site, continuous variable\nAbsoluteLatitude = Absolute values of latitude of the sample, continuous variable\nPopulation = country/region of the sample, Categorical variable\n\nThe response variable is mean orbital volume and the rest are assumed covariates. The Population variable is a descriptor of the samples, so not included in the modelling. Each value for orbital volume represents the mean of 3 replicate measurements from the same skull.\nQuestion: which covariates drive the mean orbital volume?\n\n# Dataset import\ndataHVS &lt;- read.table(\"HumanVisualSystem.txt\", dec=\".\", header = TRUE)\ndataHVS$Population&lt;-as.factor(dataHVS$Population)\nstr(dataHVS)\n\n'data.frame':   55 obs. of  6 variables:\n $ Population         : Factor w/ 12 levels \"Australia\",\"CanaryIslands\",..: 8 8 8 2 2 2 2 11 11 11 ...\n $ AbsoluteLatitude   : num  1.33 5.42 5.42 28.51 28.51 ...\n $ CranialCapacity    : int  1200 1300 1100 1300 1420 1400 1620 1340 1480 1280 ...\n $ Minimum_Illuminance: num  109648 104713 104713 64565 64565 ...\n $ Minimum_Temperature: num  26.7 24.4 24.4 14.4 14.4 ...\n $ MeanOrbitalVolume  : num  22.5 21 22 20.5 25 25.5 27.6 24.5 24 25.3 ...\n\n# Check for presence of missing values\ncolSums(is.na(dataHVS))\n\n         Population    AbsoluteLatitude     CranialCapacity Minimum_Illuminance \n                  0                   0                   0                   0 \nMinimum_Temperature   MeanOrbitalVolume \n                  0                   0 \n\n#There is no missing value.\n\n# Simplify the names of the variables\ndataHVS$Pop&lt;-dataHVS$Population\ndataHVS$Lat&lt;-dataHVS$AbsoluteLatitude\ndataHVS$Capacity&lt;-dataHVS$CranialCapacity\ndataHVS$Illumi&lt;-dataHVS$Minimum_Illuminance\ndataHVS$Temp&lt;-dataHVS$Minimum_Temperature\ndataHVS$Response&lt;-dataHVS$MeanOrbitalVolume      \n\n\n\nData exploration\nSee part 1).\n\n\nStatistical analysis\nModel building\nYou need to first analyse the full model containing all the independent variables in order to test their significance (i.e. see if they are relevant in the modelling) and decide whether you keep them or not.\n\n# Model formulation\nmod1&lt;-lm(Response~Lat+Capacity,data=dataHVS)\n# Then we check for significance, here with an F-Test\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nResponse ~ Lat + Capacity\n         Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   \n&lt;none&gt;                236.18 86.150                    \nLat       1    48.830 285.01 94.486  10.751 0.001863 **\nCapacity  1    28.079 264.26 90.328   6.182 0.016155 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLook at the test statistic, the F value and its associated p-value (Pr). They show that both covariates are significant. So here, no need to go further, because the full model is the candidate model. To understand how covariates influence the response (the orbital volume), we analyse coefficients of the model (i.e. the \\(\\beta_{j}\\)).\nModel’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Response ~ Lat + Capacity, data = dataHVS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3661 -1.5412  0.0534  1.6091  4.4284 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.406367   3.063999   5.028 6.24e-06 ***\nLat          0.048910   0.014917   3.279  0.00186 ** \nCapacity     0.005836   0.002347   2.486  0.01616 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.131 on 52 degrees of freedom\nMultiple R-squared:  0.3701,    Adjusted R-squared:  0.3459 \nF-statistic: 15.28 on 2 and 52 DF,  p-value: 6.034e-06\n\n\nThis table detailed the coefficients of the model with coefficients associated with each covariate. You can deduce the candidate model: \\[ Orbital\\:Volume = 15.4 \\:+\\: 0.04.Latitude\\: +\\: 0.005.Cranial\\:Capacity  \\] Those coefficients suggest that the orbital volume increases with the latitude (in relation with Temperature and Illuminance) and the cranial capacity.\nModel explanation: R²\nLet’s determine the part of the \\(Y\\) variation explained by your model.\n\n# R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Response ~ Lat + Capacity, data = dataHVS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3661 -1.5412  0.0534  1.6091  4.4284 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.406367   3.063999   5.028 6.24e-06 ***\nLat          0.048910   0.014917   3.279  0.00186 ** \nCapacity     0.005836   0.002347   2.486  0.01616 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.131 on 52 degrees of freedom\nMultiple R-squared:  0.3701,    Adjusted R-squared:  0.3459 \nF-statistic: 15.28 on 2 and 52 DF,  p-value: 6.034e-06\n\n\nYou can read the adjusted R² = 0.345. That means that about 35% of the variance of the Orbital Volume is explained by its relationship with the Latitude (Temperature & Illuminance) and the Cranial Capacity.\nModel validation: check the assumptions\nSee III/.\nYou must want to have the best visual representation of your model !\n\n# set your variables on each axis\nx &lt;- dataHVS$Capacity\ny &lt;- dataHVS$Lat\nz &lt;- dataHVS$Response\n# Remind the candidate model with these\nmod1&lt;-lm(z~x+y)\n\n# Create a grid from the x and y values (min to max) and predict values for every point: this is the base of the regression plane\ngrid.lines = 40\nx.pred &lt;- seq(min(x), max(x), length.out = grid.lines)\ny.pred &lt;- seq(min(y), max(y), length.out = grid.lines)\nxy &lt;- expand.grid( x = x.pred, y = y.pred)\nz.pred &lt;- matrix(predict(mod1, newdata = xy),nrow = grid.lines, ncol = grid.lines)\n# Create the fitted points for droplines to the surface\nfitpoints &lt;- predict(mod1)\n\n# Scatter plot with regression plane, in the format that you like\nscatter3D(x, y, z, pch = 19, cex = 1,colvar = NULL, col=\"red\", \n          theta = 30, phi = 10, bty=\"b\",\n          xlab = \"Cranial Capacity\", ylab = \"Latitude\", zlab = \"Response\",  \n          surf = list(x = x.pred, y = y.pred, z = z.pred,  \n          facets = TRUE, fit = fitpoints, col=ramp.col (col = c(\"dodgerblue3\",\"seagreen2\"), n = 300, alpha=0.9),           border=\"black\"), main = \" \")\n\n\n\n\nSo ! Does it look cool to you ?"
  },
  {
    "objectID": "linear_modelling.html#binomial-law",
    "href": "linear_modelling.html#binomial-law",
    "title": "Linear modelling in ecology",
    "section": "1) Binomial Law",
    "text": "1) Binomial Law\n\nINTRODUCTION\nX is a binary variable with two modalities: 1 (success) and 0 (failure). The probability of success is: \\[ P(X = 1) = \\pi \\] Then, Y, a variable corresponding to N randoms and independent draws of X, follows a binomial law whose parameters are N and \\(\\pi\\).\nThe density function of Y is: \\[ f(y;\\pi)=\\binom{N}{y}.\\pi^{y}.(1-\\pi)^{(N-y)}\\] The expectancy and the variance of Y are: \\[E(Y)=N.\\pi\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:var(Y)=N.\\pi.(1-\\pi)\\]\nIn the generalized model, the link function for Y is the function “logit” such that: \\[logit(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\] Thus, the predicted value is: \\[\\mu_{y}= \\frac {e^{\\eta}}{1+e^{\\eta}} \\]\n\n\nBINOMIAL LAW EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nLet’s apply this on the example of the dataset “badger.txt” from the book: Zuur et al. 2009 “Mixed effects models and extensions in ecology with R” - Springer. This dataset comes from a survey carried out on 36 farms in South-West England over 8 consecutive seasons running from autumn 2003 to summer 2005. It contains 277 rows and the columns:\n\nyear: Calendar year\nseason: spring, summer, autumn, winter\nfarm_code: farm identifier\nsurvey: which of the 8 survey occasions (i.e. a time indicator)\nbadger_activity: presence-absence of signs of badgers activity\nN_setts_in_fields: number of badger ‘homes’ observed\nN_buildings: number of buildings on farm\nN_cattle_in_buildings: number of cattle housed in the building yard\naccessible_feed_store_present: presence-absence of a feed’s store in farm\naccessible_cattle_house_present:presence-absence of a direct access to cattle house\naccessible_feed_present: presence-absence of accessible feed on farm\ngrass_silage: presence-absence of grass_silage\ncereal_silage: presence-absence of cereal_silage\nhay_straw: presence-absence of hay_straw\ncereal_grains: presence-absence of cereal_grains\nconcentrates: presence-absence of concentrates\nsugar_beet: presence-absence of sugar beet\nmolasses: presence-absence of molasses\n\nFor the example, the binary variable that will be explained is the badger activity and any other variables are taken as explanatory variables. The objective is to find a model that predict the occurrence of signs of badger activity on farms in order to find a way to reduce the rates of badgers’ visits to farms. This objective is motivated by the numerous transmissions of bovine tuberculosis from badgers to cattle.\nLet’s import the dataset and perform a binomial generalized linear model.\n\n\nData exploration\nSee part I/1).\n\n\nStatistical analysis\nModel building\nWe will use a backward selection with the most complete model, considered here, for a question of simplicity, as the model with all the explanatory variables but no interactions.\n\n# Let's define the model with the function \"glm\" with the family \"binomial\" and the link function \"logit\" \nmod1&lt;-glm(Activity~N_setts + N_buildings + N_cattle + season + feed_store + cattle_house + feed + grass + cereal + straw + grains + concen + sugar,data=dataBadger, family=binomial(link=logit))\n\n# We can use then the function drop1 to check the significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nActivity ~ N_setts + N_buildings + N_cattle + season + feed_store + \n    cattle_house + feed + grass + cereal + straw + grains + concen + \n    sugar\n             Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;            180.19 212.19                     \nN_setts       1   214.98 244.98 34.782 3.687e-09 ***\nN_buildings   1   180.32 210.32  0.127   0.72116    \nN_cattle      1   184.41 214.41  4.217   0.04003 *  \nseason        3   181.15 207.15  0.955   0.81222    \nfeed_store    1   183.73 213.73  3.532   0.06019 .  \ncattle_house  1   181.23 211.23  1.033   0.30942    \nfeed          1   182.53 212.53  2.338   0.12629    \ngrass         1   180.22 210.22  0.021   0.88467    \ncereal        1   180.76 210.76  0.565   0.45242    \nstraw         1   181.42 211.42  1.223   0.26869    \ngrains        1   180.33 210.33  0.132   0.71641    \nconcen        1   180.21 210.21  0.019   0.89129    \nsugar         1   181.57 211.57  1.380   0.24011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSome of the coefficient are not significant because their p values are under 0.05 so we will suppress them and test the new model.\n\n\nSingle term deletions\n\nModel:\nActivity ~ N_setts + N_cattle\n         Df Deviance    AIC    LRT Pr(&gt;Chi)    \n&lt;none&gt;        191.84 197.84                    \nN_setts   1   240.81 244.81 48.969  2.6e-12 ***\nN_cattle  1   195.62 199.62  3.779   0.0519 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis time, all of the variables are significant. Let’s check the coefficient :\n\n# Coefficients of the model\nsummary(mod2)\n\n\nCall:\nglm(formula = Activity ~ N_setts + N_cattle, family = binomial(link = logit), \n    data = dataBadger)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.638947   0.408344  -8.911  &lt; 2e-16 ***\nN_setts      0.268288   0.042921   6.251 4.09e-10 ***\nN_cattle     0.003486   0.001766   1.974   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 246.17  on 277  degrees of freedom\nResidual deviance: 191.84  on 275  degrees of freedom\nAIC: 197.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nAll the coefficients are significant.\nThe candidate model is: \\[  logit(Presence\\:of\\:badger\\:activity) = - 3.64 0.27*Number\\:of\\:Setts\\: +\\: 0.004* Number\\:of\\:Cattles \\]\nModel explanation\nHowever there is no \\(R^2\\) in Generalized Linear Models, we can still calculate a $pseudo:R^2* to estimate how far the candidate model is from the null model by determining the distance between deviance of the null model and the residual deviance of the candidate model.\n\n# Estimate of deviance explained\n(mod2$null.deviance-mod2$deviance)/mod2$null.deviance\n\n[1] 0.2207131\n\n# Some others estimates of deviance explained - package 'rcompanion' =&gt; code lines : nagelkerke(mod2)\n\nFrom these code lines, we deduce that the estimate of deviance explained is 22 %. We found about the same thing with the \\(Pseudo\\:R^2\\) estimate (package ‘rcompanion’).\nModel validation: check the assumptions\nSee III/.\nNota Bene : For this example, the validation will show a dependency that could be resolved by using a mixed model with “Farm_code” as random factor."
  },
  {
    "objectID": "linear_modelling.html#poisson-law",
    "href": "linear_modelling.html#poisson-law",
    "title": "Linear modelling in ecology",
    "section": "2) POISSON LAW",
    "text": "2) POISSON LAW\n\nINTRODUCTION\nHere we will focus on count data. This count data is a positive discrete variable. There are two types of distribution for this type of data. It can follow a Poisson or a Negative Binomial law. We will first focus on a Poisson law.\nFor the Poisson law, we have: \\[ E(y)= Var(y)=\\lambda \\]\nWe can write the distribution under a Poisson distribution as follows: \\[Pr(Y=y)=\\frac{e^{-\\lambda}.\\lambda^y}{y!}\\] \\(y\\) permits to count the number of occurrences and \\(\\lambda\\) is the mean and the variance of the Poisson distribution.\nThe link function of the Poisson law in the Generalized Linear model is log and can be written like that: \\[log(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\]\nBy applying the inverse link function to \\(\\eta\\), we obtain the predicted values of Y: \\[\\mu_{y}= e^{\\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}} = e^{\\eta} \\] Let’s take an example to illustrate the use of the Poisson law:\n\n\nPOISSON LAW EXAMPLE\n###Dataset representation and objectives of the analysis\nThe study of Gotelli and Ellison in 2002 is a good example to apply the Poisson law. It is named “Biogeography at a regional scale: determinants of ants species density in New England bogs and forests”. At each of 22 sites, 25 pitfall traps were set in two 8x8m arrays, one in the center of the bog and one in adjacent upland forest 50 to 500m from the corresponding bog. Traps are treated as 50 independent replicate observations. The data are in the file BogAnts.txt.\n\n# Dataset importation \nants&lt;-read.table(\"BogAnts.txt\", dec = \".\", header = TRUE) \nants$Location&lt;-as.factor(ants$Location)\nstr(ants)\n\n'data.frame':   44 obs. of  6 variables:\n $ Site     : chr  \"ARC\" \"ARC\" \"BH\" \"BH\" ...\n $ Latitude : num  42.3 42.3 42.6 42.6 45 ...\n $ Elevation: int  95 95 274 274 133 133 210 210 362 362 ...\n $ Area     : int  1190 1190 105369 105369 38023 38023 73120 73120 38081 38081 ...\n $ Location : Factor w/ 2 levels \"Bog\",\"Forest\": 2 1 2 1 2 1 2 1 2 1 ...\n $ Nsp      : int  9 8 10 8 6 5 9 4 6 2 ...\n\n# Check for presence of missing values\ncolSums(is.na(ants))\n\n     Site  Latitude Elevation      Area  Location       Nsp \n        0         0         0         0         0         0 \n\n# There is no missing value.\n\nWe have 6 variables. The first one gives the site name. Latitude, Area (of the bog) and Elevation are covariates for each site. Location is a qualitative variable (‘Bog’ or ‘Forest’). The response variable is the last one variable (Nsp) which give the number of ant species found in the traps, in other words the ant species richness.\nFor this research we can wonder: which continuous or categorical variables drive the species richness of ants in bogs from New England ?\n\n\nData exploration\nSee I/1).\nHowever, before continuing, we need to transform the variable ‘Area’, as in the exploration we can see a presence of outliers due to presence of very extensive bogs. We will therefore perform a log-transformation of this covariate.\n\npar(mfrow=c(1,3))\n# Bog Area\n# Cleveland plot\ndotchart(ants$Area,pch=16,col='blue',xlab='Bog Area')\n# Histogram\nhist(ants$Area,col='blue',xlab=\"Bog Area\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(ants$Area,pch=16,col='blue',xlab='')\nqqline(ants$Area,col='red')\n\n\n\n\n\npar(mfrow=c(1,3))\n\n# Log-transformation for the 'Area' variable : \nants$LogArea&lt;-log(ants$Area)\n\n# Log Bog Area\n# Cleveland plot\ndotchart(ants$LogArea,pch=16,col='blue',xlab='LogBog Area')\n# Histogram\nhist(ants$LogArea,col='blue',xlab=\"LogBog Area\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(ants$LogArea,pch=16,col='blue',xlab='')\nqqline(ants$LogArea,col='red')\n\n\n\n\nNow we can use the ‘Area’ variable.\n\n\nStatistical analysis\nModel building\nAs every model building, we will search for the candidate model by first analysing the full model with all the independent variables and their interactions. Then a backward selection will be used to select the best model based on term significance. Successively, the non-significant interactions are deleted, and then the non-significant main effects. However, a non-significant main effect is deleted only if it is non-significant AND not contained in a significant interaction.\nHere we perform a Poisson generalized linear model with these code lines:\n\n# The model is : \nmod1&lt;-glm(Nsp~ Location + Latitude + Elevation + LogArea + Location:Latitude + Location:Elevation+ Location:LogArea, data=ants,family=poisson(link=\"log\"))\n# To check the significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Latitude + \n    Location:Elevation + Location:LogArea\n                   Df Deviance    AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;                  39.973 216.33                 \nLocation:Latitude   1   39.981 214.34 0.00801   0.9287\nLocation:Elevation  1   40.370 214.72 0.39675   0.5288\nLocation:LogArea    1   40.075 214.43 0.10247   0.7489\n\n\nHere any interaction is significant. We delete the less significant interaction : Location:Latitude and do this code lines until there are only significant effects.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Elevation + \n    Location:LogArea\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                  39.981 214.34                      \nLatitude            1   55.094 227.45 15.1134 0.0001012 ***\nLocation:Elevation  1   40.426 212.78  0.4446 0.5049344    \nLocation:LogArea    1   40.078 212.43  0.0974 0.7550075    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant interaction is Location:LogArea. So we delete this interaction and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Elevation\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                  40.078 212.43                      \nLatitude            1   55.176 225.53 15.0976 0.0001021 ***\nLogArea             1   40.273 210.63  0.1943 0.6593240    \nLocation:Elevation  1   40.499 210.85  0.4208 0.5165445    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant interaction is Location:Elevation. So, we delete this interaction and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea\n          Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;         40.499 210.85                      \nLocation   1   70.185 238.54 29.6856 5.081e-08 ***\nLatitude   1   55.621 223.97 15.1221 0.0001008 ***\nElevation  1   49.909 218.26  9.4100 0.0021580 ** \nLogArea    1   40.690 209.04  0.1913 0.6618646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant effect is LogArea. So, we delete this effect and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation\n          Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;         40.690 209.04                      \nLocation   1   70.376 236.73 29.6856 5.081e-08 ***\nLatitude   1   56.649 223.00 15.9590 6.473e-05 ***\nElevation  1   50.284 216.64  9.5938  0.001952 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere all the effects are significant. So we can stop the backward selection and conclude with this the selected model: Nsp~Location + Elevation + Latitude\nIt is necessary to analyse the coefficients of the model in order to understand how the main effects influence the ant species richness in bogs.\nModel’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nglm(formula = Nsp ~ Location + Latitude + Elevation, family = poisson(link = \"log\"), \n    data = ants)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    11.9368121  2.6214970   4.553 5.28e-06 ***\nLocationForest  0.6354389  0.1195664   5.315 1.07e-07 ***\nLatitude       -0.2357930  0.0616638  -3.824 0.000131 ***\nElevation      -0.0011411  0.0003749  -3.044 0.002337 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 102.76  on 43  degrees of freedom\nResidual deviance:  40.69  on 40  degrees of freedom\nAIC: 209.04\n\nNumber of Fisher Scoring iterations: 4\n\n\nSo we can write the model like that: \\[ log(Species\\:Richness) = 11.93 + (Location_{Bog} = 0 ;\\:Location_{Forest} = +0.63^{***})\\:- 0.23^{***}.Latitude\\: -0.001^{***}. Elevation  \\] Be careful here, for a factor, there is a level called “the baseline” which mean that its coefficient is 0. It is the reference level.\nModel explanation\nIn generalized linear models, there is no R2, so we need to calculate a pseudo R2 with the distance between the null model deviance and the residual deviance of the model with this formula: \\[Pseudo\\:R^2=100\\:.\\:\\frac{Null\\:Deviance- Residual\\:Deviance}{Null\\:Deviance}\\]\nLet’s do it with R:\n\n# Pseudo R2 calculation \n(mod1$null.deviance-mod1$deviance)/mod1$null.deviance\n\n[1] 0.6040372\n\n# We can have other pseudo R2 with the package 'rcompanion'\nnagelkerke(mod1)\n\n$Models\n                                                                                  \nModel: \"glm, Nsp ~ Location + Latitude + Elevation, poisson(link = \\\"log\\\"), ants\"\nNull:  \"glm, Nsp ~ 1, poisson(link = \\\"log\\\"), ants\"                              \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                             0.235913\nCox and Snell (ML)                   0.756039\nNagelkerke (Cragg and Uhler)         0.757956\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n      -3     -31.036 62.073 2.1197e-13\n\n$Number.of.observations\n         \nModel: 44\nNull:  44\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\nSo here, the model explains 60.4% of the deviance, but with the others estimate, we found the model explain about 75% of the deviance. Now, it is necessary to check the assumptions of the model to validate. See the part dedicated to this at the end."
  },
  {
    "objectID": "linear_modelling.html#negative-binomial-law",
    "href": "linear_modelling.html#negative-binomial-law",
    "title": "Linear modelling in ecology",
    "section": "3) NEGATIVE BINOMIAL LAW",
    "text": "3) NEGATIVE BINOMIAL LAW\n\nINTRODUCTION\nThe Negative Binomial distribution has been configured in a number of different ways in the statistical literature. Perhaps the most common way to parameter is to see the Negative Binomial distribution arising as a distribution of the number of failures (X) before the r^th success in independent trials, with success probability p in each trial (consequently, r &gt; 0 and 0 &lt; p &gt; 1). In such a case the probability mass function can be expressed as: \\[Pr(X=x|r,p)=\\frac {\\Gamma(x+r)}{x!.\\Gamma(r)}.p^r.(1-p)^x\\]\nand the random variable X has the expectation (theoretical mean): \\[\\mu =\\frac{r.(1 - p)}{p}\\]\nand variance: \\[\\sigma^2 = \\frac{r.(1 – p)}{p^2}\\]\nIn a NB Generalized Linear model, the link function is log so that: \\[log(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\]\nThe predicted values of Y is obtained by applying the inverse link function to \\(\\eta\\). \\[\\mu_{y}= e^{\\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}} = e^{\\eta} \\] The negative binomial model has a NB error structure. This error structure allows, among other things, to correctly specify the relationship between the mean and the variance. This relationship is used by the maximum likelihood approach to estimate the coefficients and standard errors of the generalized linear model parameters.\nLet’s take an example to illustrate the use of the Negative binomial law:\n\n\nNEGATIVE BINOMIAL LAW EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nThe study of Timi and Poulin in 2003 is a good example to apply the Negative binomial law. We will use a subset of the original data. At each of 4 stations, fish sample were collected, with a total of 522 individual of anchovy. These fish were examined for parasites in order to understand the parasite community structure across the host population of anchovy. The data are in the file FishParasite.txt The response variable is “Number”, which represents the total number of parasites found in the fish. It is a count variable. There are 3 explicative variables: “Sex” (of the fish), “Length” (of the fish) and “Area”. Sex and Length are continuous, and Area is a categorical variable with 4 categories (A, B, C or D). Here we only consider the interaction between Length and Area.\n\n# Dataset import\npara &lt;- read.table(\"FishParasites.txt\", dec=\".\", header = TRUE)\npara$Area&lt;-as.factor(para$Area)\npara$Sex&lt;-as.factor(para$Sex)\nstr(para)\n\n'data.frame':   521 obs. of  4 variables:\n $ Area  : Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Length: int  88 93 94 94 94 94 94 94 94 96 ...\n $ Sex   : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 1 1 2 2 2 2 1 ...\n $ Number: int  3 2 8 14 12 4 10 0 2 17 ...\n\n# Missing values ? \ncolSums(is.na(para))\n\n  Area Length    Sex Number \n     0      0      0      0 \n\n# There is no missing value.\n\nLet’s begin quickly with a Poisson model in order to see the problems and then apply a Negative Binomial model.\n\n\nData exploration\nSee part I/1).\n\n\nStatistical analysis\nModel building\nWe perform a backward selection as explained previously, with a Poisson model:\n\n# Model formulation\nmod1&lt;-glm(Number~ Sex + Area + Length + Area:Length ,data=para ,family=poisson(link=\"log\"))\n# Then we check for significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNumber ~ Sex + Area + Length + Area:Length\n            Df Deviance   AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;            10030 12300                     \nSex          1    10066 12334  35.91 2.071e-09 ***\nArea:Length  3    10953 13217 922.60 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod1)\n\n\nCall:\nglm(formula = Number ~ Sex + Area + Length + Area:Length, family = poisson(link = \"log\"), \n    data = para)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.2067538  0.1056462   1.957   0.0503 .  \nSexMale      -0.1041194  0.0173649  -5.996 2.02e-09 ***\nAreaB         1.2976071  0.1447951   8.962  &lt; 2e-16 ***\nAreaC        -2.6680762  0.1814043 -14.708  &lt; 2e-16 ***\nAreaD        -5.4800806  0.2431050 -22.542  &lt; 2e-16 ***\nLength        0.0229176  0.0008601  26.645  &lt; 2e-16 ***\nAreaB:Length -0.0070126  0.0011600  -6.045 1.49e-09 ***\nAreaC:Length  0.0162007  0.0012135  13.350  &lt; 2e-16 ***\nAreaD:Length  0.0316032  0.0015890  19.889  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 19911  on 520  degrees of freedom\nResidual deviance: 10030  on 512  degrees of freedom\nAIC: 12300\n\nNumber of Fisher Scoring iterations: 5\n\n\nHere the interaction between Length and Area is significant, so we keep both variables. And Sex is also significant. So the full model is the candidate model. To see if we can apply the Poisson model, we test the overdispersion. Sometimes, the variance of the response variable may be higher than supposed by the Poisson law. If the parameter is greater than 1.5, we can say that there is overdispersion and the standard errors of the coefficient estimates are biased.\nLet’s check the overdispersion in the model with a DHARMa non parametric dispersion test:\n\n# Overdisperion checking of the Poisson model\n# Scale parameter calculation\nE1 &lt;- resid(mod1, type = \"pearson\") # (Y - mu) / sqrt(mu)\nN  &lt;- nrow(para)\np  &lt;- length(coef(mod1))\nsum(E1^2) / (N - p)\n\n[1] 24.72358\n\n# Use simulations for parameter estimation (package DHARMa)\ntestDispersion(mod1)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 31.727, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe overdispersion index is largely over 1.5, which means that there is an overdispersion in the model. So, the Poisson model can’t be apply and analyse. There are some reasons of overdispersion like for example: the presence of outliers, a dependency, a non linear relationship, the use of the wrong link function…\nWe will change and use a Negative Binomial model and recalculate the overdispersion once we have found the model.\n\n# Model formulation\nmodNB&lt;-glm.nb(Number~ Sex + Area + Length + Area:Length ,data=para)\n# Then we check for significance\ndrop1(modNB,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNumber ~ Sex + Area + Length + Area:Length\n            Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;           581.89 4182.9                     \nSex          1   582.70 4181.7  0.814    0.3669    \nArea:Length  3   637.58 4232.6 55.698 4.874e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have a significant effect for the interaction between Area and Length but not for the main effect Sex. We need to keep the main factor Area and Length and delete the variable Sex, contrary to the Poisson model.\n\n\nSingle term deletions\n\nModel:\nNumber ~ Area + Length + Area:Length\n            Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;           581.90 4181.7                     \nArea:Length  3   636.77 4230.6 54.873 7.308e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, the interaction is still significant, with the two main effect. So we have found the candidate model. Now we need to test the overdispersion, the same way as before.\n\n\n[1] 1.345759\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.86643, p-value = 0.568\nalternative hypothesis: two.sided\n\n\nWe found 1.35, which is below/under 1.5. So we succeed to delete the overdispersion. The DHARMa test validates the absence of overdispersion.\nModel explanation\nNow we can examine the coefficients of the candidate model in order to understand how the number of parasites in fish is influenced.\n\n# Coefficients of the model\nsummary(modNB)\n\n\nCall:\nglm.nb(formula = Number ~ Area + Length + Area:Length, data = para, \n    init.theta = 1.326438087, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.440440   0.459468  -0.959   0.3378    \nAreaB         1.639419   0.652440   2.513   0.0120 *  \nAreaC        -0.613328   0.825456  -0.743   0.4575    \nAreaD        -5.421868   0.718160  -7.550 4.36e-14 ***\nLength        0.028189   0.004059   6.944 3.80e-12 ***\nAreaB:Length -0.010137   0.005593  -1.812   0.0699 .  \nAreaC:Length  0.002096   0.005866   0.357   0.7209    \nAreaD:Length  0.029865   0.005618   5.316 1.06e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.3264) family taken to be 1)\n\n    Null deviance: 1096.8  on 520  degrees of freedom\nResidual deviance:  581.9  on 513  degrees of freedom\nAIC: 4183.7\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.3264 \n          Std. Err.:  0.0866 \n\n 2 x log-likelihood:  -4165.6880 \n\n\nSo, the candidate model is: \\[ log(Number\\:of\\:Parasites) = -0.44 + (Area_{A} = 0 ;\\:Area_{B} = +1.64;\\:Area_{C} = -0.61;\\:Area_{D}  = -5.42)\\] \\[ +\\: 0.028.Length\\:+ (if\\:Area=B: - 0.010. Length;\\:if\\:Area=C: + 0.002. Length ;\\:if\\:Area=C: + 0.029.Length)  \\]\nBe careful here! As we saw previously, there is always a modality of the factor which is the baseline, and every modality of this factor is compared to the baseline. So if a modality of the factor is significant, it means that there is a difference between this modality and the baseline. So in order to test which modality is different from one another, we need to change the baseline.\nAs in every Generalized Linear Models, there is no R2. So we will calculate the pseudo R2 with the formula: \\[Pseudo\\:R^2=100\\:.\\:\\frac{Null\\:Deviance-Residual\\:Deviance}{Null\\:Deviance}\\]\nLet’s perform it on R:\n\n# Estimate of deviance explained\n(modNB$null.deviance-modNB$deviance)/modNB$null.deviance\n\n[1] 0.4694521\n\n# Some others estimates of deviance explained - package 'rcompanion'\nnagelkerke(modNB)\n\n$Models\n                                                                             \nModel: \"glm.nb, Number ~ Area + Length + Area:Length, para, 1.326438087, log\"\nNull:  \"glm.nb, Number ~ 1, para, 0.689574211, log\"                          \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                            0.0820131\nCox and Snell (ML)                  0.5104780\nNagelkerke (Cragg and Uhler)        0.5105620\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n      -7     -186.08 372.16 2.2094e-76\n\n$Number.of.observations\n          \nModel: 521\nNull:  521\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\nModel validation\nSee III/."
  },
  {
    "objectID": "linear_modelling.html#normality-of-residuals",
    "href": "linear_modelling.html#normality-of-residuals",
    "title": "Linear modelling in ecology",
    "section": "Normality of residuals",
    "text": "Normality of residuals\nThe assumption of normality can be checked by producing an histogram and a quantile plot of the residuals. The histogram of residuals should follow a normal distribution. If the points in the quantile plot lie mostly on the red line, the residuals are normally distributed.\n\npar(mfrow=c(1,2))\n# Histogram example 1\nhist(mod2$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality ex 1\")\n# Quantile-Quantile plot\nqqnorm(mod2$residuals,pch=16,col='blue',xlab='')\nqqline(mod2$residuals,col='red')\n\n\n\n# Histogram example 2\nhist(mod1$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality ex 2\")\n# Quantile-Quantile plot\nqqnorm(mod1$residuals,pch=16,col='blue',xlab='')\nqqline(mod1$residuals,col='red')\n\n\n\n# Histogram example 3\nhist(resid,col='blue',xlab=\"residuals\",main=\"Check Normality ex 3\")\n# Quantile-Quantile plot\nqqnorm(resid,pch=16,col='blue',xlab='')\nqqline(resid,col='red')\n\n\n\n\nAs you can see on the first example, the residuals lie mostly on the red line so we can say the residuals are normally distributed.\nHowever, if a small deviation from normality is detected as on the second example, the Fisher test is considered fairly resistant to these deviations and you can still validate the model.\nIf the residuals don’t follow the red line like on the third example, you can’t conclude residuals are normally distributed and validate the model. Transformation can be done to solve the problem but the best is to make another hypothesis on the law of Y. Linear models lie on the assumption that Y is continuous and its distribution is close to follow a Gaussian law. If Y is semi quantitative, binary, or a proportion, the residuals are probably not going to follow a normal distribution, so you have to make another hypothesis on Y law. Therefore you will need to use a GLM."
  },
  {
    "objectID": "linear_modelling.html#homogeneity-of-variances",
    "href": "linear_modelling.html#homogeneity-of-variances",
    "title": "Linear modelling in ecology",
    "section": "Homogeneity of variances",
    "text": "Homogeneity of variances\nThe assumption of homogeneity of variances states that the variation in the residuals is approximately equal across the range of the predictor variables. It can be checked by plotting the residuals against the fitted values and the residuals against the significant main effects. To conclude that the variance is homogeneous, we need to check that it is stable and does not show any patterns.\n\npar(mfrow=c(2,3))\n# residuals vs fitted\nplot(residuals(mod2)~fitted(mod2)\n      , col='blue'\n      , pch=16, main=\"Graph1\")\nabline(h = 0)\n\n# residuals against Habitat factor\nboxplot(residuals(mod1)~ dataFly$Habitat, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Habitat\",\n          main=\"Graph2\")\n\n# residuals vs fitted\nplot(residuals(mod1)~fitted(mod1)\n      , col='blue'\n      , pch=16, main=\"Graph3\")\nabline(h = 0)\n\n# residuals against Castes factor\nboxplot(resid~ dataBombus$Castes, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Castes\",\n         main=\"Graph4\")\nabline(h = 0)\n\n# residuals vs fitted\nplot(resid~fitted(mod3)\n      , col='blue'\n      , pch=16, main=\"Graph5\")\nabline(h = 0)\n\n\n\n\n\nAs you can see in graph 1, the variation in the residuals is approximately equal across the range of the fitted values and there is no pattern, so we can conclude that the variance is homogeneous.\nThe variance of a qualitative variable is studied with a boxplot. For the second graph, we can see that the variance is similar, so we can conclude that it is homogeneous.\nIn the third graph, we can see that the variance is not stable, as we have a funnel shape. So we can’t conclude that it is homogeneous.\nOn the fourth graph, we can see that the variance is not stable, as the boxplots do not have the same height. It is therefore not possible to conclude that they are homogeneous.\nThe fifth graph shows a linear pattern so we cannot conclude positively on the homogeneity of the variance.\n\nStatistical tests are rather resistant to deviations from homogeneity, and can tolerate deviations of around 16 units. If the variance exceeds this threshold, is not stable or shows a pattern, it can be transformed to solve the problem."
  },
  {
    "objectID": "linear_modelling.html#independence-of-residuals",
    "href": "linear_modelling.html#independence-of-residuals",
    "title": "Linear modelling in ecology",
    "section": "Independence of residuals",
    "text": "Independence of residuals\nANCOVA (like other General Linear Models) lies on the assumption that all replicate measures (and so, residuals) are independent of each other. This issue needs to be considered as the design stage. Given the present design, all flies collected are independent as randomly sampled. This assumption is checked. If data are grouped/dependent in any way, then more complex designs are needed to account for additional factors. During the exploration stage, you can detect spatial dependencies with a Moran test on the response variable or spot time dependencies by plotting the autocorrelogram. Thus, you can include the dependency in your model by using mixed models (cf chapter mixte model). If the dependency is still present in the residual of the model, you can’t use general linear model and need to implement specific time (ARIMA …) and space (Spatial Lag model …) models.\nUnlike General Linear Models, Generalized Linear Models don’t require the conditions of homoscedasticity and normality of residuals, but it does require the independence of residuals. We can also check the presence of influential observations (these are the statistical units that have a too large contribution to the model)."
  },
  {
    "objectID": "bayesian_modeles.html",
    "href": "bayesian_modeles.html",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "",
    "text": "History and statement of the theorem: Biostatistics is a mathematical discipline based on statistics applied to living organisms (Vergnault, 2013). In ecology, the mathematical formulation of ecological and biological variables enables in-depth analysis of ecosystem functioning (e.g., population dynamics, quantitative ecology) (Zuur et al., 2007). Since its inception in the 17th century, many theorists have studied the subject, and today biostatistics is at the heart of our understanding of the living world (Williams, 2017). One of them, Thomas Bayes, is at the origin of one of the most widely used branches of biostatistics: Bayesian statistics. This 18th-century English pastor, who wore many hats (mathematics, geometry, theology, etc.), developed a central probability theorem known as Bayes’ Theorem (Droesbeke et al., 2002). The need to create new statistics is linked to Thomas Bayes’ astonishing life experience, as he navigated between belief and science. What’s more, in the eighteenth century, religion was still at the heart of everyday life throughout Europe, regardless of denomination (in this case, the Anglican Church) (Bellhouse, 2004). The mathematician was therefore faced with the question of how to formulate probable scientific hypotheses, including random parameters and admitting the unknown, a priori, of an experiment. Implicitly, this means that it is necessary to consider the results of the test even before exploiting the data (Cornfield, 1967). It is formulated in such a way that for (An) a complete system of events, all of non-zero probability, for any event Y, we have (Traonmilin and Richou, 2018): \\[ P(\\theta \\mid Y) = \\dfrac{P(Y \\mid \\theta) \\cdot P(\\theta)}{P(Y)} \\] It therefore states the relationship between the probability of one event (A) occurring and the probability of another event (B) occurring. Bayesian theory is applied upstream of the drafting of statistical hypotheses, hence the a priori terminology often used to describe this approach (Puga et al., 2015). The benefits of this technique for ecology: Bayesian statistics were shelved for almost two centuries, as they were too complex to perform and required a great deal of computing power. However, it was in the 1970s and 1980s that Bayesian statistics enjoyed a new lease of life (Traonmilin and Richou, 2018). This new impetus came at a time when the scientific world was becoming increasingly aware of ecological issues (Zimmermann, 2020). Recent discoveries in computing and computing power also made it possible to carry out more advanced analyses, and thus to integrate the use of Bayesian statistics. The democratization of the use of predictive models also contributes to the spread of these probability calculations (Manabe and Smagorinsky, 1967). In the field of ecology, one of the main difficulties in analysing processes operating in ecosystems is the complexity of interactions and the abundance of variables. These same variables are not always observable, and their influence can escape the observer’s gaze (Zuur et al., 2007) . This form of statistics thus makes it possible to create a link between non-visible variables and their observable repercussions on the environment. The main objective of the Bayesian approach is therefore to assess the probability that there is an effect of these unperceived variables (Wikle, 2003). In this study, we attempt to take stock of the use of Bayesian methods in ecology and what they can offer in comparison with so-called “frequency” statistics, through concrete examples using modeling tools."
  },
  {
    "objectID": "bayesian_modeles.html#explanation-in-the-special-case-of-conjugation-between-prior-and-posterior",
    "href": "bayesian_modeles.html#explanation-in-the-special-case-of-conjugation-between-prior-and-posterior",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Explanation in the special case of conjugation between prior and posterior",
    "text": "Explanation in the special case of conjugation between prior and posterior\nFor context first, let’s take a simple example to explain the idea behind the Bayesian method. We want to estimate the mean abundance per \\(m^2\\) of one fungus in a forest. To do that, we set up some sampling areas in which we count the number of mushrooms. We can simulate the data by taking random observations in a Poisson distribution. Let’s suppose that our study has 200 sampling area. Because it is count data, the number of mushrooms that we will count should follow a Poisson distribution :\n\nn_sample = 200 # sampling area\nlbda = 5 # mean of poisson distribution\npois_distr = dpois(1:20, lambda = lbda)\nplot(pois_distr,type =\"h\",\n     lwd = 2, col = 'blue',\n     xlab = \"Mushroom Count\",\n     ylab = expression(paste( 'Density or ','[y]')) )\n\n\n\n\nTo simulate the sampling campaign we are taking values in this Poisson distribution. In our case, those will be used to estimate the mean number of mushrooms in the forest. (\\(\\lambda = 5\\) is already known because we simulate the data, but in reality, this is an unknown).\n\nset.seed(1000)\nY = rpois(n = n_sample, lambda = lbda)\nhist(Y)\n\n\n\n\nSo let’s pretend that we don’t know the \\(\\lambda\\). We want to find the the value of the mean number of mushrooms we will call \\(\\hat{\\lambda}\\) and we also want to know the probability of this \\(\\hat{\\lambda}\\). The Bayesian method will give us a range of estimated mean \\(\\hat{\\lambda}\\) and the probability of those values to be true knowing the observation \\(Y\\). This is what we call the posterior distribution. We can simply write this as follows \\([\\hat{\\lambda} \\mid Y]\\) which is the probability of \\(\\hat{\\lambda}\\) knowing our observations. This probability can be found with the equation : \\[ [\\hat{\\lambda} \\mid Y] = [Y \\mid \\hat{\\lambda}  ]\\cdot[\\hat{\\lambda}] \\] The two components of the right-hand side of the equation are \\([Y \\mid \\hat{\\lambda} ]\\) the likelihood of our data and \\([\\hat{\\lambda}]\\) the prior distribution. First, let’s start with the likelihood. Our data follow a Poisson distribution so our likelihood will follow a Poisson distribution : \\[ L(\\hat{\\lambda} ; y)=[y \\mid \\hat{\\lambda}] = \\frac{e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y}}{y!} \\] This is a first good step, but there is a small issue here, this formula isn’t completely usable in this form. This is because it can only take one observation. In English words, it is like asking what is the probability of one observation (one count of mushroom) given a model with a mean \\(\\hat{\\lambda}\\). This form isn’t powerful enough because it uses only one observation. What we want is to use all the data that we have, we want to know the probability of all the observations given a model with a mean \\(\\hat{\\lambda}\\). To do so we can write the likelihood of all our data \\(Y\\) as the product of the likelihood of each observation \\(y_i\\). (We are allowed to do this only because observations are independent) \\[ \\begin{align} [Y \\mid \\hat{\\lambda}] &= \\prod^{n}_{i=1}[y_i \\mid\n\\hat{\\lambda}] \\\\ &=\\prod^{n}_{i=1}\\frac{e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y_i}}{y_i!} \\\\ &\\propto \\prod^{n}_{i=1}e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y_i}\\\\ \\end{align} \\] As you can see we don’t keep the \\(\\frac{1}{y_i!}\\), it is because we are only interested in the terms that are impacted by \\(\\hat{\\lambda}\\). The last form which is proportional to the likelihood function has a more convenient form for the next step. Let’s rearrange the function in a more convenient form \\[ \\begin{align} \\prod^{n}_{i=1}e^{-\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{y_i}&=\ne^{-n \\cdot\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{\\sum^{n}_{i=1} y_i}\\\\ &= e^{-n\n\\cdot\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{n \\cdot \\bar{y}}\\\\ \\end{align} \\] The first line just uses the power/exponential multiplication properties. The second line is just a writing simplification that is common in other resources, \\(mean(y)=\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n \\Rightarrow n\\cdot \\bar{y} = \\hat{\\lambda}^{\\sum^{n}_{i=1} y_i}\\). Now we want to find the prior distribution of \\(\\hat{\\lambda}\\). First, we have to choose the distribution family of our prior. We will use a Gamma distribution. We are using this one because it lets the prior and the posterior have the same distribution family, it is called conjugate distributions. The prior is called a conjugate prior for the likelihood function. This means that the prior function and the likelihood function have the same form! And this means that we can simplify! let’s try it : \\[ \\begin{align} [\\hat{\\lambda}]&\\sim Gamma(\\lambda,\\alpha_p,\\beta_p) \\\\ &=\n\\lambda^{\\alpha_p -1}\\frac{\\beta_p^\\alpha\\cdot e^{-\\beta_p\n\\lambda}}{\\Gamma(\\alpha_p)} \\propto \\lambda^{\\alpha_p -1} \\cdot e^{-\\beta_p\n\\lambda} \\end{align} \\] Same as the likelihood, we are only interested in the term that varies with \\(\\hat{\\lambda}\\) so we remove \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\) and keep the proportional formula of the prior. We can now find the real formula of our posterior distribution. \\[ \\begin{align} [\\lambda \\mid y] &= [y \\mid \\lambda]\\cdot[\\lambda] \\\\ [\\lambda\n\\mid y] &\\propto e^{-n \\cdot\\lambda} \\cdot\\lambda^{n \\cdot \\bar{y}} \\cdot\n\\lambda^{\\alpha_p -1} \\cdot e^{-\\beta_p \\lambda}\\\\ [\\lambda \\mid y] &\\propto\ne^{-\\beta_p \\lambda-n\\lambda} \\cdot\\lambda^{n\\bar{y}+\\alpha_p -1} \\\\ [\\lambda\n\\mid y] &\\propto e^{-\\lambda (\\beta_p +n)} \\cdot\\lambda^{n\\bar{y}+\\alpha_p -1}\\\\\n\\end{align} \\] Does the last formula remind you of something familiar? That’s right it is a Gamma distribution! This is the magic of the conjugate distributions. We can now write : \\[ \\begin{align} [\\lambda \\mid y] &\\propto e^{-\\lambda \\beta}\n\\cdot\\lambda^{\\alpha -1}\\\\ [\\lambda \\mid y] &\\sim Gamma(\\alpha_p +n\\bar{y},\n\\beta_p +n) \\end{align} \\] We can do some simulations to show the results. We are looking for the probability of \\(\\hat{\\lambda}\\), so for the computation we create a vector of all \\(\\hat{\\lambda}\\) for which we want to know the probability:\n\nlambda_hat &lt;- seq(0,10, by = 0.01)\n\nAnd now, in order to compare them, we compute the distribution of the prior and the posterior (which are both following a gamma distribution) with an increasing amount of sample:\n\nalph = 1\nbet= 1\npar(mfrow = c(2,3))\nn_obs = 0\nfor(n_obs in list(0,1:5,1:10,1:50,1:100,1:200)){\n  # prior distribution\n  l_prior = dgamma(lambda_hat, shape = alph, rate = bet)\n  # posterior distribution\n  l_post = dgamma(lambda_hat, shape =  alph + sum(Y[n_obs]), rate = bet + max(n_obs))\n  plot(lambda_hat,l_prior, ylim = c(0,max(c(l_post,l_prior))),\n       type = 'l', lwd = 2, col = 'orange',\n       xlab = expression(lambda),\n       ylab = expression(paste('[', lambda, '|y]')) )\n  lines(lambda_hat,l_post, type = 'l',lty = 3, lwd = 2, col = 'purple')\n  abline(v = 5, lty = 2, lwd = 2)\n  title(paste(\"n = \",max(n_obs)))\n}\n\n\n\n\nWhen we add no data in the computation of the posterior, it is normal that we don’t see any modifications from the prior. Adding 5 observation already bring some good information, the prior and the posterior have no longer the same shape and we have a better estimation of the true \\(\\lambda\\). Increasing the number of data gives us a better approximation of the true mean. The density of probability is also higher with a lot of data because we have more confidence in the approximation."
  },
  {
    "objectID": "bayesian_modeles.html#the-role-of-prior-knowledge",
    "href": "bayesian_modeles.html#the-role-of-prior-knowledge",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "The role of prior knowledge",
    "text": "The role of prior knowledge\nIn Bayesian statistics, prior knowledge and beliefs play a central role in the formulation and interpretation of Bayesian models. As explained before, the fundamentals of Bayesian inference lies in combining prior information with observed data to obtain updated or posterior probabilities.\nBayesian analyses differ from frequentist analyses by incorporating prior information via conditional probabilities, known as Bayes’ rule (Nathan P. Lemoine, 2019): \\[\\frac{[\\theta]\\cdot[Y\\mid\\theta]}{[Y]}=[\\theta\\mid Y]\\] With Pr(θ) = the probability of the parameter or hypothesis θ based on prior information Pr(Y|θ) = the likelihood of the data conditioned on the hypothesis Pr(Y) = the normalization constant Pr(θ|Y) = the posterior probability of the of the hypothesis conditioned on the observed data.\nIncorporating existing information to a data set can be based on previous studies, expert opinions, historical data or simply known subjective beliefs. It will allow the future model to avoid over-fitting and favor a more plausible and simple estimation."
  },
  {
    "objectID": "bayesian_modeles.html#informative-and-non-informative-priors",
    "href": "bayesian_modeles.html#informative-and-non-informative-priors",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Informative and non-informative priors",
    "text": "Informative and non-informative priors\nUse of informative priors: In practice, we want to base our decisions on all available information. Therefore, it is considered responsible to include informative priors in applied research whenever possible. Priors allow you to combine information from the literature, from the data, or to combine information from different datasets.\nUse of non-informative priors: In basic research where the results should only reflect the information in the current dataset. Results from a case study can be used in a meta-analysis which assumes independence between the different included studies.\nAn illustration of how two different priors affect the posterior distribution in a Beta-Binomial model:\n\n# Setup the aesthetic of future plot\nmain_theme = theme_bw()+\n  theme(line = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        panel.border = element_blank(),\n        axis.ticks =  element_line(colour = \"black\"),\n        axis.text.x = element_text(colour = \"black\", size=14,\n                                   angle = 45, hjust = 1),\n        axis.text.y = element_text(colour = \"black\", size=14),\n        legend.title = element_text(colour = \"black\", size=14),\n        legend.title.align=0.5,\n        legend.text = element_text(colour = \"black\", size=14),\n        axis.title=element_text(size=22),\n        strip.background = element_rect(fill=\"white\"))\n\n\n# for non-informative uniform prior\n# Define the range of theta values\ntheta &lt;- seq(0, 1, by = 0.01)\n# Define the number of trials and successes\nn &lt;- 10\nx &lt;- 5\n# Calculate likelihood, prior, and posterior for non-informative uniform prior\n\nlikelihood &lt;- dbinom(x = x, size = n, prob =theta)\nprior_uniform &lt;- dunif(min = 0, max = 1,x = theta)\nposterior_uniform &lt;- likelihood * prior_uniform \n# Create data frame for plotting\ndf_uniform &lt;- data.frame(theta, likelihood, prior_uniform, posterior_uniform)\n# Plotting the graph \ngraph1 &lt;- ggplot(data = df_uniform, aes(x = theta)) +\n  geom_line(aes(y = posterior_uniform, color = \"Posterior\"), linewidth = 1) +\n  geom_line(aes(y = likelihood, color = \"Likelihood\"), linetype = \"dashed\", linewidth = 1) +\n  geom_line(aes(y = prior_uniform, color = \"Prior\"), linewidth = 1) +\n\n  labs(title = \"With Non-informative Uniform Prior\",\n       x = \"Theta (Probability of Success)\",\n       y = \"Density\",\n       color = \"Legend\") +\n  scale_color_manual(values = c(\"black\",'purple', \"orange\"),\n                     labels = c(\"Likelihood\",\"Posterior\", \"Prior\"))+\n  main_theme \n\n## for highly informative uniform prior\n\n# Calculate new prior, and posterior for highly informative prior\n\nprior_informative &lt;- dbeta(theta, 2, 2)\nposterior_informative &lt;- likelihood * prior_informative\n# posterior_rescaled &lt;- posterior_informative * (max_likelihood / max_posterior) * scale_factor_posterior\n# # Create data frame for plotting\ndf_informative &lt;- data.frame(theta, likelihood, prior_informative, posterior_informative)\n# Plotting the graph \ngraph2 &lt;- ggplot(data = df_informative, aes(x = theta)) +\n  geom_line(aes(y = posterior_informative, color = \"Posterior\"), linewidth = 1) +\n  geom_line(aes(y = likelihood, color = \"Likelihood\"), linetype = \"dashed\", linewidth = 1) +\n\n  geom_line(aes(y = prior_informative, color = \"Prior\"), linewidth = 1) +\n\n  labs(title = \"With Highly Informative Prior\",\n       x = \"Theta (Probability of Success)\",\n       y = \"Density\",\n       color = \"Legend\") +\n\n  scale_color_manual(values = c(\"black\", 'purple', \"orange\"),\n                     labels = c(\"Likelihood\", \"Posterior\", \"Prior\"))+\n  main_theme\n\ngraph1\n\n\n\ngraph2\n\n\n\n\nHere we illustrate the posterior distribution with a non informative prior is equal to the likelihood. But with knowledge, we can take an informative prior,it is shown on the second figure where we have a strong prior center on a value of success of 0.5, thus the posterior isn’t equal to the likelihood anymore. This emphasizes that the impact of the chosen prior influences the posterior distribution with both its shape and the quantity of information it holds compared to the information within the data."
  },
  {
    "objectID": "bayesian_modeles.html#metropolis-hastings-algorithm-implementation",
    "href": "bayesian_modeles.html#metropolis-hastings-algorithm-implementation",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Metropolis-Hastings algorithm : implementation",
    "text": "Metropolis-Hastings algorithm : implementation\nLet’s implement the algorithm to try to understand how it works. In the following lines, we will describe the various steps of the Metropolis-Hastings algorithm along with the corresponding R code lines.\n\nStep 1 : Definition of the likelihood function and the prior law\nAs with the beginning of any Bayesian statistical analysis, we first pose the problem and define the likelihood probability distribution and the prior distributions of the parameters to be estimated. We define the likelihood as a binomial distribution with parameters \\(N\\) as the total population size and \\(p\\) as the probability of capturing individuals. It is this parameter \\(p\\) that we aim to estimate. We make the assumption that we have no prior information about this parameter, so we choose a non-informative prior: \\[ p \\sim \\beta(1,1) \\]\n\n# [y | p]\nlikelihood&lt;- function(p, y, N){\n  if(p &lt; 0 | p &gt; 1) {\n    return(0)\n  } else {\n    return(dbinom(x = y, size = N, prob = p))\n  }\n}\n\n\n# [p]\nprior.dist = function(p, a.prior = 1, b.prior = 1){\n  dbeta(x = p, shape1 = a.prior, shape2 = b.prior)\n}\n\n\n\nStep 2 : Definition of a candidate position\nWe will now aim to move, so we need to define a function to determine a candidate position. Here, we randomly draw this position from a normal distribution with a mean of \\(p_c\\), which is the current position, and we arbitrarily choose a standard deviation value.\n\nmove  &lt;- function(p, sd.explore = 0.1){\n  candidate &lt;- rnorm(1, mean = p, sd = sd.explore)\n  return (candidate)\n}\n\n\n\nStep 3 : Compute of the ratio\nOnce we have this candidate position, we need to decide whether to keep it or not. The decision criterion to calculate is the Metropolis-Hastings ratio \\(r\\), which we define as: \\[ r = \\dfrac{[p_{t+1} \\mid Y]\\space \\cdot \\space [p_{t+1}] \\space \\cdot \\space\ng(p_{t+1} \\mid p_{t})}{[\\lambda_{t} \\mid Y]\\space \\cdot \\space [p_{t}] \\space\n\\cdot \\space g(p_{t} \\mid p_{t+1})} \\] where \\(g(p_{t+1} \\mid p_{t})\\) is the probability of transitioning from the candidate position to the current position.\n\nproba_move &lt;- function(p1, p2, sd.explore = 0.1){\n  dnorm(p1, mean = p2, sd = sd.explore)\n}\n\n\nMH.ratio &lt;- function(p_c,p, y, N){\n  ratio = (likelihood(p_c, y, N) * prior.dist(p) * proba_move(p, p_c))/\n    (likelihood(p, y, N) * prior.dist(p) * proba_move(p_c, p))\n  return(ratio)\n}\n\n\n\nStep 4 : Decide if we go to the candidate position or not\nTo choose if we keep the candidate position \\(\\lambda_{t+1}\\) we define \\(u\\) : \\[ u \\sim unif(0,100) \\] If \\(u\\) is greater than the ratio \\(r\\), we remain at the current position; conversely, if \\(u\\) is less than \\(r\\), we transition to the candidate position.\n\n# parameter algorithm\nn_iter = 10000\nthin = 10\n#data\ny = 3\nN = 10\n#initialization\np_init = 0.5\np_sample = rep(NA, n_iter)\np_save = rep(NA, n_iter)\np_sample[1] = p_init\ni=2\nfor(i in 2:n_iter){\n  p = p_sample[(i-1)]\n  p_c = move(p)\n  ratio = MH.ratio(p_c,p, y, N)\n  if(runif(1)&lt;ratio){\n    p_sample[i] = p_c\n  }else{\n    p_sample[i] = p\n  }\n  }\np_save = p_sample[seq(1,n_iter, by= thin)]\ndata = data.frame(iteration = 1:length(p_save),\n                    step = p_save)\n\nThere is a parameter we haven’t discussed in this function, and that’s \\(thin\\). It means that we will only save the samples within the chain at intervals of \\(thin\\). This is due to the fact that the samples are correlated with each other as they depend on the previous position and therefore do not accurately reflect the distribution. Consequently, if we want independent samples, we must discard the majority of samples and keep only one sample every \\(thin\\) steps, with \\(thin\\) being “sufficiently large.”\n\n\nStep 5 : Visualization\n\nplot(step~iteration, data, \"l\")\n\n\n\np1 &lt;- ggplot(data = data)+\n  geom_histogram(aes(x = step))+\n  theme_bw()\np1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nalph = 1\nbet= 1\n\nBoth graphs represent the sampling of \\(p\\) in its marginal posterior distribution. This is possible when a sufficient number of iterations is used, allowing the Markov chain to reach a stationary state."
  },
  {
    "objectID": "bayesian_modeles.html#the-case-of-multiple-parameters",
    "href": "bayesian_modeles.html#the-case-of-multiple-parameters",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "The case of multiple parameters",
    "text": "The case of multiple parameters\nAnd how does it work in multiple dimensions? Obviously, above two parameters to estimate, visualization becomes impossible. So, to keep it visual, we will only perform an example with one additional parameter to estimate. We will use a simple example: estimating a population mean and standard deviation. We’ll define some population-level parameters, collect some data, then use the Metropolis-Hastings algorithm to simulate the joint posterior of the mean and standard deviation. Let’s start by simulating some data.\n\n# population level parameters\nmu &lt;- 7\nsigma &lt;- 3\n# collect some data (e.g. a sample of heights)\nn &lt;- 50\nx &lt;- rnorm(n, mu, sigma)\n\nThen, as previously, we define the likelihood and the prior distributions for the parameters to be estimated \\(\\mu\\) and \\(\\sigma\\): \\[ Y \\sim N(\\mu,\\sigma) \\]\n\n# likelihood function\nll &lt;- function(x, muhat, sigmahat){\n  sum(dnorm(x, muhat, sigmahat, log = T))\n}\n\n\\[ \\mu \\sim N(0,100) \\] \\[ \\sigma \\sim unif(0,10) \\]\n\n# prior\npmu &lt;- function(mu){\n  dnorm(mu, 0, 100, log = T)\n}\npsigma &lt;- function(sigma){\n  dunif(sigma, 0, 10, log = T)\n}\n\nIn the context of the Metropolis-Hastings algorithm, the focus is typically on probability ratios rather than the probabilities themselves. Logarithms simplify calculations and improve numerical stability in this context, especially for the ratio calculation. Indeed, as mentioned earlier, we performed: \\[ r_t = \\dfrac{[\\theta_{t+1} \\mid Y]\\space \\cdot \\space [\\theta_{t+1}] \\space\n\\cdot \\space g(\\theta_{t+1} \\mid \\theta_{t})}{[\\theta_{t} \\mid Y]\\space \\cdot\n\\space [\\theta_{t}] \\space \\cdot \\space g(\\theta_{t} \\mid \\theta_{t+1})} \\] Which is equal to : \\[ log(r_t) = [\\theta_{t+1} \\mid Y]\\space \\cdot \\space [\\theta_{t+1}] \\space +\n\\space g(\\theta_{t+1} \\mid \\theta_{t}) - [\\theta_{t} \\mid Y]\\space \\cdot \\space\n[\\theta_{t}] \\space + \\space g(\\theta_{t} \\mid \\theta_{t+1}) \\]\n\n# for the compute of the ratio\n# poseterior\npost &lt;- function(x, mu, sigma){\n  ll(x, mu, sigma) + pmu(mu) + psigma(sigma)\n}\n# Probabilité de transition multivariée (loi normale)\ntransition_prob &lt;- function(theta_candidate, theta_current, proposal_sd = 0.1) {\n  dnorm(theta_candidate, mean = theta_current, sd = proposal_sd, log = TRUE)\n}\n# to compute a new candidate\njump &lt;- function(x, dist = .2){ # must be symmetric\n  x + rnorm(1, 0, dist)\n}\n\n\nAlgorithm implementation\n\niter = 10000\ntheta.post &lt;- data.frame(mu = rep(NA,iter), sigma = rep(NA,iter))\ntheta.post[1, 1] &lt;- 9\ntheta.post[1, 2] &lt;- 5\nfor (t in 2:iter){\n    # theta_star = proposed next values for parameters\n    theta_star &lt;- c(jump(theta.post[t-1, 1],0.1),jump(theta.post[t-1, 2],0.1))\n    #ratio\n    pstar &lt;- post(x, mu = theta_star[1], sigma = theta_star[2])\n    pprev &lt;- post(x, mu = theta.post[t-1, 1], sigma = theta.post[t-1, 2])\n    r &lt;- (pstar+transition_prob(c(theta.post[t-1,1],theta.post[t-1,2]), theta_star, 0.1) )-(pprev+transition_prob(theta_star, c(theta.post[t-1,1],theta.post[t-1,2]), 0.1))\n    ratio = exp(r[1])\n    # Acceptation or not\n    if(runif(1)&lt;ratio){\n      theta.post[t, ] &lt;- theta_star\n    } else {\n      theta.post[t, ] &lt;- theta.post[t-1, ]\n    }\n}\n\n\n\nVisualization\n\nxlims &lt;- c(4, 10)\nylims &lt;- c(1, 6)\n  par(mfrow=c(1, 2))\n  plot(theta.post[ 1:1000,1], theta.post[1:1000,2],\n       type=\"l\", xlim=xlims, ylim=ylims, col=\"blue\",\n       xlab=\"mu\", ylab=\"sigma\", main=\"Markov chains\")\n  text(x=7, y=1.2, paste(\"Iteration \", 1000), cex=1.5)\n  sm.density(x=cbind(c(theta.post[1:1000,1]), c(theta.post[1:1000,2])),\n             xlab=\"mu\", ylab=\"sigma\",\n             zlab=\"\", zlim=c(0, .7),\n             xlim=xlims, ylim=ylims, col=\"white\",\n             verbose=0)\n\nWarning in rgl.init(initValue, onlyNULL): RGL: unable to open X11 display\n\n\nWarning: 'rgl.init' failed, running with 'rgl.useNULL = TRUE'.\n\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nWarning in persp.default(xgrid, ygrid, dgrid, xlab = opt$xlab, ylab = opt$ylab,\n: surface extends beyond the box\n\n  title(\"Posterior density\")\n\n\n\n\nThe figure 1 represents the joint sampling of the two parameters. At each iteration, a new pair of parameters is proposed, and the ratio calculation assigns a weight to the proximity of the candidate parameter pair to the prior. Figure 2 represents the joint distribution of the two parameters formed by the sampling. It is from this joint distribution that the marginal posterior distributions of the two parameters arise. Therefore, we can also calculate the statistics (mean, standard deviation, etc.) of interest on these marginal posterior distributions."
  },
  {
    "objectID": "bayesian_modeles.html#using-jags",
    "href": "bayesian_modeles.html#using-jags",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Using JAGS",
    "text": "Using JAGS\n\ntwo parameter example\nFor this example, we are going to use data taken from a normal distribution. We will estimate the mean and the standard deviation like we did before. But for this example, we will use the software JAGS with the library rjgas. JAGS is a software that lets us build our model to estimate the posterior distribution of parameters that we want. We start by creating the data:\n\nset.seed(1000)\nY2 = rnorm(n = 200, mean = 10, sd = 1)\nhist(Y2)\n\n\n\n\nWe can suppose here that we want to estimate the mean length and length variation of a snakes species in one dessert. So we sample 200 snakes and measure them, Y2 contain all our samples.\nrjags need 3 main component. Fist, the model:\n\nmod1 = \"\n  model{\n  ## model\n  for(i in 1:n_obs){# loop when there is multiple data. (will compute the product of the likelihood)\n    Y2[i] ~ dnorm(m,preci) # likelihood of our data given parameters\n  }\n  ## Prior distribution\n  # All parameters that we want to estimat need a prior distibution\n  m ~ dunif(0, 10**3)\n  sd ~ dunif(0, 10**3)\n  preci &lt;- 1/(sd*sd) # In JAGS dnorm take the precision as input not the standard deviation\n  }\n\"\n\nIt looks like R syntax but it is JAGS syntax, which is slightly different for some function. In the model, we setup the prior for each parameter that we want to estimate with m ~ dunif(0, 10**3) and sd ~ dunif(0, 10**3).The uniform probability law is used because we suppose that we don’t have any prior knowledge on the mean and the standard deviation. We give to the model a range of possible value between 0 and 1000 for the mean and the standard deviation. In general for uniform prior we select large upper and lower bound to be sure that the value is included in it. The likelihood is then compute with Y2[i] ~ dnorm(m,preci)). JAGS is not like R because it takes the precision as parameter for the dnorm. Then we loop over all observation, RJAG will automatically make the product of the likelihood of all the data. Our model is complete, we have the priors and the likelihood formula setup to compute the posterior distribution. Second component needed for rjags, the list of data:\n\ndata_list1 = list(\n  Y2 = Y2,\n  n_obs= length(Y2)\n)\n\nThis list contains all information needed to make the model run, nothing really interesting to say here. Third, the list of initialization:\n\ninit_list1 &lt;- list(\n  m = 5,\n  sd = 2\n)\n\nJAGS will probably run MCMC algorithm to find the best parameters, those algorithm need values to start. Try to give values that make sense. If you don’t, the algorithm might take more time to converge or JAGS could also give error messages. Now that everything is setup, we can run the model.\n\nmjags1 &lt;- jags.model(file=textConnection(mod1),\n                     data=data_list1,\n                     inits=init_list1,\n                     n.chains = 3) # Number of MCMC\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 2\n   Total graph size: 210\n\nInitializing model\n\n\nWe choose to compute 3 Markov chains.\n\n# function to extract mcmc\npostSamples1 &lt;- coda.samples(mjags1,\n                             variable.names = c(\"m\",\"sd\"),\n                             n.iter = 10000,\n                             thin = 10)\npostSamples_df1 &lt;-  postSamples1 %&gt;% ggs()\n\nAlways verify if the markov chain are stationary. (otherwise, you will have a bad estimation of the posterior distribution)\n\npostSamples_df1%&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\", nrow = 2)+\n  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+\n  scale_color_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  main_theme+\n  labs(col = \"Chains\", x= \"Iterations\")\n\n\n\n\nAll MCMC seem to be stationary, we can go on.\n\n### Dataframe of estimated and real value of the mean and standard error\nref   &lt;- data.frame(val=c(mean(Y2),sd(Y2)), Parameter = c(\"m\", \"sd\"))\nesti   &lt;- data.frame(val=c(mean(postSamples_df1$value[postSamples_df1$Parameter ==\"m\"]),mean(postSamples_df1$value[postSamples_df1$Parameter ==\"sd\"])), Parameter = c(\"m\", \"sd\"))\n\n\npostSamples_df1 %&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\") +\n  geom_histogram(data = postSamples_df1, aes(x= value, y =..density..),\n                 bins=60, position = \"identity\", alpha = 0.4, col = 'purple4', fill = \"white\")+\n  scale_fill_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  geom_vline( aes(xintercept=val), esti, col = 'purple1',\n              linewidth=1.2)+\n  geom_vline( aes(xintercept=val), ref,\n              linetype = 2, linewidth=1.2)+\n  main_theme\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHere we look at the posterior distribution estimated by the 3 chain for the mean and the standard error. Purple full lines represent the means of posterior distributions( \\(\\hat{m} =\\) 10.1 , \\(\\hat{\\sigma} =\\) 1) and dark dotted lines are the real value of standard error ( \\(\\sigma =\\) {r} round(ref$val[2],1)) and mean( \\(m =\\) 10.1) of the data inY2. The estimations are really good, which is normal with 200 data.\n\n\nMultiple class exemple\nThe next example is a little bit more elaborated. It is almost the same because we are going to estimate means and standard error like the previous one, but this time we have multiple classes in our data. We can suppose that we want to study the length of snakes in five different desserts (that we are going to call A,B,C,D,E for simplicity).\n\nset.seed(1000)\ny_means = round(runif(n= 5, min = 5, max = 15))\nn_obs = 40\nY3 = c()\nclass = c()\nIDclass = c()\nID = c(\"A\",\"B\",\"C\",\"D\",\"E\")\nfor(i in 1:length(y_means)){\n Y3= c(Y3, rnorm(n = n_obs, mean = y_means[i], sd = 2))\n class = c(class,rep(ID[i],n_obs))\n IDclass = c(IDclass,rep(i,n_obs))\n}\ndata = data.frame(Y3, class, IDclass)\nggplot(data)+\n  geom_point(aes(class,Y3))+\n  geom_violin(aes(class,Y3), alpha =0.2)+\n    main_theme\n\n\n\n\n\nmod2 = \"\n  model{\n  ## model\n  for(i in 1:n_obs){\n    Y3[i] ~ dnorm(m[IDclass[i]],preci)\n  }\n  ## Prior distribution\n  for(n in 1:n_class){\n    m[n] ~ dunif(0, 10**3)\n  }\n  sd ~ dunif(0, 10**3)\n  preci &lt;- 1/(sd*sd)\n  }\n\"\n\nFor this time we want to estimate 5 different means and we will suppose that standard deviation is the same between each class (but we could also estimate it with respect to the class). To estimate multiple parameters we initiate as much prior as there is class with the loop for(n in 1:n_class){ m[n] ~ dunif(0, 10**3)}. m[IDclass[i]] is here to select the correct estimated mean that is the same class as the data.\n\ndata_list2 = list(\n  Y3 = data$Y3, # data vector\n  n_obs= length(data$Y3), # number of observations\n  IDclass = data$IDclass, # vector to specify class of each observation\n  n_class = max(data$IDclass) # Number of class\n)\n\nFor this example we add IDclass = data$IDclass that will be used to identify which data belongs to which class and n_class = max(data$IDclass) to specify the number of class.\n\ninit_list2 &lt;- list(\n  m = rep(5,5),\n  sd = 2\n)\n\nThis is where we initiate the prior. The only change is that we initialize 5 means instead of one with m = rep(5,5). So we have one prior of mean per class. Now we can run the model:\n\nmjags2 &lt;- jags.model(file=textConnection(mod2),\n                     data=data_list2,\n                     inits=init_list2,\n                     n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 6\n   Total graph size: 415\n\nInitializing model\n\npostSamples2 &lt;- coda.samples(mjags2,\n                             variable.names = c(\"m\",\"sd\"),\n                             n.iter = 10000,\n                             thin = 10)\n\nSome data manipulation for the visualization\n\npostSamples_df2 &lt;-  postSamples2 %&gt;% ggs()\nparams_names = unique(postSamples_df2$Parameter)\npostSamples_df2$true_values = NA\ntemp = c()\n\ndata3 = data.frame(Y3 =Y3, class = IDclass)\nref2 = postSamples_df2%&gt;%\n  group_by(Parameter)%&gt;%\n  summarise(val = mean(value))\n\nesti2 = data3%&gt;%\n  group_by(class)%&gt;%\n  summarise(val = mean(Y3))\n\nesti2 = rbind(esti2,data.frame(class=\"sd\",val= mean(data3%&gt;%\n  group_by(class)%&gt;%\n  summarise(val =sd(Y3))%&gt;%pull(val))))\n\nesti2$class = ref2$Parameter\nnames(esti2)[1] = c(\"Parameter\")\nesti2$Parameter = as.character(esti2$Parameter) \nref2$Parameter = as.character(ref2$Parameter) \n\nCheck if the chains are stationary.\n\npostSamples_df2%&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\")+\n  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+\n  scale_color_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  main_theme\n\n\n\n\nFinally, we look at the results of posterior distribution of the estimated means and the estimated standard deviation.\n\npostSamples_df2 %&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\") +\n  geom_histogram(aes(x= value, y =after_stat(density)), col = 'purple4',fill = \"white\", bins = 60)+\n  geom_vline( aes(xintercept=val), esti2, col = 'purple1',\n              linewidth=1.2)+\n  geom_vline( aes(xintercept=val), ref2,\n              linetype = 2, linewidth=1.2)+\n  main_theme\n\n\n\n\nPurple full lines represent the means of posterior distributions and doted line are the true values of the data. The utility to have posterior distribution and not only compute the mean or the standard error is that we can know the probability of each mean or sd values, therefore we have a good estimation of the confidence that we should have when making conclusion on values."
  },
  {
    "objectID": "bayesian_modeles.html#predator-prey-body-size",
    "href": "bayesian_modeles.html#predator-prey-body-size",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Predator-prey body size:",
    "text": "Predator-prey body size:\nFor example, predators are often gape-limited, meaning that larger predators should be able to eat larger prey and smaller predators, smaller preys, depending on the size of their mouth. It is known from the existing literature that predators are 2 or 3 times larger than their prey (Trebilco et al. 2013). So, in this example we could use a prior mean of the intercept to a value below zero, like an average predator/prey mass comparison (Wesner and Pomeranz 2021)."
  },
  {
    "objectID": "bayesian_modeles.html#spider-abundance-according-to-the-presence-of-fish",
    "href": "bayesian_modeles.html#spider-abundance-according-to-the-presence-of-fish",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Spider abundance according to the presence of fish:",
    "text": "Spider abundance according to the presence of fish:\nIn this example from a study done by Warmbold and Wesner in 2018, they hypothesized that fish would reduce the emergence of adult aquatic insects by eating their larvae, causing a reduction in terrestrial spiders that feed on the adult forms of those insects. The authors used a Bayesian inference approach to estimate the posterior probabilities of each parameter in their models. They preferred Bayesian inference to frequentist inference, because their interest lay in estimating the probabilities of the hypotheses or parameters θ, given the data y, i.e. p(θ|y), rather than the probability of the data given a null hypothesis.The authors based their priors for intercepts on prior knowledge from a pilot experiment. The priors for the intercepts and slopes were assigned as normal distributions, the standard deviations as half-cauchy distributions, and the priors for the scale in the gamma distribution were assigned as exponential distributions.\n\nAnt species diversity\nAs said before, bayesian inference not only uses the sample data but also any available prior information. In this example from Gotelli and Ellison, 2002, we want to see how many species of ants we can find in sampling grids according to different environment variables. They did simple additive models of richness and models that included all possible interactions. Here, they used the Bayes’ theorem to calculate the posterior probability of the model conditional on the data with explicit explication on the prior from the literature. In this example, they precisely used WinBUGS which implements the MCMC methods with presented before using a Gibbs sampler.\n\n\nModeling the life cycle of a fish\nThe paper shows an integrated life cycle model using Hierarchical Bayesian Models (HBMs) for marine fish species, highlighting larval drift processes, the contribution of several nurseries to recruitment, as well as natural and marine mortality. the Peach. These MBHs, associated with Markov chain Monte Carlo methods, make it possible to incorporate complex demographic models into statistical frameworks, while processing varied data. This approach aims to provide inferences while robustly assessing the uncertainty surrounding parameter estimates and predictions. The widespread use of MBHs in fish population dynamics demonstrates their potential for increasing biological realism."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The book we dreamt we had",
    "section": "",
    "text": "This website is produced by the M2 MODE student uring year 2023-2024 as part of their assignment in the Online Collaborative Ressources course.\nAfter a short introduction regarding the principles and the tools of a reproducible science, they have been collaborating to write a M1 companion book for the next student generation.\nTable of Contents\n\nMultivariate analysis\nMixed models in Ecology\nDependent Data\nBayesian models"
  },
  {
    "objectID": "AMV_chapter.html",
    "href": "AMV_chapter.html",
    "title": "Multivariate analysis",
    "section": "",
    "text": "Biologists’ data, whether on individuals, populations or communities, are usually presented in the form of rectangular tables, with observations (n) in the rows and variables (p) in the columns.\nThe graphical representation of these n observations and p variables is easily achieved when there are only 2 or 3 variables (dimension). However, when the number of variables increases, the graphical representation becomes complicated, and Multivariate Analyses come into their own!\nThe aim of these analyses is to reduce the number of dimensions and examine the structure of the data by answering the following questions:\n\nWhich observations are similar?\nAre there observations that stand out? Subgroups?\nWhich variables are correlated?\nAre there particular links between certain observations and/or variables?\n\nMultivariate analysis methods are therefore used to describe the data and generate hypotheses that can then be tested."
  },
  {
    "objectID": "AMV_chapter.html#introduction",
    "href": "AMV_chapter.html#introduction",
    "title": "Multivariate analysis",
    "section": "Introduction",
    "text": "Introduction\nPCA can be used to process a measurement table with :\n\nIn row: the n observations.\nIn column: the p quantitative variables.\n\nThe 2 sets (observations and variables) are totally distinct and non-interchangeable. In other words, if you interchange the rows and columns (with the variables in the rows and the observations in the columns), the table no longer has the same meaning.\nIn PCA tables, the mean of a column has a meaning, while the mean of a row does not.\nThe aim of PCA is to achieve the best geometric representation of individuals and variables. To achieve this, we seek to reduce the dimension by finding the best projection plane (subspace) for “best” visualization of the point cloud in reduced space.\nHowever, this reduction must :\n\nTwo individuals who resemble each other must be close in the representation space.\nPreserve correlations between variables:\n\nTwo variables that are correlated must be represented by vectors forming an acute angle.\nTwo independent variables are represented by orthogonal vectors."
  },
  {
    "objectID": "AMV_chapter.html#mathematics",
    "href": "AMV_chapter.html#mathematics",
    "title": "Multivariate analysis",
    "section": "Mathematics",
    "text": "Mathematics\nPrincipal Component Analysis (PCA) is a powerful method for simplifying the complexity of highly correlated data. This approach aims to reduce dimensionality in a linear manner, thereby providing a clearer perspective on the underlying structure of the data.\nIn essence, PCA performs a linear transformation of the original variables to new variables called principal components. These components are carefully selected to capture the majority of the variance present in the dataset, thereby maximizing the separation between different observations.\nThe space of principal components can be expressed as :\n\\[\nZ_p=\\Sigma^p_{j=1}ø_j\\times X_j\n\\]Where :\n\n\\(Z_p\\) is the principal component \\(p\\)\n\\(Ø_j\\) is the weight vector comprising the \\(j\\) weights for principal component \\(p\\), i.e., the coefficients of the linear combination of the original variables from which the principal components are constructed.\n\\(X_j\\) is the standardized predictor, meaning it has a mean equal to 0 and a standard deviation of 1.\n\nThe process begins with a matrix \\(Y\\), where each row represents an observation and each column represents a continuous variable.\n\n\n\n\n\nFirst, we normalize the observations as follows :\n\\[\nY_{std} = \\frac{y_i-\\overline{y}}{\\sigma_y}\n\\] ; which is equivalent to centering as in:\n\\[\ny_c= [y_i-\\overline{y}]\n\\] ; then scaling as in:\n\\[\ny_s = \\frac{y_i}{\\sigma_y}\n\\]\nWe can then calculate the variance-covariance matrix:\n\\[\nR = cov(Y_{std})\n\\]The decomposition of this matrix provides the matrix \\(U\\) of eigenvectors, which contains the principal components.\nThe eigenvectors, defined as the principal components, have associated eigenvalues. The graphical representation of the distances of observations to these eigenvectors illustrates how each principal component captures the variation in the data.\n\n\n\n\n\n\n\n\n\n\nThe orthogonal aspect of the principal components becomes evident when observing the second principal component, which, in turn, maximizes the variance in the data. These principal components, each aligned along an orthogonal axis, form a new coordinate system in which the information is more clearly structured.\nIn addition to representing the variance of the principal components, the eigenvectors also provide a relative understanding of their influence. This influence is calculated by dividing the eigenvalues by the total sum of these values, thus providing context on the contribution of each principal component \\(v_k\\) to the overall information.\n\\[\nExplained\\ variance\\ of\\ v_k = \\frac{\\lambda_{v_k}}{\\Sigma_{i=1}^p\\lambda_v}\n\\]The conclusion of the PCA involves obtaining the coordinate matrix \\(F\\). This matrix results from the multiplication of matrix \\(U\\) by the standardized matrix \\(Y_{std}\\), enabling a rotation of the new data space. This process aligns the data along the principal components, providing a simplified yet informative view of the underlying structure of the initial data."
  },
  {
    "objectID": "AMV_chapter.html#interpretation",
    "href": "AMV_chapter.html#interpretation",
    "title": "Multivariate analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere’s an example of how PCA can be applied. dataset presentation package needed\n\na) Importing the dataset :\n\n# Loading the \"iris\" dataset available on R :\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\ndata(\"decathlon2\") \ndata=decathlon2[decathlon2$Competition==\"OlympicG\",c(1:10,12)]\nsummary(data)\n\n     X100m         Long.jump        Shot.put       High.jump    \n Min.   :10.44   Min.   :6.990   Min.   :13.07   Min.   :1.880  \n 1st Qu.:10.72   1st Qu.:7.303   1st Qu.:14.52   1st Qu.:1.940  \n Median :10.88   Median :7.475   Median :14.86   Median :2.045  \n Mean   :10.82   Mean   :7.474   Mean   :14.90   Mean   :2.017  \n 3rd Qu.:10.94   3rd Qu.:7.688   3rd Qu.:15.29   3rd Qu.:2.112  \n Max.   :11.14   Max.   :7.960   Max.   :16.36   Max.   :2.150  \n     X400m        X110m.hurdle       Discus        Pole.vault   \n Min.   :46.81   Min.   :13.97   Min.   :40.11   Min.   :4.400  \n 1st Qu.:48.56   1st Qu.:14.07   1st Qu.:43.90   1st Qu.:4.625  \n Median :49.05   Median :14.23   Median :44.73   Median :4.900  \n Mean   :49.01   Mean   :14.29   Mean   :45.43   Mean   :4.843  \n 3rd Qu.:49.41   3rd Qu.:14.36   3rd Qu.:47.66   3rd Qu.:5.000  \n Max.   :50.79   Max.   :14.95   Max.   :51.65   Max.   :5.400  \n    Javeline         X1500m          Points    \n Min.   :51.53   Min.   :264.4   Min.   :7926  \n 1st Qu.:55.43   1st Qu.:270.5   1st Qu.:8088  \n Median :58.11   Median :276.3   Median :8236  \n Mean   :59.58   Mean   :275.1   Mean   :8317  \n 3rd Qu.:62.92   3rd Qu.:278.6   3rd Qu.:8396  \n Max.   :70.52   Max.   :287.6   Max.   :8893  \n\n\nIn this data set, we have 14 individuals (athletes) on whom 10 quantitative variables (performances in different sports disciplines) have been recorded:\n\nX100m\nLong.jump\nShot.put\nX400m\nX110m.hurdle\nDiscus\nPole.vault\nJavelin\nX1500m\n\nIf NAs are present in the data. In order to carry out the PCA without any problems with NAs, they can be omitted from the analysis with the following command:\n\n# data=na.omit(object = data)\n\nWe want to find out whether particular links between certain observations and/or variables can be observed in this data set.\n\n\nb) Study of correlations :\nObtain the correlation matrix between variables using the cor() function:\n\ncor(data)\n\n                   X100m  Long.jump    Shot.put   High.jump       X400m\nX100m         1.00000000 -0.8350966 -0.21746312 -0.26333512  0.54802600\nLong.jump    -0.83509656  1.0000000  0.31407521  0.24623935 -0.51919384\nShot.put     -0.21746312  0.3140752  1.00000000  0.77045776 -0.18546890\nHigh.jump    -0.26333512  0.2462394  0.77045776  1.00000000 -0.17357206\nX400m         0.54802600 -0.5191938 -0.18546890 -0.17357206  1.00000000\nX110m.hurdle  0.38366645 -0.4914562  0.03157435  0.01126452  0.35162059\nDiscus       -0.51341585  0.5136697  0.88239852  0.71362867 -0.36823138\nPole.vault    0.07021566  0.1578300 -0.41715958 -0.65024394  0.19459315\nJaveline     -0.06085398  0.2256900  0.53582882  0.20440490  0.04744169\nX1500m       -0.47201188  0.5394643  0.10753385 -0.03189887  0.15026787\nPoints       -0.64440656  0.7809521  0.76009926  0.60096524 -0.52293108\n             X110m.hurdle     Discus  Pole.vault    Javeline      X1500m\nX100m          0.38366645 -0.5134159  0.07021566 -0.06085398 -0.47201188\nLong.jump     -0.49145618  0.5136697  0.15783003  0.22568995  0.53946433\nShot.put       0.03157435  0.8823985 -0.41715958  0.53582882  0.10753385\nHigh.jump      0.01126452  0.7136287 -0.65024394  0.20440490 -0.03189887\nX400m          0.35162059 -0.3682314  0.19459315  0.04744169  0.15026787\nX110m.hurdle   1.00000000 -0.1755752  0.05716187  0.23674145 -0.41440555\nDiscus        -0.17557519  1.0000000 -0.46586569  0.47246648  0.12824707\nPole.vault     0.05716187 -0.4658657  1.00000000  0.08916975  0.42631317\nJaveline       0.23674145  0.4724665  0.08916975  1.00000000  0.01073007\nX1500m        -0.41440555  0.1282471  0.42631317  0.01073007  1.00000000\nPoints        -0.23729481  0.8596594 -0.13740583  0.60192747  0.24700780\n                 Points\nX100m        -0.6444066\nLong.jump     0.7809521\nShot.put      0.7600993\nHigh.jump     0.6009652\nX400m        -0.5229311\nX110m.hurdle -0.2372948\nDiscus        0.8596594\nPole.vault   -0.1374058\nJaveline      0.6019275\nX1500m        0.2470078\nPoints        1.0000000\n\n\nVariables are correlated if their value is greater than 0.9 (as a general rule). Here, we can see that no variables are correlated.\nHere’s another way of visualizing correlations between variables: (useful when you have a lot of variables, as here):\n\nabs(cor(data))&gt;0.9\n\n             X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nX100m         TRUE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nLong.jump    FALSE      TRUE    FALSE     FALSE FALSE        FALSE  FALSE\nShot.put     FALSE     FALSE     TRUE     FALSE FALSE        FALSE  FALSE\nHigh.jump    FALSE     FALSE    FALSE      TRUE FALSE        FALSE  FALSE\nX400m        FALSE     FALSE    FALSE     FALSE  TRUE        FALSE  FALSE\nX110m.hurdle FALSE     FALSE    FALSE     FALSE FALSE         TRUE  FALSE\nDiscus       FALSE     FALSE    FALSE     FALSE FALSE        FALSE   TRUE\nPole.vault   FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nJaveline     FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nX1500m       FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nPoints       FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\n             Pole.vault Javeline X1500m Points\nX100m             FALSE    FALSE  FALSE  FALSE\nLong.jump         FALSE    FALSE  FALSE  FALSE\nShot.put          FALSE    FALSE  FALSE  FALSE\nHigh.jump         FALSE    FALSE  FALSE  FALSE\nX400m             FALSE    FALSE  FALSE  FALSE\nX110m.hurdle      FALSE    FALSE  FALSE  FALSE\nDiscus            FALSE    FALSE  FALSE  FALSE\nPole.vault         TRUE    FALSE  FALSE  FALSE\nJaveline          FALSE     TRUE  FALSE  FALSE\nX1500m            FALSE    FALSE   TRUE  FALSE\nPoints            FALSE    FALSE  FALSE   TRUE\n\n\nIf TRUE (outside the diagonal), then both variables are correlated If two variables are correlated, one must be removed. The choice of deleting one of the two correct variables is arbitrary and depends on the question being asked.\nTo remove a variable from the data set, use the following function: data=data[,-(column_of_the_variable_to_remove)}]\n\n\nc) Performing PCA :\nTo run the PCA, you need to load the following two packages: factoextra and FactoMineR\nThen, to perform the PCA on R, you can use the function : PCA(). Remember to store the result of this PCA in a new variable, so that you can easily retrieve the PCA information you need for subsequent interpretation.\n\n\nd) Interpretation of outputs :\n\n1) Inertia and choice of axes :\n\n# Recover the eigenvalues of each axis and the inertia carried by the principal components\nPCA$eig\n\n          eigenvalue percentage of variance cumulative percentage of variance\ncomp 1  4.753038e+00           4.320944e+01                          43.20944\ncomp 2  2.418289e+00           2.198444e+01                          65.19388\ncomp 3  1.551340e+00           1.410310e+01                          79.29698\ncomp 4  9.863587e-01           8.966897e+00                          88.26388\ncomp 5  5.554931e-01           5.049937e+00                          93.31381\ncomp 6  3.216961e-01           2.924510e+00                          96.23832\ncomp 7  2.203570e-01           2.003246e+00                          98.24157\ncomp 8  1.149733e-01           1.045212e+00                          99.28678\ncomp 9  6.178365e-02           5.616696e-01                          99.84845\ncomp 10 1.666416e-02           1.514924e-01                          99.99994\ncomp 11 6.323601e-06           5.748728e-05                         100.00000\n\n# Graphical display of the inertia of each axis : \nfviz_eig(PCA, addlabels = TRUE)\n\n\n\n\nTo determine the number of axes used in the PCA analysis, we need to identify the jump in variance explained by the different axes. Here we can see that the first axis represents 43.2% of the variance, the second axis 22%, the third axis 14.1%, …\nWe can therefore see that the jump in variance explained by the different axes is between the first and second axes. The difference between the variance explained by the axes other than the first is negligible compared to the difference between the first axis and the others.\nFor the PCA interpretation, we therefore retain the first two axes, which together explain 65.19% of the variance.\n\n\n2) Interpretation of the biological meaning of the axes :\nIn PCA, the axes are interpreted according to the columns (i.e. variables) via the correlation circle and the absolute contributions of the variables.\nTo obtain this information in R, use the following commands:\n\n# Obtain the absolute contributions of variables :\nPCA$var$contrib\n\n                 Dim.1       Dim.2       Dim.3      Dim.4       Dim.5\nX100m        10.824776 10.03651625  1.63515357  0.3755928 23.39362182\nLong.jump    12.418510 13.24601095  0.18397545  1.0039891  1.29845908\nShot.put     13.118558  8.89903353  2.86005522  2.7322519  1.12053773\nHigh.jump    10.101229 10.94386007  1.34856164  9.6446306  5.75676065\nX400m         6.050422  1.75575069 17.68011201 32.4590377  0.09123132\nX110m.hurdle  2.265809 13.96580459 10.80010469  7.4256988 54.75065458\nDiscus       17.620323  2.99067030  0.06343601  0.5423102  0.71456580\nPole.vault    2.249144 16.38731995 23.04479888  4.8414968  0.03149013\nJaveline      4.133763  3.25169636 31.78461582  9.3858284 10.87089875\nX1500m        1.779257 18.51325372  8.53578854 28.5491707  1.51933507\nPoints       19.438208  0.01008361  2.06339817  3.0399930  0.45244507\n\n# Obtain the circle of correlations :\nfviz_pca_var(PCA)\n\n\n\n# Graphical representation of absolute variable contribution values for an axis: \n# Axis 1 case : \nfviz_contrib(PCA, choice = \"var\", axes = 1)\n\n\n\n# Axis 2 case: \nfviz_contrib(PCA, choice = \"var\", axes = 2)\n\n\n\n\nThe absolute contribution shows how the initial variable contributes to the formation of the axis. To find out from these results which variables contribute to the formation of the synthetic axis, we use the threshold of \\(\\frac{1}{nb\\_variables}\\times100\\) (in general). For a variable to contribute, its contribution value must be greater than this threshold. This threshold value corresponds to the red dotted line on the graphs showing the absolute contributions of the variables along the axes.\nFrom these results, we can see that axis 1 is mainly represented by the following variables:\n\nPoints, Discus, Shotput, Long.jump, X100m, High.jump\n\nAxis 2 is represented by the variables:\n\nX1500m, Pole.vault, X110m.hurdle, X100m\n\nThe correlation circle also shows that some variables are related and others independent. Two related variables are represented by vectors forming an acute angle. Two independent variables are represented by orthogonal vectors. Two negatively related variables are represented by two opposite vectors.\n\n\n3) Projection of individuals to create a typology based on the axes:\nFor this, we use the relative contributions of the individuals. These relative contributions can be used to create a typology of individuals based on the axes. The relative contribution of an individual corresponds to the share of information on the axis explained by the point and supported on the axis (in percentage).\nTo obtain the relative contributions of individuals on R, use the following commands:\n\n# Relative contributions of individuals :\nPCA$ind$cos2\n\n                Dim.1        Dim.2        Dim.3        Dim.4        Dim.5\nSebrle     0.68963196 6.540767e-12 0.1505401735 3.421119e-03 0.0919816558\nClay       0.68982961 7.883438e-02 0.1191186204 1.593953e-04 0.0009981983\nKarpov     0.74957187 2.476108e-02 0.1608118952 2.542731e-03 0.0008147725\nMacey      0.12167773 7.112491e-01 0.0847580597 1.109719e-04 0.0239527703\nWarners    0.03513216 7.035171e-01 0.2043199790 2.553907e-02 0.0041832424\nZsivoczky  0.01766196 7.411226e-01 0.0583141713 1.304529e-02 0.1454252042\nHernu      0.23531031 2.767503e-01 0.1923125481 1.038527e-01 0.0748120286\nBernard    0.03767052 2.359745e-02 0.4446099398 2.664423e-01 0.0658970688\nSchwarzl   0.74103094 1.190975e-01 0.0087379756 1.268006e-06 0.0098608763\nPogorelov  0.12049107 2.472670e-02 0.0687218609 7.285021e-01 0.0045896049\nSchoenbeck 0.47746580 2.724861e-02 0.2569700383 5.580082e-02 0.0190441531\nBarras     0.32981991 4.127756e-01 0.0005470956 1.535366e-02 0.1765554931\nNool       0.23451399 1.310712e-01 0.2420363184 2.449685e-01 0.1242846825\nDrews      0.53811419 2.784934e-01 0.1528997775 1.306683e-02 0.0056915740\n\n# Obtaining the projection of individuals : \nfviz_pca_ind(PCA)\n\n\n\n\nThe individuals who most explain Axis 1 are :\n\nSebrle, Clay, Karpov, Schwarzl, Drews, Schoenbeck\n\nThose who most explain axis 2 are :\n\nMacey, Warners, Zsivoczky, Barras\n\n\n\n\ne) Biological conclusion :\nThus, thanks to the results of the absolute contributions of the variables and the relative contributions of the individuals, we can represent the projection of the individuals and variables as follows:\n\nfviz_pca_biplot(PCA)\n\n\n\n\nKnowing the biological interpretation of the axes :\n\n\n\nPCA interpretation\n\n\nWe can therefore see that the individuals with the highest scores at this tournament are those who had the best results in the discus, shot put, long jump and high jump events. In addition, having good results in the 100m race doesn’t give you a good ranking.\nThanks to PCA, we can see that the final score is more linked to the score of certain disciplines than others. And therefore, that links between certain variables and individuals can be observed."
  },
  {
    "objectID": "AMV_chapter.html#introduction-1",
    "href": "AMV_chapter.html#introduction-1",
    "title": "Multivariate analysis",
    "section": "Introduction",
    "text": "Introduction\nCA deals with tables that cross two categorical variables with several modalities, such as contingency tables, frequency tables, …\nIn these tables, rows and columns are interchangeable and the variables are all in the same units.\nThe aim of CA is to reveal structures and associations between these qualitative variables by representing them in a multidimensional space. It allows us to visualize relationships between categories of different variables, and to highlight trends, groupings or disparities. It is particularly useful for analyzing categorical data, when you want to understand the underlying structure of the data and identify associations between categories.\nUnlike PCA, for a CA, we’re not interested in the distances between points, but in the distances between profiles of different modalities. A CA is therefore simply a PCA on the profiles, by equipping the space with a suitable distance: the \\(\\chi^2\\) distance. With this distance, the weight of rows (or columns) is relativized, but not cancelled out, and the symmetry between rows and columns is preserved."
  },
  {
    "objectID": "AMV_chapter.html#mathematics-1",
    "href": "AMV_chapter.html#mathematics-1",
    "title": "Multivariate analysis",
    "section": "Mathematics",
    "text": "Mathematics\nCorrespondence Analysis (CA) represents a statistical method aimed at exploring the relationship between two sets of categories. In contrast to PCA, which applies to continuous variables, CA is designed for categorical variables, allowing the analysis of the structure of a contingency table.\n\n\n\n\n\nIn essence, Correspondence Analysis (CA) seeks to graphically represent associations between rows and columns in a contingency table. It helps reveal trends, groupings, or oppositions within categories, providing insight into the structure of dependence between variables.\nThe process begins with a contingency table showing the observed frequencies of co-occurrence between different categories. These data are then transformed to obtain a probability table where the frequency of each co-occurrence is calculated as follows:\n\\[\nf_{ij} = \\frac{X_{ij}}{n}\n\\]With \\(f_{ij}\\) representing the probability that events \\(i\\) and \\(j\\) occur jointly, \\(X_{ij}\\) the number of times this co-occurrence has been observed, and \\(n\\) the total number of observations.The marginal probabilities of the columns \\((f_{.j})\\) and rows \\((f_{i.})\\) are calculated as follows:\n\\[\nf_{.j} = \\sum_{i=1}^{I}f_{ij}\\\\\\\\\\\\  \nf_{i.}=\\sum_{j=1}^{J}f_{ij}\n\\]\n\n- Profiles\n\nRow profiles\nThe row profile is \\(L_i = (\\frac{f_{i1}}{f_{i.}},...,\\frac{f_{ij}}{f_{i.}})\\). In matrix terms, we will set up \\(D_n=diag(f_{i.})\\) such that \\(L = D_n^{-1}F\\). We can then define the average row profile, obtained by summing the columns of our frequency table :\n\\[\n(\\sum_{i=1}^nf_{i.}\\frac{f_{i1}}{f_{i.}},...,\\sum_{i=1}^nf_{i.}\\frac{f_{ij}}{f_{i.}})=(f_{.1},...,f_{.j})\n\\]\nColumn profiles\nThe column profile is \\(C_j=(\\frac{f_{1j}}{f_{.j}},...,\\frac{f_{ij}}{f_{.j}})\\). As with row profiles, it can be expressed and calculated in matrix terms by setting: \\(D_p=diag(f_{.j})\\), such that \\(C=D_p^{-1}F^T\\). We can then define the average column profile by summing the rows of our frequency table:\n\\[\n(\\sum_{j=1}^pf_{.j}\\frac{f_{1j}}{f_{.j}},...,\\sum_{j=1}^pf_{.j}\\frac{f_{ij}}{f_{i.}})=(f_{1.},...f_{i.})\n\\]\n\n\n\n- Distances between profiles\nOne can measure the distance between two row profiles by\\[d^2(i,i')=\\sum_{j=1}^p(\\frac{f_{i j}}{f_{i.}}-\\frac{f_{i'j}}{f_{i'.}})^2\\]\nBut this distance does not take into account the importance of each column. A more sensible choice is to use the \\(\\chi^2\\) distance, which does take the importance of each column into consideration.\n\\(d^2(i,i')=\\sum_{j=1}^p\\frac{1}{f_{.j}}(\\frac{f_{i j}}{f_{i.}}-\\frac{f_{i'j}}{f_{i'}})^2\\)\nLet’s use the notation \\(x\\) to refer to the vector of distances between two rows \\(L_{11}-L_{21}\\) or between two columns \\(C_{11}-C_{12}\\). Adopting matrix notation, the square distance of the \\(\\chi^2\\) is of the form:\n\\[\nx^TD_p^{-1}x = \\sum_{j=1}^p\\frac{x^2_j}{f_{.j}}\n\\]for a row point \\(x\\ \\epsilon\\ \\mathbb{R}^n\\) .\n\n\n- Independence\nIn the theory of tests, the objective is to determine whether there is a link between the two qualitative variables. The null hypothesis \\(H_0\\) is the independence between the two variables (no effect). The alternative hypothesis \\(H_1\\) is the presence of a link between the two variables.\nThe association between the two categorical variables is assessed through the discrepancy between the observed data and the independence model.This independence model is calculated based on the following equation:\n\\[\nP(A\\ \\cap\\ B) = P(A) \\times P(B)\n\\]Therefore, the joint probability is the product of the marginal probabilities:\n\\[\nf_{ij} = f_{i.} \\times f_{.j}\n\\]One can then calculate the discrepancy between the observed data \\((f_{ij})\\) and the independence model:\n\nThe significance of the association is measured with a \\(\\chi^2\\) test\n\\[\n\\chi_{obs}^2 = \\sum_{i=1}^I\\sum_{j=1}^J\\frac{(obs.headcount - th.headcount)^2}{th.headcount}=\\sum_{i=1}^I\\sum_{j=1}^J\\frac{(n\\times f_{ij} - n\\times f_{i.}\\times f_{.j})^2}{n\\times f_{i.}\\times f_{.j}}\n\\]\nThe strength of the association \\(Ø^2\\) is measured by the discrepancy between theoretical and observed probabilities.\n\n\\[\nØ^2 = \\sum_{i=1}^I\\sum_{j=1}^J\\frac{(obs.prob - th.prob)^2}{th.prob}\n\\]\n\n\n- Binary Correspondence Analysis\nBy conducting a correspondence analysis, we aim to simultaneously represent the row profiles belonging to \\(\\mathbb{R}^p\\) and the column profiles belonging to \\(\\mathbb{R}^n\\) of a frequency table. As this representation is done in the Cartesian plane, it will be obtained through a double principal component analysis.\nThe two principal component analyses are not directly performed on the variables but rather on the row profiles (direct analysis) and the column profiles (dual analysis) presented earlier. Moreover, binary correspondence analysis incorporates the notion of column (or row) weights and the \\(\\chi^2\\) distance.\n\n- Direct Analysis (Rows)\nThe direct analysis is performed on the row profiles. The rows of \\(L = D_n^{-1}F\\) are elements of \\(\\mathbb{R}^p\\). We aim to represent them in this space using the distance function \\(x^TD_p^{-1}x\\).\nIn the direct analysis, we seek the vector \\(u\\ \\epsilon\\ \\mathbb{R}^p\\) such as :\n\\[\n(u^TD_p^{-1}F^TD_n^{-1})D_n(D_n^{-1}FD_p^{-1}u)\n\\]\nWith \\(F\\) : the observed frequency matrix\nis maximal, given that \\(u^TD_p^{-1}u=1\\).\nThe solution is given by the principal eigenvector of\n\\[\nD_pD_p^{-1}F^TD_n^{-1}FD_p^{-1}=F^TD_n^{-1}FD_p^{-1}\\equiv S\n\\]\n\n\n- Dual Analysis (Columns)\nDual analysis is performed on the column profiles. The rows of \\(C = D_p^{-1}F^T\\) are elements of \\(\\mathbb{R}^n\\). We aim to represent them in this space using the distance function \\(x^TD-n^{-1}x\\).\nIn dual analysis, we seek the vector \\(v\\ \\epsilon\\ \\mathbb{R}^n\\) such as\n\\[\n(v^TD_n^{-1}FD_p^{-1})D_p(D_p^{-1}F^TD-n^{-1}v)\n\\]is maximal, given that \\(v^TD_n^{-1}v=1\\).\nThe solution is given by the principal eigenvector of\n\\[\nD_n(D_n^{-1}FD_p^{-1}F^TD_n^{-1})=FD_p^{-1}F^TD_n^{-1}\\equiv T\n\\]\n\n\n\n- Projection of both analyses onto the same plane\nThe usual goal of correspondence analysis is to produce a 2-dimensional graph that summarizes the information contained in the frequency table and highlights different interesting associations. In other words, we want to somehow overlay the figures resulting from dual and direct analyses.\nTo present both analyses in the same plane, it is essential to ensure that both PCAs project the data into the same dimensions. The transition relationships and the concept of the center of gravity presented in the following sections allow us to ensure that we are indeed projecting the “individuals” from dual and direct analyses into the same dimensions.\n\n- Transition Relationships\nThe following relationships\n\\[\nD_n^{-1}F\\varphi_j=\\sqrt{\\lambda_j}\\Psi_j\\equiv \\hat{\\Psi}_j\n\\]\n\\[\nD_p^{-1}F^T\\Psi_j=\\sqrt{\\lambda_j}\\varphi_j\\equiv \\hat{\\varphi}_j\n\\]\nWith \\(\\varphi_j\\) : the eigenvector associated with the eigenvalue \\(\\lambda_j\\) of the matrix F; \\(\\Psi_j\\) : the modality coordinates of the row variable in the row space defined by the eigenvalues; \\(\\hat{\\Psi}_j\\) the contrinution of the component \\(j\\) to the row variable in the row space and \\(\\hat{\\varphi}_j\\) the contrinution of the component \\(j\\) to the column variable in the column space\nestablish an almost barycentric link between the two types of analysis, meaning that the coordinates of points in one space are proportional to the components of the factor from the other space corresponding to the same eigenvalue.\nVerification: The first relationship is confirmed as follows.\n\\[\nD_n^{-1}F\\varphi_j=D_n^{-1}(FD_p^{-1}u_j) = D_n^{-1}(\\sqrt{\\lambda_j}v_j)=\\sqrt{\\lambda_j}\\Psi_j\n\\]The second identity is demonstrated in a similar manner.\nThe consequences of the preceding transition relationships are that…\n\n\\(1=\\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_p \\geq 0\\)\n\\((f_1,…,f_p)^T\\) is an eigenvector associated with the eigenvalue \\(\\lambda_1=1\\) of \\(S=F^TD_n^{-1}FD_p^{-1}\\).\nNote that since the first eigenvalue is always equal to one, many software packages only mention the other \\(p-1\\) eigenvalues.\n\n\n\n- Center of Gravity\nTo project the observations onto the same plane, we need to determine the origin of the graph.\nThe center of gravity of the rows is defined by \\(G_L=(g_1,…,g_p)^T\\), where\n\\[\ng_j=\\sum_{i=1}^nf_i\\frac{f_{ij}}{f_i}=\\sum_{i=1}^nf_{ij}=f_j, 1\\leq j \\leq p\n\\]Similarly, the center of gravity of the columns is defined by\n\\[\nG_C=(f_1,...,f_n)^T\n\\]Centering of the rows is obtained by calculating\n\\[\n\\frac{f_{ij}}{f_i}-g_j=\\frac{f_{ij}}{f_i}-f_j=\\frac{f_{ij}-f_if_j}{f_i}\n\\]so that \\(\\sum_{j=1}^p\\frac{f_{ij}-f_if_j}{f_i}=0\\)\nfor all \\(i\\ \\epsilon\\) \\({1,…,n}\\).\nThe consequence of centering the rows is that the analysis is no longer conducted on \\(S=F^TD_n^{-1}FD_p^{-1}\\) but rather on \\(S*=(s^*_{jj'})\\), where\n\\[\ns^*_{jj'}=\\sum_{i=1}^n\\frac{(f_{ij}-f_if_j)(f_{ij'}-f_if_j)}{f_if_{j'}}\n\\]To detect associations between rows and columns, one must establish a connection with the statistic of the \\(\\chi^2\\) test.\nBy definition, \\(trace(S*)=\\sum_{j=1}^p\\sum_{i=1}^n\\frac{(f_{ij}-f_if_j)^2}{f_if_j}\\)"
  },
  {
    "objectID": "AMV_chapter.html#interpretation-1",
    "href": "AMV_chapter.html#interpretation-1",
    "title": "Multivariate analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere’s an example of how CA can be applied.\nYou can import R package using the code\n\n# Creating the dataset :\n## Step 1: Create eye and hair color vectors\ncolors_of_eyes &lt;- c(\"Blue\", \"Brown\", \"Green\", \"Gray\", \"Black\", \"Yellow\")\nhair_colors &lt;- c(\"Black\", \"Brown\", \"Blond\", \"Redhead\", \"Chestnut\", \"Blues\", \"White\", \"Pink\")\n\n## Step 2: Create a data array with fixed values\nfixed_values &lt;- matrix(c(\n  50, 35, 32, 24, 0, 0,\n  15, 84, 34, 18, 16, 0,\n  7, 4, 1, 20, 10, 0,\n  0, 18, 0, 21, 34, 6,\n  5, 3, 1, 0, 0, 3,\n  8, 4, 2, 5, 13, 24,\n  9, 10, 0, 12, 0, 10,\n  1, 20, 5, 46, 2, 5\n), nrow = length(hair_colors), ncol = length(colors_of_eyes))\n\n## Store the matrix in an array variable : \ndata &lt;- as.data.frame(fixed_values)\n\n## Name rows and columns\ncolnames(data) &lt;- colors_of_eyes\nrownames(data) &lt;- hair_colors\n\n\ndata\n\n         Blue Brown Green Gray Black Yellow\nBlack      50    34    10    5     2      0\nBrown      35    18     0    3     5     10\nBlond      32    16     0    1    13      1\nRedhead    24     0    18    0    24     20\nChestnut    0     7     0    0     9      5\nBlues       0     4    21    3    10     46\nWhite      15     1    34    8     0      2\nPink       84    20     6    4    12      5\n\n\nThis table shows the number of people with each eye color (in columns) and hair color (in rows), for a total of 617 people sampled.\nBy performing a CA on this dataset, we seek to show the correspondences/oppositions between the different modalities of a variable.\n\na) Performing CA :\nTo run the CA, such as the PCA, you need to load the following two packages: factoextra and FactoMineR.\nThen, to perform the CA on R, you can use the function : CA(). Remember to store the result of this PCA in a new variable, so that you can easily retrieve the PCA information you need for subsequent interpretation.\n\n\n\n\n\nIn a CA, rows are called rows and columns called col.\n\n\nb) Interpretation of outputs :\n\n1) Inertia and choice of axes :\n\n# Recover the eigenvalues of each axis and the inertia carried by the principal components\nCA$eig\n\n       eigenvalue percentage of variance cumulative percentage of variance\ndim 1 0.415178377             55.3969063                          55.39691\ndim 2 0.215052153             28.6942303                          84.09114\ndim 3 0.073125570              9.7570841                          93.84822\ndim 4 0.043902740              5.8579062                          99.70613\ndim 5 0.002202466              0.2938732                         100.00000\n\n# Graphical display of the inertia of each axis : \nfviz_eig(CA, addlabels = TRUE)\n\n\n\n\nTo determine the number of axes used in the CA analysis, we need to identify the jump in variance explained by the different axes. It’s the same as for PCA. Here we can see that the first axis represents 55.4% of the variance, the second axis 28.7%, the third axis 9.8%, …\nWe can therefore see that the jump in variance explained by the different axes is between the first and second axes. The difference between the variance explained by the axes other than the first is negligible compared to the difference between the first axis and the others.\nFor the CA interpretation, we therefore retain the first two axes, which together explain 65.19% of the variance.\n\n\n2) Interpretation of the biological meaning of the axes :\nIn CA, in contrast to PCA, axes are interpreted either column-wise or row-wise. The choice depends on the initial biological question.\nHere we’ll interpret the axes according to the lines, i.e. according to the hair colors. To find out which variables contribute to the synthetic axis, we use the threshold of : \\(\\frac{1}{nb\\_variables}\\)\nThe threshold in our case is: \\(\\frac{1}{8}\\).\nTo obtain this information in R, use the following commands:\n\n# Obtaining absolute line contribution values : \nRow_contrib=CA$row$contrib\nRow_contrib&gt;(1/8)*100\n\n         Dim 1 Dim 2 Dim 3 Dim 4 Dim 5\nBlack     TRUE FALSE  TRUE FALSE  TRUE\nBrown    FALSE FALSE FALSE FALSE  TRUE\nBlond    FALSE FALSE FALSE FALSE FALSE\nRedhead  FALSE FALSE  TRUE FALSE  TRUE\nChestnut FALSE  TRUE FALSE  TRUE FALSE\nBlues     TRUE FALSE  TRUE FALSE FALSE\nWhite    FALSE  TRUE FALSE FALSE FALSE\nPink      TRUE FALSE FALSE  TRUE FALSE\n\n\nAccording to these results, the lines explaining the first axis are :\n\nBlack, Blues, Pink\n\nThe lines explaining the second axis are :\n\nChestnut, White\n\n\n\n3) Interpretation of relatives contributions :\nRelative contributions represent the quality of representation of the column/row by the axis.\nEither the rows or the columns are interpreted, depending on the choice made in the next step. In our case, we chose the columns, since we interpreted the axes with the absolute contributions of the rows.\n\n# Obtain relative column contribution values :\nCA$col$cos2\n\n            Dim 1      Dim 2      Dim 3       Dim 4        Dim 5\nBlue   0.85575259 0.01448471 0.00950636 0.120205825 5.051631e-05\nBrown  0.57556811 0.05455375 0.17954769 0.189337969 9.924835e-04\nGreen  0.45208660 0.52672933 0.01096464 0.008743165 1.476265e-03\nGray   0.01081361 0.82960368 0.08394864 0.020223487 5.541059e-02\nBlack  0.06314083 0.46740988 0.41166914 0.056819078 9.610773e-04\nYellow 0.74157647 0.18279275 0.05802235 0.017605743 2.692628e-06\n\n\nBlue and yellow eye colors are well represented by axis 1. Grey eye color is represented by axis 2.\n\n# Relative line contribution values : \nCA$row$cos2\n\n              Dim 1       Dim 2      Dim 3       Dim 4        Dim 5\nBlack    0.69781032 0.070862823 0.15315848 0.066726581 1.144180e-02\nBrown    0.52754846 0.167378322 0.26792495 0.020035304 1.711296e-02\nBlond    0.69002130 0.182751826 0.08463182 0.041935390 6.596638e-04\nRedhead  0.49239795 0.083209727 0.41251121 0.007108659 4.772456e-03\nChestnut 0.03638017 0.564361623 0.02919618 0.365627649 4.434383e-03\nBlues    0.87482351 0.049760467 0.07057401 0.004824928 1.708337e-05\nWhite    0.15364853 0.827793121 0.00721218 0.010354558 9.916087e-04\nPink     0.78192585 0.000285946 0.02156349 0.195417315 8.073975e-04\n\n\nThis verifies that the modalities that contribute most to the formation of the first two axes all have a good representation quality (not always the case).\n\n\n\nc) Biological conclusion :\nFor the question of links between modalities in rows or columns, you need to know that :\n\nIf we have a positive sclaire product, i.e. less than 90 degree difference between the arrows of two different modalities. Then we have a conjunction representing an affinity between these two modalities.\nIf we have a negative scalar product, i.e. more than 90? difference between the arrows of two different modalities. Then we have an opposition of these two modalities. They reject each other.\nIf we have a zero scalar product, i.e. the arrows between two different modalities are perpendicular. Then there is no link between these two modalities.\n\n\nfviz_ca_biplot(CA)\n\n\n\n\nOn this graph, the rows are shown in blue and the columns in red. In other words, hair color in blue and eye color in red.\nFrom this analysis, we can see :\n\nA conjunction between black, pink, brown or blond hair and blue or brown eyes.\nA conjunction between yellow eyes and blue hair\nAn opposition between these two groups on axis 1.\n\nIn an analysis where the data are not created randomly, but come from different individuals from different populations. This could reveal a particular typology of one population in relation to another."
  },
  {
    "objectID": "AMV_chapter.html#introduction-2",
    "href": "AMV_chapter.html#introduction-2",
    "title": "Multivariate analysis",
    "section": "Introduction",
    "text": "Introduction\nMCA is a statistical method adapted to table of type “individuals x quatitative variable”. Eingen Values correspond to means of squared correlation ratios. It could be used a pre-processing before a classification or a coinertia analysis on tables with quantitative."
  },
  {
    "objectID": "AMV_chapter.html#mathematics-2",
    "href": "AMV_chapter.html#mathematics-2",
    "title": "Multivariate analysis",
    "section": "Mathematics",
    "text": "Mathematics\nThe MCA stands out from other multivariate analyzes with a single table (PCA, CA or mix between MCA and PCA, etc.) because it only takes qualitative variables as input. Its goal is to find the relationships between modalities by visualizing the possible associations and producing a quantitative indicator of their relationships.\nThe data taken by an MCA is a table comprising \\(J\\) qualitative variables to describe \\(I\\) statistical individuals. All individuals have the same weight \\(\\frac{1}{I}\\). The value \\(V_{i,j}\\) in the table corresponds to the modality taken by individual \\(i\\) for variable \\(j\\).\nTo obtain a complete disjunctive table (CDT) and carry out the statistical analysis, it is necessary to subdivide each variable \\(j\\) into \\(n_{j}\\) modalities corresponding to all of the responses \\(V_{.,j}\\) of the \\(I\\) individuals for this variable. The table will always have \\(I\\) rows but \\(\\sum_{j=1}^J {n_j}\\) columns (we will denote K columns), in which each of the \\(V_{i,k}\\) values is either 1 or 0 depending on the presence or absence of the modality.\nThe value \\(V_{i,k}\\) is transformed again : the old value of \\(V_{i,k}\\) (1 or 0) is divided by the probability \\(p_{k}\\) of having this modality (i.e. \\(p_k=\\frac{\\sum_{i=0}^I V_{i,k}}{I}\\), result from which we subtract 1 to center the result.\n\\[V'_{i,k}=\\frac{\\sum_{i=1}^I V_{i,k}}{I}-1\\]\nWe define the distance D between 2 individuals \\(i\\) and \\(i’\\) as : \\[D=\\frac{1}{J}\\sum_{k=1}^K \\frac{1}{p_{k}}(V'_{i,k}-V'_{i',k})^2\\]\nSo 2 individuals taking the all same modalities are at a distance D = 0, the more similar the responses are over a large number of modalities, the lower the distance D will be and conversely. If two individuals share a rare modality, the distance will be reduced to take into account the common specificity.\nThe inertia of the point cloud : \\(N = \\frac{K}{J} – 1\\). \\(η^2(x,y)\\) is the correlation ratio between 2 variables \\(x\\) and \\(y\\) and \\(η^2(F_s,k)\\) is the correlation ratio between the modality \\(k\\) and l’axe \\(F_s\\). The axis \\(F_1\\) is the one which maximize its correlation ratio with all the modalities \\(k\\) : \\[F_1=\\max(\\sum_{k=1}^K η^2(F_s,V_{.,k}))\\]"
  },
  {
    "objectID": "AMV_chapter.html#interpretation-2",
    "href": "AMV_chapter.html#interpretation-2",
    "title": "Multivariate analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThis part is a simple example of MCA using R. You can import R package using the code. Let’s have to look to fictive the data set we will be working on :\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhippo &lt;- read.table(file = \"https://raw.githubusercontent.com/AnsaldiL/MODE_reproduciblescience/master/hp.csv\", sep=';', header=TRUE)\nhippo=hippo[,-1]\nstr(hippo)\n\n'data.frame':   143 obs. of  4 variables:\n $ Age      : int  20 21 23 24 23 35 26 22 22 20 ...\n $ Sex      : chr  \"F\" \"M\" \"F\" \"F\" ...\n $ Frequency: chr  \"regularly\" \"rarely\" \"regularly\" \"regularly\" ...\n $ Lake     : chr  \"G1\" \"G3\" \"G1\" \"G3\" ...\n\n\nWater consumption behaviour of Hippopotamus was observed in Penjari National Park, Benin. Drinking frequency was evaluated by a technician and rated “rarely” or “regularly”. Sex is indicated with F for female and M for male. As there are 3 lakes in the park which are noted “G1”, “G2” and “G3”.\nWe are performing MCA with the package FactoMineR.\nLet’s have a look to Eigen Values :\n\nres.mca$eig\n\n      eigenvalue percentage of variance cumulative percentage of variance\ndim 1  0.4348479               32.61359                          32.61359\ndim 2  0.3391468               25.43601                          58.04960\ndim 3  0.3304681               24.78511                          82.83470\ndim 4  0.2288706               17.16530                         100.00000\n\nfviz_eig(res.mca, addlabels = TRUE)\n\n\n\n\nThe total variance of the data set is divided between four dimensions. The first dimension concentrate 32.6% of the total variance. The first plan gather 58% of the variance. You can better visualize the repartition of variance with the plot.\nHere, you can see the results for the individuals, their coordinates (\\(coord\\)), absolute contributions (\\(contrib\\)), and the quality of their projection (\\(cos2\\)).\n\nhead(res.mca$ind$coord)\n\n       Dim 1      Dim 2        Dim 3      Dim 4\n1 -0.2750115 -0.3282379  0.953675653  0.2177425\n2  1.1910863 -0.2970654 -0.817927621  0.0463482\n3 -0.2750115 -0.3282379  0.953675653  0.2177425\n4 -0.5234147 -0.6299231 -0.447176405  0.1119000\n5 -0.2750115 -0.3282379  0.953675653  0.2177425\n6 -0.6770613  0.6031855  0.005825436 -0.2073689\n\nhead(res.mca$ind$cos2)\n\n       Dim 1      Dim 2        Dim 3        Dim 4\n1 0.06632695 0.09448565 7.976083e-01 0.0415790594\n2 0.65134489 0.04051620 3.071527e-01 0.0009862576\n3 0.06632695 0.09448565 7.976083e-01 0.0415790594\n4 0.31017449 0.44925117 2.263977e-01 0.0141766670\n5 0.06632695 0.09448565 7.976083e-01 0.0415790594\n6 0.52978429 0.42047948 3.921931e-05 0.0496970050\n\nhead(res.mca$ind$contrib)\n\n      Dim 1     Dim 2        Dim 3       Dim 4\n1 0.1216265 0.2221539 1.924579e+00 0.144863948\n2 2.2814613 0.1819619 1.415677e+00 0.006563565\n3 0.1216265 0.2221539 1.924579e+00 0.144863948\n4 0.4405736 0.8181846 4.231479e-01 0.038259024\n5 0.1216265 0.2221539 1.924579e+00 0.144863948\n6 0.7371954 0.7502017 7.181106e-05 0.131389632\n\n\nCoordonates of the individuals are their position on the first plan. \\(cos2\\) represents the quality of the representation of the individuals on the first plan. Contribution is how the point contribute to the creation of different axis.\nYou can access the same information for the variable (instead of the individuals):\n\nres.mca$var$coord\n\n                Dim 1       Dim 2       Dim 3      Dim 4\nF         -0.47746388 -0.21701783  0.15112124 -0.3434173\nM          1.14818694  0.52187620 -0.36341060  0.8258369\nrarely     1.23505546 -0.11004338 -0.08731625 -0.8834510\nregularly -0.53107385  0.04731865  0.03754599  0.3798839\nG1         0.46448522 -0.40376182  1.45603286  0.2760402\nG2        -0.33088584  1.22351803 -0.17862073 -0.3340851\nG3        -0.02692834 -0.93083264 -0.95986346  0.1241336\n\nres.mca$var$contrib\n\n                Dim 1      Dim 2       Dim 3      Dim 4\nF         12.34263168  3.2693937  1.62699252 12.1316114\nM         29.68109047  7.8621135  3.91252964 29.1736370\nrarely    35.15982729  0.3578914  0.23124469 34.1811065\nregularly 15.11872574  0.1538933  0.09943522 14.6978758\nG1         4.62603750  4.4819347 59.81574581  3.1042541\nG2         3.05186316 53.5031214  1.17025431  5.9111247\nG3         0.01982416 30.3716520 33.14379781  0.8003904\n\nres.mca$var$cos2\n\n                 Dim 1       Dim 2       Dim 3       Dim 4\nF         0.5482177869 0.113256440 0.054919059 0.283606714\nM         0.5482177869 0.113256440 0.054919059 0.283606714\nrarely    0.6559056566 0.005207104 0.003278375 0.335608865\nregularly 0.6559056566 0.005207104 0.003278375 0.335608865\nG1        0.0837850567 0.063310139 0.823313280 0.029591524\nG2        0.0625631068 0.855426501 0.018231638 0.063778755\nG3        0.0004019773 0.480314347 0.510741640 0.008542035\n\n\nLet’s have a look to the plot of this MCA analysis :\n\nplot(res.mca)\n\n\n\n\nAs this is a factice data set, some individuals are overlapping. We will not focus on this artefact of the data set.\nHow to interprete these graphs?\nGeneral description : The individuals are in black. The variables are in red. The percentage of variance of the first two axis is written on them. Individuals in the center of the cloud are individuals taking a mean value for all of their caracteristics, unlikely individuals far from the middle which are specific individuals, very different from others. Close individuals present close characteristics.\nInterpretation: Here, we can see that the first axis separates individuals drinking regularly from individuals drinking rarely. The second axis separates individuals drinking at G2 from individuals drinking at G3. Male and female seamed to be separated by the first axis. As we can gather individuals sharing close properties, females seams to drink regularly and male more rarely. It is a bit less clear but male are drinking preferably in pound 1.\nNote that supplementary quantitaive data could be added to the analysis to help interpretation. As they are quantitatives, they would not participate to the creation of axis but they may be ploted on the final graph."
  },
  {
    "objectID": "AMV_chapter.html#introduction-3",
    "href": "AMV_chapter.html#introduction-3",
    "title": "Multivariate analysis",
    "section": "Introduction",
    "text": "Introduction\nTwo table analysis is used when the scientist possesses two data set with expecting the second to explain the first. The first one is called Y, the response variable and the second one is called X, the explanatory variable. For example :\nTable Y | Table X\na data set with temperature data | a data set with cities caracteristics such as heigth of buildings, concreted area, number of cars… |\na data set with the quantity of chemicals in soil | a data set with % of pesticide applied |\na data set with functional traits of an animal | a data set with the quantities of different food given to the animal |\nRDA is used when expecting a linear response from \\(X\\) to \\(Y\\). Only the variables of \\(X\\) explaining \\(Y\\) would be kept. The canonical axis are a linear combinaison of the explicative variable (\\(Y\\)). The \\(Y\\) table will be ordered with a PCA. Then, a multiple regression of \\(Y\\) on \\(X\\) is done and stored in a new table, \\(Y'\\). The residual matrix correspond to \\(Y-Y'\\), what was not explained by \\(X\\)."
  },
  {
    "objectID": "AMV_chapter.html#interpretation-3",
    "href": "AMV_chapter.html#interpretation-3",
    "title": "Multivariate analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere are the library you will need to perfor the RDA, make sure your import them at the begining of your working document.\nLet’s import the data, they are from @thilenius1963synecology. However, the data of abundance have been changed to facilitated the interpretation. For access to real data and metadata, please see the JDBakker Git page: appliedmultivariatestatistics/Oak_data_47x216.csv at main · jon-bakker/appliedmultivariatestatistics (github.com)\n\nOak &lt;- read.csv(\"https://raw.githubusercontent.com/AnsaldiL/MODE_reproduciblescience/master/Oak_data.csv\", sep=';', header = TRUE) \n\nTopo &lt;- Oak %&gt;%\nselect(LatAppx, LongAppx, Elev.m, Slope.deg) %&gt;%\ndecostand(\"range\")\n\nOak_sp=Oak[,28:83]\n\nWe can explore the tables\n\nhead(Oak_sp)\n\n  Abgr.s Abgr.t Acar Acgld.t Acma.s Acma.t Acmi Adbi Agha Agre Agse Agte Aica\n1      0      0    0       0      0      0    0    0    0    0    0    1    1\n2      5      0    0       0      1      0    0    0    0    0    0    0    0\n3      4      0    2       0      1      0    0    0    0    0    0    0    0\n4      0      0    0       0      0      0    0    0    0    0    0    0    1\n5      0      0    0       0      0      0    0    0    0    0    0    0    0\n6      0      0    0       1      0      0    0    0    0    0    0    0    0\n  ALL Alpr Amal.s Amal.t Apan Aqfo Arel Arme.s Arme.t Avfa Beaq.s Brco Brla\n1   0    2      0      0    0    0    0      0      0    0      0    0    1\n2   0    0      1      0    0    0    0      0      0    0      0    0    0\n3   0    0      0      0    0    0    0      0      0    0      0    0    0\n4   0    0      0      0    0    2    0      0      0    0      0    0    0\n5   0    0      0      0    0    0    1      0      0    0      0    0    0\n6   0    0      0      0    0    1    0      0      0    0      0    0    0\n  Brpu Brri Brse Brst Brvu Caqu CAR Cato Cear Ceum Ceve.s Cipa Civu Coco.s\n1    1    0    0    0    0    1   0    0    0    0      0    0    0      0\n2    0    0    0    0    0    1   0    0    0    0      0    0    0      0\n3    0    0    0    0    0    0   0    0    0    0      0    0    0      0\n4    1    0    0    0    0    3   0    0    0    1      1    0    0      0\n5    0    0    0    0    0    1   0    0    0    0      1    0    0      0\n6    0    0    0    0    0    0   0    0    0    0      1    0    0      0\n  Coco.t Cogr Conu.s Conu.t CORY.t Cost Crca Crdo.s Crdo.t Crox.s Cyec Cyfo\n1      0    0      0      0      0    0    0      0      0      0    0    0\n2      0    0      0      0      0    0    0      0      0      0    0    0\n3      0    0      0      0      0    0    0      0      0      0    0    0\n4      0    0      0      0      0    0    0      0      0      0    0    0\n5      0    0      0      0      0    0    0      0      0      0    0    0\n6      0    0      0      0      0    0    0      0      0      0    0    0\n  Cygr Daca Dacar Dagl\n1    0    0     0    0\n2    0    0     0    0\n3    0    0     0    0\n4    0    0     0    0\n5    0    0     0    0\n6    0    0     0    0\n\nhead(Topo)\n\n    LatAppx  LongAppx     Elev.m  Slope.deg\n1 0.3882609 1.0000000 0.06578947 0.03448276\n2 0.5073864 0.6239120 0.13157895 0.24137931\n3 0.5750381 0.7687530 0.33333333 0.20689655\n4 0.7392188 0.6721803 0.06578947 0.17241379\n5 0.8261238 0.9377551 0.33333333 0.34482759\n6 0.3309702 0.9046193 0.06578947 0.17241379\n\n\nSo here, the first table (“Oak_sp”) represents a table of plant abundance for 47 sites (47 rows). We have 56 differents species. The second table (“Topo”) corresponds to 4 topographics variables (Latitute, Longitute of sites, the elevation and the slope). As it is not the same unit for each variable we have centred-reduced these four variables.\nAs explained above, the CCA objective is to link the abundance of plant by the topographics variables. Here, we have a priori hypothesis on the effect of these environmental variables on the plants.\nWe have seen in introduction of the part 3 that CCA was used when the respond have a non-linear distribution but rather a gaussian relationship.\nSo we begin by performing a CA on the species table:\n\ncaoak &lt;- dudi.coa(Oak_sp,scannf=F,nf=2)\n\nThen, we can perform the PCA on the CA that corresponds to the CCA with the ADE4 package:\n\nccaoak &lt;- pcaiv(caoak,Topo,scannf=F,nf=2)\n\nYou can also realize the CCA with one code row only with the vegan package:\n\nccavegan=cca(Oak_sp,Topo,scan=F)\n\nIt is important to use the two methods because each allows us to have different information during the analyse of the result\nAn we already have completed our analyse, it was no difficult! But now, we need analyse the result and test the significant.\nTo test the significant, we use a bootstrapping test thank you the ADE4 package.It is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between are different tables. To produce this, we perform a permutation test with the function randtest.\n\nrandtest(ccaoak)#with ADE4\n\nMonte-Carlo test\nCall: randtest.pcaiv(xtest = ccaoak)\n\nObservation: 0.1270957 \n\nBased on 99 replicates\nSimulated p-value: 0.01 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n3.5741057845 0.0836576869 0.0001477081 \n\nplot(randtest(ccaoak))\n\n\n\n\nThe outputs above corresponds to the permutation test. We see that the number of permutation of columns and rows was to 999 (default value).\nThe results of this test shows that our result is significatively different to the permutation result with a threshold of 5% (p-value = 0.02)\nFinally, the results of the CCA are plot in the distribution law calculated by the permutation. So we can conclude that our environmental variables explain a part of the plant distribution.\nWe have now to look the collinearity between the environmental variable. It corresponds to the correlation between them. For that, we use the Variance Inflation Factor (VIF).\n\ntest=vif.cca(ccavegan) \ntest\n\n  LatAppx  LongAppx    Elev.m Slope.deg \n 1.072988  1.115539  1.169013  1.111402 \n\n\nIf all the variables have a number lower than 10, you can conclude that you do not not have collinearity. If you have, you need to select some variables to remove with, for example, an ordistep function.\nNow that we have tested all the conditions, we can look at the results.\n\nccaoak\n\nCanonical correspondence analysis\ncall: pcaiv(dudi = caoak, df = Topo, scannf = F, nf = 2)\nclass: caiv pcaiv dudi \n\n$rank (rank)     : 4\n$nf (axis saved) : 2\n\neigen values: 0.3446 0.179 0.1231 0.05905\n\n vector length mode    content                \n $eig   4      numeric eigen values           \n $lw    47     numeric row weigths (from dudi)\n $cw    56     numeric col weigths (from dudi)\n\n data.frame nrow ncol content                             \n $Y         47   56   Dependant variables                 \n $X         47   4    Explanatory variables               \n $tab       47   56   modified array (projected variables)\n\n data.frame nrow ncol content                               \n $c1        56   2    PPA Pseudo Principal Axes             \n $as        2    2    Principal axis of dudi$tab on PAP     \n $ls        47   2    projection of lines of dudi$tab on PPA\n $li        47   2    $ls predicted by X                    \n\n data.frame nrow ncol content                                  \n $fa        5    2    Loadings (CPC as linear combinations of X\n $l1        47   2    CPC Constraint Principal Components      \n $co        56   2    inner product CPC - Y                    \n $cor       4    2    correlation CPC - X                      \n\nprint(paste(\"The pourcentage of variance explained by topographic variables is: \",sum(ccaoak$eig)/sum(caoak$eig),\"%\"))\n\n[1] \"The pourcentage of variance explained by topographic variables is:  0.127095660396168 %\"\n\n\nThe first output (by ADE4) allows us to see each part of the analyse that could be use to describe the result. We will use especially the last part afterward. You also have, at the beginning, the number of rank that corresponds to the number of eigenvalue with their score below. We can calculate the percentage of variance explained by topographic variables.\n\nccavegan\n\nCall: cca(X = Oak_sp, Y = Topo, scan = F)\n\n              Inertia Proportion Rank\nTotal          5.5529     1.0000     \nConstrained    0.7057     0.1271    4\nUnconstrained  4.8471     0.8729   38\nInertia is scaled Chi-square \n17 species (variables) deleted due to missingness\n\nEigenvalues for constrained axes:\n  CCA1   CCA2   CCA3   CCA4 \n0.3446 0.1790 0.1231 0.0590 \n\nEigenvalues for unconstrained axes:\n   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 \n0.7256 0.5739 0.4521 0.4240 0.3563 0.2448 0.2239 0.1857 \n(Showing 8 of 38 unconstrained eigenvalues)\n\n\nHere, it is the output with the vegan package. We can see the total inertia in the CCA analyse (5.55) and the inertia explained by the topographic variables (0.71) and by the residuals (4.85). We can therefore conclude that our topographic variables explain only 13% of the plant variances (same result that before). Below that, we have the eigenvalues for the constrained axes (variance part explained by topographic variables) and for the unconstrained axes (variance part explained by the residuals)\nWe can now, observe the complet plot of the CCA result:\n\nplot(ccaoak)\n\n\n\n\nHere, we can have a complete plot with the ADE4 package.\n\nThe “Loadings” part represents the canonical coefficients. For each axis, the arrows explain the relative weight of the topographic variables in the multiple regression calculation.\nThe “Correlation” part shows the correlation between topographic variables themselves and with CCA axes\nThe “Scores and Predictions” part allows us to understand, for each site, the real plant abundance (base of arrows) and the abundance predicted by multiple regressions with the topographic variables.\nThe “Species” part corresponds to the coordinates of each plant species in the CCA analyse\n\nBut this can also be decomposed in different plots and output.We can detail each part and observe the biological results.\nWe can begin by the absolute contribution of species:\n\ncontrib=inertia.dudi(ccaoak, col = TRUE, row = FALSE)\ncontrib$col.abs\n\n               Axis1        Axis2\nAbgr.s  6.021847e+00  1.212813788\nAbgr.t  3.177359e+01 10.422451480\nAcar    2.635049e+00  0.134855688\nAcgld.t 7.493068e+00  1.910539707\nAcma.s  2.515315e-02  1.689674506\nAcma.t  8.936196e-01  1.898419377\nAcmi    3.919403e-02  0.004891136\nAdbi    9.970574e+00  5.143409094\nAgha    8.209713e-01 17.598302739\nAgre    8.571153e-01  1.890912149\nAgse    1.299772e-01  8.924213094\nAgte    4.233778e+00  1.403202784\nAica    3.007289e+00  0.000614369\nALL     1.076835e-01  6.374919825\nAlpr    1.413603e-01  0.237212759\nAmal.s  5.831519e-02  0.289243172\nAmal.t  2.843090e-02  2.844815278\nApan    6.791621e+00  1.385883395\nAqfo    6.407529e+00 11.512960665\nArel    3.725579e+00  2.142662004\nArme.s  0.000000e+00  0.000000000\nArme.t  1.002592e-05  2.416236998\nAvfa    1.389095e-01  1.919937151\nBeaq.s  1.385684e+00  4.664898207\nBrco    0.000000e+00  0.000000000\nBrla    1.570857e+00  0.757779956\nBrpu    4.750564e-01  0.266026605\nBrri    8.163005e-01  1.287295464\nBrse    1.292897e-01  0.143416542\nBrst    0.000000e+00  0.000000000\nBrvu    4.429482e+00  0.016067778\nCaqu    1.841713e-02  1.290440906\nCAR     6.964536e-01  0.036573989\nCato    1.206790e+00  0.522162462\nCear    9.936004e-01  0.303130230\nCeum    8.493802e-01  0.392276059\nCeve.s  1.019629e+00  2.122835911\nCipa    1.182233e-04  1.942686629\nCivu    0.000000e+00  0.000000000\nCoco.s  0.000000e+00  0.000000000\nCoco.t  9.551865e-01  0.003571735\nCogr    5.754591e-02  1.919957503\nConu.s  0.000000e+00  0.000000000\nConu.t  0.000000e+00  0.000000000\nCORY.t  0.000000e+00  0.000000000\nCost    0.000000e+00  0.000000000\nCrca    0.000000e+00  0.000000000\nCrdo.s  0.000000e+00  0.000000000\nCrdo.t  5.203645e-02  0.143607303\nCrox.s  0.000000e+00  0.000000000\nCyec    0.000000e+00  0.000000000\nCyfo    0.000000e+00  0.000000000\nCygr    4.350974e-02  2.829101566\nDaca    0.000000e+00  0.000000000\nDacar   0.000000e+00  0.000000000\nDagl    0.000000e+00  0.000000000\n\n\nThe output, here, is the absolute contribution of each species for the axis 1 and the axis 2. This allows us to observe what species contributes the most of the axes.\n\nccaoak$fa\n\n                      RS1           RS2\n(Intercept) -1.448845e-16  1.304775e-16\nLatAppx      5.546487e-01 -1.321836e-01\nLongAppx     8.933635e-02  1.113464e-01\nElev.m      -2.378758e-02 -2.828042e-01\nSlope.deg    2.306181e-02  3.754766e-01\n\ns.corcircle(ccaoak$fa)\n\n\n\n\nThis line gives you the canonical coefficients on each axis. The circle is explain in the “Loadings” part of the main plot of the RDA. On axis 1, the variable with the main contribution of the predictive power is \\(LatAppx\\). On axis 2, \\(Slope.deg\\) and \\(Elev.m\\) contributes the most of the predictive power.\n\nccaoak$co\n\n               Comp1        Comp2\nAbgr.s  -0.344559610 -0.111446061\nAbgr.t  -1.499805193  0.619091450\nAcar    -0.405170072 -0.066061105\nAcgld.t -0.657447384 -0.239263999\nAcma.s   0.098964466 -0.584591822\nAcma.t  -0.527599532  0.554232875\nAcmi     0.093384329 -0.023775904\nAdbi    -1.970348894 -1.019944979\nAgha    -0.376925736  1.257752946\nAgre    -0.408495885 -0.437292359\nAgse     0.318149791  1.899989382\nAgte     0.560360412  0.232504722\nAica     0.510110670  0.005254841\nALL      0.204765928  1.135503753\nAlpr     0.135452232  0.126461872\nAmal.s  -0.213102669  0.342056435\nAmal.t  -0.210430331 -1.517079157\nApan     0.566164741  0.184326417\nAqfo     0.455971525 -0.440508302\nArel     0.525654360  0.287308446\nArme.s   0.000000000  0.000000000\nArme.t   0.002281469 -0.807217082\nAvfa     0.328900132  0.881271477\nBeaq.s   0.555259619  0.734265751\nBrco     0.000000000  0.000000000\nBrla     0.451534247  0.226027947\nBrpu     0.384680895  0.207471640\nBrri     0.797302436  0.721614627\nBrse     0.259080296 -0.196661887\nBrst     0.000000000  0.000000000\nBrvu     0.656643322  0.028503562\nCaqu    -0.043729888 -0.263818130\nCAR     -0.520749860  0.086007708\nCato     0.969424379  0.459588372\nCear     0.470186239 -0.187174556\nCeum     0.575087756 -0.281674227\nCeve.s  -0.476305048 -0.495325172\nCipa    -0.009595104 -0.886477227\nCivu     0.000000000  0.000000000\nCoco.s   0.000000000  0.000000000\nCoco.t   0.862466323 -0.038010697\nCogr     0.172846131 -0.719558962\nConu.s   0.000000000  0.000000000\nConu.t   0.000000000  0.000000000\nCORY.t   0.000000000  0.000000000\nCost     0.000000000  0.000000000\nCrca     0.000000000  0.000000000\nCrdo.s   0.000000000  0.000000000\nCrdo.t   0.201303763 -0.241020771\nCrox.s   0.000000000  0.000000000\nCyec     0.000000000  0.000000000\nCyfo     0.000000000  0.000000000\nCygr     0.184073508 -1.069770155\nDaca     0.000000000  0.000000000\nDacar    0.000000000  0.000000000\nDagl     0.000000000  0.000000000\n\ns.label(ccaoak$co,boxes=F)\n\n\n\n\n\\(co\\) gives you access to the coordinates of each species on the first plan. The table explicits the coordinates and the plot help you to find close species. On the first axis, two species have high negative value : Adbi (-1.97) & Abgr.t (-1.50). They are opposed at species as Coco.t (0.86) & Cato (0.96). You can realize the same for the second axis.\nHere, we can highlight that there is a group formed by Agse (coord = 0.31 ; 1.90 respectively for the axis 1 and 2), Agha (coord = -0.38 ; 1.25) Avfa (coord = 0.33 ; 0.88) and ALL (coord = 0.20 ; 1.14). An other on with two outliers, Abgr.t and Adbi. These groups will be correlated with the environment data further in the analysis.\n\nccaoak$cor \n\n                 RS1         RS2\nLatAppx    0.9867311 -0.09977934\nLongAppx   0.3726389  0.33053940\nElev.m    -0.1505108 -0.45170572\nSlope.deg  0.1235033  0.65338800\n\ns.corcircle(ccaoak$cor)\n\n\n\n\nHere, we have the correlation between axes and the topographic variables. We can conclude that the elevation (corr = -0.15[axis1] ; -0.45[axis2]) and the slope (corr = 0.12[axis1] ; 0.65[axis2]) are strongly negatively correlated. The Longitude seems to be positively correlated with the slope and negatively correlated with the elevation.\n\ns.match(ccaoak$ls, ccaoak$li)\n\n\n\nplot(ccavegan,scaling=1) \n\n\n\nplot(ccavegan,scaling=2) \n\n\n\n\nThe first plot with the arrows represents the real abundances for each site (base of the arrow) and the predictions of abundance calculated by a multiple regression thanks to the topographic variables. We see here that there are some large differences between reality and prediction. Most of the sites are in a center group but some sites differ. It is the case with the sites n°7,40,41,42. We have another group with the sites n°43,44,45,46.\nThe second plot, with scaling = 1 preserves euclidian distances between stations.\nThe third plot, with scaling = 2 preserves correlations between species.\nTo conclude on the CCA analyse, we have seen that the topographic variables explain 13% of the plant variances. The first axis represents 34% of the constrained inertia and is explained by the Latitude (corr = 0.99). We can see that two species have a high coordinate in this axis (Adbi & Abgr.t).\nThe second axis (18% of the constrained inertia) is explained by the Elevation (corr = -0.45), the Slope (corr = 0.65) and the Longitude (corr = 0.33).\nFinally, the sites n°43,44,45,46 are sites with a small latitude value and with a high abundance of species as Abgr.t."
  },
  {
    "objectID": "AMV_chapter.html#introduction-4",
    "href": "AMV_chapter.html#introduction-4",
    "title": "Multivariate analysis",
    "section": "Introduction",
    "text": "Introduction\nThe RLQ analysis is a generalized co-inertia with 3 tables : environmental variables for each sites (R), species abundances for each sites (L) and trait variables for each species (Q). We will have to perform a CA on the table L and weighted analysis (MCA or PCA) on tablesR and Q. Then a function named rlq() perform 3 co-inertia analysis between R and L and L and Q and then between the results of each co-inertia."
  },
  {
    "objectID": "AMV_chapter.html#interpretation-4",
    "href": "AMV_chapter.html#interpretation-4",
    "title": "Multivariate analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThe use of RLQ analysis is important in ecology to integrate the traits of species with the environmental variables. So here, we don’t have 2 tables (environment & specie) as RDA part but 3 tables:\n\nenvironmental variables by sites (R)\nabundance of species by sites (L)\ntrait values by species (Q)\n\nTo perform the RLQ, we need to decompose the analyse by three type of analyses already done in this chapter:\n\nwe will use a CA analyse on the abundance of species\nwe will use a MCA on the environmental table by taking the sites weight on the CA\nwe will use a MCA on the trait table by taking the species weight on the CA\n\nHere, we use MCA for R and Q because our variables are factors but you can perform a PCA if your variables are quantitatives. Warning, for R and Q you have the obligation to weight by the L table (see below).\nYou can import R package using the code\n\nlibrary(tidyverse)\nlibrary(ade4)\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(factoextra)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(RVAideMemoire)\n\n*** Package RVAideMemoire v 0.9-83-7 ***\n\nlibrary(PerformanceAnalytics)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\n\nAttaching package: 'PerformanceAnalytics'\n\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\n\nHere, we work with a dataset of ADE4 package. The data comes from @dray2008testing, @legendre1997relating\n\n#import the dataset\ndata(aviurba)\n\n#create the three tables\nsummary(aviurba$mil)    #(R)\n\n farms    small.bui high.bui industry fields   grassland scrubby  deciduous\n yes:22   yes:13    yes: 8   yes:12   yes:22   yes:12    yes:30   yes:36   \n no :29   no :38    no :43   no :39   no :29   no :39    no :21   no :15   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n conifer  noisy      veg.cover\n yes: 8   yes:10   R37    :9  \n no :43   no :41   R5     :8  \n                   R98    :7  \n                   R93    :7  \n                   R22    :6  \n                   R87    :5  \n                   (Other):9  \n\nR&lt;-aviurba$mil\n\nsummary(aviurba$fau)    #(L)\n\n      Sp1               Sp2              Sp3               Sp4        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.05882   Mean   :0.1373   Mean   :0.07843   Mean   :0.7647  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :4.0000  \n      Sp5              Sp6              Sp7              Sp8       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :3.000  \n Mean   :0.1176   Mean   :0.2353   Mean   :0.1569   Mean   :2.078  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:3.000  \n Max.   :2.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n      Sp9              Sp10             Sp11             Sp12        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.5882   Mean   :0.6275   Mean   :0.3922   Mean   :0.03922  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :3.0000   Max.   :4.0000   Max.   :4.0000   Max.   :1.00000  \n      Sp13              Sp14              Sp15             Sp16       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.05882   Mean   :0.03922   Mean   :0.1961   Mean   :0.3725  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :2.0000   Max.   :1.0000  \n      Sp17              Sp18             Sp19             Sp20       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.07843   Mean   :0.4118   Mean   :0.7059   Mean   :0.2157  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :2.0000   Max.   :2.0000  \n      Sp21              Sp22             Sp23              Sp24        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.00000  \n Mean   :0.03922   Mean   :0.3725   Mean   :0.07843   Mean   :0.09804  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :1.00000  \n      Sp25              Sp26             Sp27             Sp28        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.03922   Mean   :0.1961   Mean   :0.2353   Mean   :0.03922  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.0000   Max.   :2.0000   Max.   :1.00000  \n      Sp29           Sp30             Sp31             Sp32       \n Min.   :0.00   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:2.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :3.00   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :2.49   Mean   :0.6667   Mean   :0.2353   Mean   :0.8627  \n 3rd Qu.:3.00   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :4.00   Max.   :4.0000   Max.   :2.0000   Max.   :3.0000  \n      Sp33             Sp34              Sp35             Sp36      \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.00000   Median :1.0000   Median :0.000  \n Mean   :0.2941   Mean   :0.07843   Mean   :0.5686   Mean   :0.451  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :2.00000   Max.   :3.0000   Max.   :3.000  \n      Sp37              Sp38             Sp39              Sp40       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.05882   Mean   :0.1765   Mean   :0.03922   Mean   :0.4118  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :3.0000  \n\nL&lt;-aviurba$fau\n\nsummary(aviurba$traits) #(Q)\n\n   feed.hab    feed.strat     breeding     migratory \n insect:19   ground :27   ground  : 6   resident:23  \n grani :12   aerial : 3   building:14   migrant :17  \n omni  : 9   foliage:10   scrub   :12                \n                          foliage : 8                \n\nQ&lt;-aviurba$traits\n\nand explore the tables\n\nhead(R)  \n\n   farms small.bui high.bui industry fields grassland scrubby deciduous conifer\nR1   yes        no       no       no    yes        no      no       yes      no\nR2   yes        no       no       no     no       yes     yes        no      no\nR3    no        no       no       no    yes        no     yes       yes      no\nR4    no        no       no       no    yes        no     yes        no      no\nR5    no        no       no      yes     no        no     yes        no      no\nR6    no        no       no      yes     no       yes     yes        no      no\n   noisy veg.cover\nR1    no       R98\nR2    no       R87\nR3    no      R100\nR4    no      R100\nR5    no        R5\nR6   yes        R5\n\nhead(L)\n\n   Sp1 Sp2 Sp3 Sp4 Sp5 Sp6 Sp7 Sp8 Sp9 Sp10 Sp11 Sp12 Sp13 Sp14 Sp15 Sp16 Sp17\nR1   0   0   0   0   0   0   1   0   0    0    0    0    0    0    0    1    0\nR2   0   0   0   0   0   0   1   0   0    0    0    0    1    0    0    0    0\nR3   0   2   0   0   0   0   1   0   1    0    0    0    0    1    0    0    0\nR4   0   0   0   0   0   0   1   0   1    0    0    0    0    0    1    0    0\nR5   0   0   0   0   0   0   0   0   2    0    0    0    0    0    1    1    0\nR6   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    1    0\n   Sp18 Sp19 Sp20 Sp21 Sp22 Sp23 Sp24 Sp25 Sp26 Sp27 Sp28 Sp29 Sp30 Sp31 Sp32\nR1    1    1    0    0    1    0    0    0    1    0    0    4    0    0    1\nR2    2    1    0    0    1    0    1    0    1    0    0    4    4    1    1\nR3    2    0    0    0    1    0    0    0    1    1    0    1    0    1    0\nR4    2    0    0    0    1    0    1    0    0    2    0    1    0    1    1\nR5    1    0    0    0    1    0    0    0    1    1    0    1    1    1    0\nR6    0    0    0    0    0    0    0    0    0    0    0    4    1    0    1\n   Sp33 Sp34 Sp35 Sp36 Sp37 Sp38 Sp39 Sp40\nR1    1    0    1    0    0    0    0    1\nR2    1    0    0    0    0    0    0    0\nR3    1    0    0    0    0    0    0    1\nR4    1    0    0    0    0    1    0    0\nR5    0    0    0    0    0    0    0    0\nR6    1    2    1    2    0    0    0    0\n\nhead(Q)\n\n    feed.hab feed.strat breeding migratory\nSp1     omni     ground  foliage   migrant\nSp2     omni     ground building   migrant\nSp3     omni     ground   ground   migrant\nSp4    grani     ground building  resident\nSp5    grani     ground    scrub  resident\nSp6    grani     ground  foliage  resident\n\n\nThe first part is to perform our CA on specie table\n\nafcL &lt;- dudi.coa(log(L+1), scannf = FALSE) \nafcL  \n\nDuality diagramm\nclass: coa dudi\n$call: dudi.coa(df = log(L + 1), scannf = FALSE)\n\n$nf: 2 axis-components saved\n$rank: 39\neigen values: 0.3987 0.2736 0.1917 0.1624 0.1421 ...\n  vector length mode    content       \n1 $cw    40     numeric column weights\n2 $lw    51     numeric row weights   \n3 $eig   39     numeric eigen values  \n\n  data.frame nrow ncol content             \n1 $tab       51   40   modified array      \n2 $li        51   2    row coordinates     \n3 $l1        51   2    row normed scores   \n4 $co        40   2    column coordinates  \n5 $c1        40   2    column normed scores\nother elements: N \n\n\nThe first CA is done. We use log transformation because the abundance of species has a large range and we add “+1” to avoid the log(0) for some species. You must adapt the presence of transformation (or not) to your data.\nNow, we can perform the two MCA analysis on the trait table and environmental table\n\nacmR &lt;- dudi.acm(R, row.w = afcL$lw, scannf = FALSE,nf = 4)\nscatter(acmR)\n\n\n\nacmQ &lt;- dudi.acm(Q, row.w = afcL$cw, scannf = FALSE,nf = 4)\nscatter(acmQ)\n\n\n\n\nThe scatterplot allows to see the ordination of each table and the repartition of factor on the simple axe of MCA.\nBut now, we will use the RLQ analyse that creates two co-inertia (R-L, L-Q), assembles and compares the co-inertia. We use the rlq function for that.\n\nrlq &lt;- rlq(acmR, afcL, acmQ, scannf = FALSE)\nrlq\n\nRLQ analysis\ncall: rlq(dudiR = acmR, dudiL = afcL, dudiQ = acmQ, scannf = FALSE)\nclass: rlq dudi \n\n$rank (rank)     : 8\n$nf (axis saved) : 2\n\neigen values: 0.01068 0.003128 0.001525 0.0004536 0.0001298 ...\n\n  vector length mode    content                    \n1 $eig   8      numeric Eigenvalues                \n2 $lw    28     numeric Row weigths (for acmR cols)\n3 $cw    12     numeric Col weigths (for acmQ cols)\n\n   data.frame nrow ncol content                                      \n1  $tab       28   12   Crossed Table (CT): cols(acmR) x cols(acmQ)  \n2  $li        28   2    CT row scores (cols of acmR)                 \n3  $l1        28   2    Principal components (loadings for acmR cols)\n4  $co        12   2    CT col scores (cols of acmQ)                 \n5  $c1        12   2    Principal axes (loadings for acmQ cols)      \n6  $lR        51   2    Row scores (rows of acmR)                    \n7  $mR        51   2    Normed row scores (rows of acmR)             \n8  $lQ        40   2    Row scores (rows of acmQ)                    \n9  $mQ        40   2    Normed row scores (rows of acmQ)             \n10 $aR        4    2    Corr acmR axes / rlq axes                    \n11 $aQ        4    2    Corr afcL axes / coinertia axes              \n\naxe=c(1:8)\nprint(paste(\"The contribution of axe n°\",axe, \"are\", rlq$eig/(sum(rlq$eig))*100,\"%\"))\n\n[1] \"The contribution of axe n° 1 are 66.6748744763001 %\"  \n[2] \"The contribution of axe n° 2 are 19.5228230896623 %\"  \n[3] \"The contribution of axe n° 3 are 9.51702542395437 %\"  \n[4] \"The contribution of axe n° 4 are 2.83090411660832 %\"  \n[5] \"The contribution of axe n° 5 are 0.810200444830495 %\" \n[6] \"The contribution of axe n° 6 are 0.421072849575587 %\" \n[7] \"The contribution of axe n° 7 are 0.124897113199049 %\" \n[8] \"The contribution of axe n° 8 are 0.0982024858697611 %\"\n\n#randtest(rlq)\n#summary(rlq)\n#plot(rlq)\n\nHere, the output of the RLQ is complex but only few information are, at this point important. We see that we have 8 eigenvalues and we have their values. All the different compounds of the output will be used after in representations or analysis.\nNevertheless, we can calculate the contribution of each axis of the RLQ by performing the formule below: \\[\\frac{rlq$eig}{\\sum{rlq$eig}}.100\\]\nAfter that, and before to plot and analyse the result, it is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between are different tables. To produce this, we perform a permutation test with the function randtest.\n\nrandtest(rlq)\n\nclass: krandtest lightkrandtest \nMonte-Carlo tests\nCall: randtest.rlq(xtest = rlq)\n\nNumber of tests:   2 \n\nAdjustment method for multiple comparisons:   none \nPermutation number:   999 \n     Test        Obs  Std.Obs   Alter Pvalue\n1 Model 2 0.01602442 9.199803 greater  0.001\n2 Model 4 0.01602442 2.514658 greater  0.017\n\nplot(randtest(rlq))\n\n\n\n\nThe outputs above corresponds to the permutation test. We see that the number of permutation of columns and rows was to 999 (default value).\nThe results of this test shows that the permutation of sites (rows) is the first result (p_value=0.1%) and the permutation of species (columns) is the second result (p_value=1.9%) So each result is significant (5% threshold) and we can conclude that our RLQ result is not linked to random effect.\nFinally, the results of the RLQ are plot in the distribution law calculated by the permutation.\nWe can know analyse the result by using different types of plot:\n\nplot(rlq)\n\n\n\n\nThe plot above is the complet plot, difficult to understand and that will be resumed step by step afterward.\nHowever, we have here, up the score (the position) of the sites and species in this analyse. Below in the centre, we have the correlation of the environmental variables between them (R Canonical weights) and the correlation of the traits between them (Q Canonical weight). The plots of the R axes and the Q axes represent the reprojection of axis in the RLQ analyse in order to the simple analyses (CA or MCA here).\nNow, we can decompose the result and start with the environmental variables:\n\n#analyse the environmental variables:\nround(rlq$l1,dig=2)\n\n                 RS1   RS2\nfarms.yes      -0.11  0.73\nfarms.no        0.10 -0.65\nsmall.bui.yes  -2.27 -0.53\nsmall.bui.no    0.71  0.16\nhigh.bui.yes   -2.17 -0.48\nhigh.bui.no     0.35  0.08\nindustry.yes    0.43  2.63\nindustry.no    -0.11 -0.67\nfields.yes      1.34  0.52\nfields.no      -1.34 -0.52\ngrassland.yes  -1.07  0.26\ngrassland.no    0.30 -0.07\nscrubby.yes    -0.10 -0.91\nscrubby.no      0.14  1.37\ndeciduous.yes  -0.56 -0.64\ndeciduous.no    1.36  1.55\nconifer.yes    -0.87  0.91\nconifer.no      0.17 -0.18\nnoisy.yes      -2.61  0.67\nnoisy.no        0.43 -0.11\nveg.cover.R100  4.35 -4.95\nveg.cover.R98   1.74 -1.64\nveg.cover.R93   0.13  3.16\nveg.cover.R87  -0.94 -0.98\nveg.cover.R62  -1.86 -0.35\nveg.cover.R37  -2.01 -0.10\nveg.cover.R22  -2.91  0.05\nveg.cover.R5    1.04  3.24\n\ns.label(rlq$l1, boxes=FALSE)\n\n\n\n#calcul of the absolute contribution\niner=inertia.dudi(rlq,col.inertia=T,row.inertia=T)\n\nabscoiE=iner$row.abs\nabscoiE\n\n                     Axis1        Axis2\nfarms.yes       0.05223514  2.304444147\nfarms.no        0.04623764  2.039853800\nsmall.bui.yes  11.21784230  0.599492102\nsmall.bui.no    3.52200412  0.188219231\nhigh.bui.yes    5.99404001  0.297237022\nhigh.bui.no     0.97600864  0.048399060\nindustry.yes    0.33291147 12.714009779\nindustry.no     0.08422136  3.216444135\nfields.yes      8.17323519  1.238389579\nfields.no       8.20233957  1.242799407\ngrassland.yes   2.29313411  0.132425116\ngrassland.no    0.64653934  0.037336694\nscrubby.yes     0.04972304  4.526463864\nscrubby.no      0.07475839  6.805519570\ndeciduous.yes   2.01878003  2.632197876\ndeciduous.no    4.90713304  6.398193444\nconifer.yes     1.13513324  1.240985532\nconifer.no      0.22330026  0.244123224\nnoisy.yes       8.82492060  0.589761352\nnoisy.no        1.47048833  0.098271387\nveg.cover.R100 16.95851693 21.972519979\nveg.cover.R98   4.57563062  4.054631466\nveg.cover.R93   0.02364930 14.452989048\nveg.cover.R87   0.82853117  0.891676376\nveg.cover.R62   3.46745570  0.119501611\nveg.cover.R37   5.23067194  0.012183942\nveg.cover.R22   7.43352189  0.002336584\nveg.cover.R5    1.23703663 11.899594674\n\n\nThe table corresponds to the position of each modality for each environmental variables on the two first axis. You can link this table to the plot. For example, “veg.cover.R100” seems to stand out from the other modalities. In the table, we can see that the position on the first axis is 4.35 and -4.95 on the second axis. Closer to the center of the cloud, we can talk about “veg.cover.R98” that have position of 1.74 on the first axis and -1.64 on the second axis. In addition, we can observe the contribution of this two variables. On the second table we have the absolute contribution of the modalities for each variable. The contribution of the “veg.cover.R100” of the first axis is 17% and 22% on the second axis. This is a high value if we take the threshold to 1/N with N the number of modalities (28 here so 3%). The contribution of “veg.cover.R98” is 4.6% for the first axis and 4.1% for the second axis.\nWe can therefore conclude that this two variables are correlated. This seems to be biologically coherent since these two variables are part of a plant cover gradient and are the two highest values.\nWe can also see that the variable negatively correlated with the first axis are modalities explaining the urbanization ()\nNow we can realise the same analyse for the traits.\n\n#analyse the traits:\nround(rlq$c1,dig=2)\n\n                     CS1   CS2\nfeed.hab.insect     0.72  0.73\nfeed.hab.grani     -0.69 -0.24\nfeed.hab.omni      -0.13 -1.64\nfeed.strat.ground  -0.11  0.01\nfeed.strat.aerial  -1.26  1.24\nfeed.strat.foliage  2.60 -1.97\nbreeding.ground     3.60  4.16\nbreeding.building  -1.16  0.35\nbreeding.scrub      1.71 -2.22\nbreeding.foliage   -0.15 -0.62\nmigratory.resident -0.24 -0.23\nmigratory.migrant   0.54  0.50\n\ns.arrow(rlq$c1, boxes=FALSE)\n\n\n\n#absolute contribution\nabscoiV=iner$col.abs\nabscoiV\n\n                         Axis1        Axis2\nfeed.hab.insect     5.69187893  5.861738276\nfeed.hab.grani      5.10226145  0.632420595\nfeed.hab.omni       0.05897841  8.814467814\nfeed.strat.ground   0.22470905  0.003581384\nfeed.strat.aerial   7.09044249  6.818080636\nfeed.strat.foliage 19.80381068 11.362659415\nbreeding.ground    28.19988784 37.628640395\nbreeding.building  17.18536604  1.552603443\nbreeding.scrub     13.29440922 22.370738798\nbreeding.foliage    0.11612276  2.116775390\nmigratory.resident  1.00414733  0.881790779\nmigratory.migrant   2.22798580  1.956503077\n\n\nAs seen before, the first table is the position of the modalities for the two first axes of the RLQ analyses. The plot is the representation of that and the second table is the absolute contributions.\nHere, we have larger differences between the different variable and modalities. We’re going to focus on a few examples, but you can take the time to go into detail about each modality. “feed.strat.foliage” (coord= 2.60; -1.97 for first and second axis respectively) and “breeding.scrub” (coord= 1.71: -2.22) break away from the centre of the cloud. In addition their contributions are all greater than 10%. We can conclude that these modalities are correlated and highly contribute of axes.\nThe last point is to observe the species coordinates.\n\n#analyse the species:\nround(rlq$lQ,dig=2)\n\n     AxcQ1 AxcQ2\nSp1   0.04 -0.44\nSp2  -0.22 -0.19\nSp3   0.97  0.76\nSp4  -0.55 -0.03\nSp5   0.17 -0.67\nSp6  -0.30 -0.27\nSp7   0.71 -0.24\nSp8  -0.29  0.70\nSp9   0.99  1.17\nSp10 -0.29  0.70\nSp11 -0.29  0.70\nSp12  1.19  1.35\nSp13 -0.41 -0.38\nSp14  1.19  1.35\nSp15  1.19  1.35\nSp16 -0.20  0.22\nSp17  0.00  0.40\nSp18  0.71 -0.24\nSp19  0.31 -1.02\nSp20  1.39 -0.74\nSp21  1.39 -0.74\nSp22  1.20 -0.92\nSp23  1.39 -0.74\nSp24  1.39 -0.74\nSp25  0.48 -0.28\nSp26  0.48 -0.28\nSp27  0.64  0.93\nSp28  0.17 -0.67\nSp29 -0.55 -0.03\nSp30 -0.55 -0.03\nSp31 -0.30 -0.27\nSp32 -0.30 -0.27\nSp33  0.38 -0.76\nSp34  0.85 -1.16\nSp35 -0.30 -0.27\nSp36 -0.20  0.22\nSp37  1.18 -1.33\nSp38 -0.16 -0.62\nSp39 -0.41 -0.38\nSp40 -0.16 -0.62\n\ns.label(rlq$lQ,boxes=FALSE)\n\n\n\ns.label(rlq$lQ, label=aviurba$species.names.fr,boxes=FALSE)\n\n\n\n\nHere, is to see that we seems to have a gradient of response along the first axis (i.e. Loriot Jaune/Sp37 [coord=1.18; -1.33] and Alouette des champs/Sp9 [coord=0.99; 1.17]) and along the second axis (i.e. Loriot Jaune/Sp37 [coord=1.18; -1.33] and Fauvette des jardins/Sp21 [coord=0.99; 1.17]). We will see that we need to link this to the others variables (trait and environment) For species and sites, we do not have contribution because these variables do not contribute of the axis creation.\nTo conclude on this analysis, the two first axes explain 86.2% of the total variation with respectively 66.7%, and 19.5% of the total inertia.\nThe correlations between the environmental variables and the RLQ axes showed that the first axis is explained by Veg.cover.R100 (contribution of 16.95%), veg.cover.R22 (7.43%), noisy.yes (8.82%), small.bui.yes (11.22%), field.yes (8.17%), field.no (8.2%) and Veg.cover.R22 (7.43%). The traits variables that contribute to the first axis of the RLQ axes are breeding.ground (28.19%), feed.strat.foliage (19.8%), breeding.scrub (13.3%) and negatively with breeding.building (17.18%) and feed.strat.aerial (7.09%). In other words, breeding.ground, feed.strat.foliage, breeding.scrub, feed.strat.aerial and breeding.building were the higher explanatory attributes for this RLQ axis (explained by the length and the angle between the axes and the vectors)."
  },
  {
    "objectID": "r_chapter.html",
    "href": "r_chapter.html",
    "title": "The Companion book a M1 MODE student should have!",
    "section": "",
    "text": "title: “Linear modelling in ecology Léa”\nbibliography: references.bib execute: freeze: auto output: html_document: toc: true toc_float: true —\nThis chapter is a simple example using R\nYou can import R package using the code\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nand then describe the purpose of your chapter as well as executing R command.\nFor example a basic summary of a dataset is given by\n\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n\nand produce a graph\n\ndf %&gt;% ggplot() +\n    aes(x=species, y = body_mass_g) +\n    geom_boxplot()  \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nA citation @bauer2023writing"
  },
  {
    "objectID": "r_chapter.linear_modelling.html",
    "href": "r_chapter.linear_modelling.html",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "lwskjglm This chapter is a simple example using R\nYou can import R package using the code\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nand then describe the purpose of your chapter as well as executing R command.\nFor example a basic summary of a dataset is given by\n\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n\nand produce a graph\n\ndf %&gt;% ggplot() +\n    aes(x=species, y = body_mass_g) +\n    geom_boxplot()  \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nA citation (bauer2023writing?)"
  },
  {
    "objectID": "Mixed_models.html",
    "href": "Mixed_models.html",
    "title": "Mixed models in ecology",
    "section": "",
    "text": "library(nlme)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(predictmeans)\n\nLoading required package: glmmTMB\n\n\nWarning in checkMatrixPackageVersion(getOption(\"TMB.check.Matrix\", TRUE)): Package version inconsistency detected.\nTMB was built with Matrix version 1.6.4\nCurrent Matrix version is 1.6.1.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\n\nLoading required package: lmerTest\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(sp)"
  },
  {
    "objectID": "Mixed_models.html#introduction",
    "href": "Mixed_models.html#introduction",
    "title": "Mixed models in ecology",
    "section": "Introduction",
    "text": "Introduction\nGeneral Linear Models, such as linear regressions, ANOVA, and ANCOVA, are commonly employed to depict the relationships between a dependent variable, denoted as (Y), and one or more independent variables (\\(X_1, X_2, ..., X_n\\)).  These models are based on several assumptions, including homoscedasticity of the variance, non-collinearity of residuals, and normality of residuals. Generalized Linear Models (GLMs) can address homoscedasticity and normality assumptions by accommodating data from different distributions like Poisson, binomial, or Gamma distributions, which are often encountered in ecology.   However, it is crucial to validate the non-collinearity of residuals.\nIn biological and ecological experiments, the assumption of independence of measurements, necessary for non-collinearity of residuals, is frequently violated.  This is because measurements are often correlated within families, regions, repeated on the same individuals, or across time and sites. In such cases, it becomes necessary to employ mixed models. These models, extensions of both general and generalized linear models, consider the correlation of measurements by introducing individuals, regions, families, or other factors as random effects in the models. This incorporation allows for a more accurate representation of the complex dependencies present in the data.\nWhat is a random effect, and how do I determine if my effect is random or fixed ?\nTo clarify the distinction between fixed and random effects, let’s examine two examples:\n\nExample 1: Comparing individual cars\nAbdel, Antonio, Odeline, and Aela want to compare the oil consumption of their individual cars. They conduct a test by measuring oil consumption during a 30-kilometer drive, repeated five times in a day, with consistent traffic conditions and driving patterns.  The data set consists of one factor with four levels (representing the four cars) and five replicates each. Performing a one-way ANOVA allows them to determine which car is the most economical. In this scenario, the factor “car” is fixed, and the analysis provides conclusions specific to the four studied cars.\nExample 2: Assessing homogeneity within a car model\nA car constructor aims to evaluate the homogeneity of oil consumption within a car model, treating the model as a population of cars with expected heterogeneity in gas consumption.  Similar to Example 1, they measure oil consumption by driving each car 30 kilometers, five times in a day, resulting in a data set with one factor and four levels, each with five replicates. Unlike the first example, the cars in this case were sampled from a larger population, and the objective is to draw conclusions about the entire population, not just the sampled cars. Here, a mixed model with the factor ‘car’ as a random factor should be used.\n\nIn summary, a factor is designated as fixed when the experimenter intentionally chooses a limited number of levels for investigation, aiming to assess the impact of each level on the response variable. A factor is considered random when the selected levels represent only a sample from all possible levels. In this case, the objective is to understand the variability in the response variable attributed to this factor.\nFor example, let’s consider a researcher investigating the influence of the number of training sessions per week on the concentration of red blood cells in recreational athletes. The researcher collects data from 50 athletes in a local club who train between 1 and 5 times a week. Initially planning a simple ANOVA with the number of training sessions as the main factor, the researcher discovers that most athletes in the data set belong to only 10 families, leading to non-independent measurements. To address this issue, the researcher opts for a mixed model, treating the number of training sessions as a fixed factor and the family as a random factor. This approach allows the exploration of variability between families without the intention of directly comparing them.\nNow that we have a general understanding of what mixed models are, we can delve into the mathematical formalism of these models. In this chapter, you will discover how matrices can be employed to create mixed models, explore the various dependency structures that exist, and ultimately, find an implementation of mixed models in R."
  },
  {
    "objectID": "Mixed_models.html#formalization-of-the-linear-mixed-model",
    "href": "Mixed_models.html#formalization-of-the-linear-mixed-model",
    "title": "Mixed models in ecology",
    "section": "1. Formalization of the linear mixed model",
    "text": "1. Formalization of the linear mixed model\nThe linear mixed model can be formulated as follows:\n\\[\nY_{i} = \\beta X_{i} + \\gamma_i Z_{i} + \\varepsilon_{i}\n\\]\nwhere:\n\n\\(Y_i = n_i \\times 1\\) measurements for subject \\(i\\), where \\(n\\) is the number of observations\n\\(X_i = n_i \\times p\\) matrix of vectors for fixed effects, where \\(p\\) is the number of fixed parameters\n\\(\\beta_i= p \\times 1\\) parameters for fixed effects\n\\(Z_i = n_i \\times p\\) matrix of vectors for random effects\n\\(\\gamma_i = r \\times 1\\) parameters for random effects, where \\(p\\) is the number of random parameters\n\\(\\varepsilon_i = n_i \\times 1\\) residuals for individual \\(i\\)\n\n(Giorgi 2020)\nA mixed effects model incorporates random effects (\\(\\gamma_i\\)), or a combination of both random and fixed effects (\\(\\beta\\)), whereas a standard linear model includes only fixed effects.\nWhen it is clear that the researcher intends to compare particular, predefined levels of a treatment, those levels are considered fixed effects. Conversely, when the levels of the treatment are drawn from a larger population of possible levels, the treatment is treated as a random effect.\nIn addition, random effects are included in a model when there is a correlation or dependence among the observations that cannot be ignored.\nRANDOM VARIABLE = “something that could not be known before sampling/measurement/observation”.\nIn matrix form, the mixed model is written as:\n\\[\nY \\sim \\mathcal{N{n}}(X\\theta, \\Sigma)\n\\] where:\n\\(Y\\) is the response vector of the observations, \\(X\\theta\\) is the expectation of the response vector \\(Y\\) and \\(\\Sigma\\) is the variance matrix.\nWe can note that if the response vector \\(Y\\) is of dimension \\(n\\), the matrix \\(\\Sigma\\) is of dimensions \\(n \\times n\\). Since \\(\\Sigma\\) is symmetric, it comprises \\(n(n + 1)/2\\) parameters. This is because, in a symmetric matrix, the elements above (or below) the main diagonal are the same as those below (or above), reducing the total number of parameters needed to describe the matrix.\nHowever, the limitation of the available data prevents considering models where all these \\(n(n + 1)/2\\) parameters are free. This restriction arises from the need to have a significant amount of data to reliably estimate each parameter, which quickly becomes unrealistic with a limited dataset.\nTo address this issue, the linear mixed-effects model proposes an approach where a structure is imposed on the variance matrix \\(\\Sigma\\). This structure, governed by a limited number of parameters called “variance parameters,” denoted \\(\\psi\\), reduces the number of parameters needed to describe the covariance matrix. Consequently, the model can be realistically adapted even with a limited amount of data, while accounting for the correlation between observations within the framework of linear mixed-effects models. The model parameters include \\(\\theta\\) for the expectation and \\(\\psi\\) for the variance."
  },
  {
    "objectID": "Mixed_models.html#matrix-computation-in-mixed-models",
    "href": "Mixed_models.html#matrix-computation-in-mixed-models",
    "title": "Mixed models in ecology",
    "section": "2. Matrix computation in mixed models",
    "text": "2. Matrix computation in mixed models\nIt is possible to encounter (mixed) linear models written under their matrix form, for their concision. It is therefore natural to present this form in the context of mixed models.\nAs a reminder, a linear model, like linear regression, with \\(p\\) explanatory variables can be written \\[y_i = \\beta_0 + \\beta_1x_i^{(1)} + \\ldots + \\beta_px_i^{(p)} + \\varepsilon_i\\], where \\(y_i\\) represent an observation of the response variable \\(Y\\) for the individual \\(i\\), \\(\\beta_0\\) the intercept, \\(\\beta_1,...,\\beta_p\\) the coefficients associated to each explanatory variable \\(X_1,...,X_p\\), \\(x_i^{(1)},...,x_i^{(p)}\\) the \\(p\\) observations (for the \\(p\\) explanatory variables) for the individual \\(i\\), and \\(e_i\\) an error term associated to the individual \\(i\\).  We can see \\(e_i\\) as a realization of a random variable \\(E_i\\) distributed according to a normal law \\(\\mathcal{N}(0,\\sigma^2)\\). Noting \\[y=\\begin{pmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\\\\\n\\end{pmatrix}\\], \\[X=\\begin{pmatrix}\n1&x_1^{(1)} & \\ldots & x_1^{(p)}\\\\\n\\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & x_n^{(1)} & \\ldots & x_n^{(p)}\n\\end{pmatrix}\\], \\[\\theta=\\begin{pmatrix}\n\\beta_0\\\\\n\\vdots\\\\\n\\beta_p\\\\\n\\end{pmatrix}\\] and \\[e=\\begin{pmatrix}\n\\varepsilon_1\\\\\n\\vdots\\\\\n\\varepsilon_n\\\\\n\\end{pmatrix}\\], we can rewrite the previous model under the form \\[y=X\\theta+e\\]Here, \\(e\\) is a vector of \\(n\\) independent realizations or a random variable \\(E_i\\) following a normal distribution \\(\\mathcal{N}(0,\\sigma^2)\\).  Hence, \\(e\\) is a realization of a random variable \\(E\\) following the distribution \\(\\mathcal{N}_n(0,\\sigma^2I_n)\\) (\\(e_i\\) is an observation of the random variable \\(E_i\\) distributed according to a normal law \\(\\mathcal{N}(0,\\sigma^2)\\)).  Similarly, \\(y\\) is an observation of \\(Y=X\\theta+E\\) where \\(Y\\sim\\mathcal{N}_n(X\\theta,\\sigma^2I_n)\\) (\\(y_i\\) is an observation of \\(Y_i\\) distributed according to a normal law \\(\\mathcal{N}((X\\theta)_i,\\sigma^2)\\)). Hence, by introducing \\(Y\\) and \\(E\\), the previous model can be written \\(Y=X\\theta+E\\) where \\(\\mathrm{E}\\stackrel{iid}\\sim\\mathcal{N}_n(0,\\sigma^2I_n)\\).\nBy definition, the mean response is equal to \\(X\\theta\\), more or less an error term equals to 0 in average but that varies of \\(\\sigma^2I_n\\). Thus, we can write \\[Y\\sim\\mathcal{N}_n(X\\theta,\\sigma^2I_n)\\]We note that when writing \\(\\mathrm{E}\\stackrel{iid}\\sim\\mathcal{N}_n(0,\\sigma^2I_n)\\), each error has got the same variance (\\(\\sigma^2\\)) because every samples are independent. We will see subsequently that if this independence condition is not respected, all the errors do not have the same variance, we speak of a dependence structure. The dependence between the measurements determines the dependence structure (measurements repeated over time, individuals grouped by common ancestry, etc.). We will see here that in the context of mixed models, the previous equation is written \\[Y\\sim\\mathcal{N}_n(X\\theta,\\sum)\\]. When writing this, we note that the average response does not change. Generalization concerns errors. Now, let’s study study the components of variance \\(\\sum\\) by following an example. We decide to study the heritability of a trait (height for example). We want to see if individuals from one ascendant are more similar than those from another ascendant. We have \\(m\\) ascendants, numbered \\(i=1,...,m\\), from a larger population, each having \\(n\\) descendants numbered \\(j=1,...,n\\). We pose \\(Y_{ij}=\\) the trait value for the j-ème descendant of the i-ème ascendant. Individuals with the same ascendant are therefore grouped by their belonging to the same ascendant. The fact of sampling ancestors from a larger population (random effect) introduces a correlation between the traits measured on the descendants of the same ancestor. This correlation is uniform between individuals. The descendants between the groups (for different ancestors) are independent. Thus, we write \\[\n\\mathbb{Cov}(Y_{ij},Y_{i'j'}) = \\left\\{\n    \\begin{array}{ll}\n        \\gamma^2 & \\mbox{si } i=i'\\\\\n        0 & \\mbox{sinon.}\n    \\end{array}\n\\right.\n\\] This model therefore includes a variance associated with the ascendant effect \\(\\gamma^2\\) and a residual variance \\(\\sigma^2\\). We precise that \\(\\gamma^2\\) represent the variability between the ascendants. Starting from the linear model \\(Y=X\\theta+E\\) where \\(E\\) is still \\(\\sigma^2I_n\\), we just have to take into account this variance associated with the ascendant effect. To do this, we introduce the matrix \\(Z\\), of dimension \\(n\\times m\\), where \\[\nZ_{a,i} = \\left\\{\n    \\begin{array}{ll}\n        1 & \\mbox{l'individu } a=(i,j) \\mbox{ est le descendant de l'ascendant }i\\\\\n        0 & \\mbox{sinon.}\n    \\end{array}\n\\right.\n\\] and the vector \\(U\\) of dimension \\(m\\) including the random effects \\(\\gamma^2\\). Hence, the previous model can be rewritten \\(Y=X\\theta+ZU+E\\). We remind that \\(U\\) and \\(E\\) are independents, gaussiens centered, and that \\(\\mathbb{Var}(U)=\\gamma^2I_m\\). Consequently, \\(\\mathbb{E}(Y)=X\\theta\\) and \\(\\sum = \\mathbb{V}(Y)=Z\\mathbb{V}(U)Z'+\\mathbb{V}(E) = \\gamma^2ZZ'+\\sigma^2(I_n)\\). We get back to the matrix form \\(Y\\sim\\mathcal{N}(X\\theta,\\sum)\\) with \\[\\sum=\\begin{pmatrix}\nR&0 & \\ldots & 0\\\\\n0 & \\ddots & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & 0 \\\\\n0 & \\ldots & 0 & R\n\\end{pmatrix}\\], where \\[R=\\begin{pmatrix}\n\\sigma^2+\\gamma^2&\\gamma^2 & \\ldots & \\gamma^2\\\\\n\\gamma^2 & \\ddots & \\ddots & \\gamma^2 \\\\\n\\vdots & \\ddots & \\ddots & \\gamma^2 \\\\\n\\gamma^2 & \\ldots & \\gamma^2 & \\sigma^2+\\gamma^2\n\\end{pmatrix}\\]\\(R\\) corresponds to the representation of the variances due to the random effect and residuals. Note that the diagonal blocks are of the same dimensions if the number of descendants is identical for each ascendant."
  },
  {
    "objectID": "Mixed_models.html#dependency-structures",
    "href": "Mixed_models.html#dependency-structures",
    "title": "Mixed models in ecology",
    "section": "3. Dependency structures",
    "text": "3. Dependency structures\nThis part of the chapter is inspired by (L. Bel 2016)’s book. \n\n3.1 Case of repeated measurements\nWe want to evaluate the effect of different diet on weight gain in rats. Several animals (\\(j\\)) follow each diet (\\(i\\)), and they kept the same diet across all the experiment. Each week animal weight (\\(Y_{ij}\\)) is measured, during T weeks. In this case, measures are repeated across time, such measurements are called longitudinal data.  To analyse this data, the temporal dependency must be taken into account, for this, the following model can be used: \nModel The model proposed here takes into account the kinetic aspect of the experiment and predicts that the dependence between two measurements depends on the time interval between them.    We assume that the weights are Gaussian with the following expected values :\n\\[ E(Y_{ijt}) = µ + α_i + γ_t + (αγ)_{it}.\\]\nDependency structure We assume that all measurements have the same variance\n\\[ V(Y_{ijt} =  σ^2) \\]\nand the covariance between them is\n\\[Cov(Y_{ijt}, Y_{i'j't'}) = \\left\\{\n  \\begin{array}{ll}\n  σ^2ρ^{|t-t'|} \\ \\ si \\ \\ (i, j) = (i', j') \\\\\n     0 \\ sinon \\\n   \\end{array}\n  \\right.\\]\nThis structure assumes that measurements made on different animals are independent. It is also assumed that |ρ| &lt; 1, which implies that the longer the time interval, the less correlated the tests on the same animal. This form of covariance corresponds to an autoregressive process of order 1, generally denoted AR(1). This model has two variance parameters: the temporal correlation ρ and the variance of each observation \\(σ^2\\).\n\\[\n  ψ = \\left( {\\begin{array}{cc}\n    ρ \\\\\n    σ^2 \\\\\n  \\end{array} } \\right)\n\\]\nBecause of the independence between the measurements obtained on different animals, the variance matrix Σ also has the same diagonal block shape, but the block (R) differs.\n\\[  \nR = \\left( {\\begin{array}{cc}\n     σ^2 & σ^2ρ & σ^2ρ^2 & ... & σ^2ρ^{T-1}\\\\\n     σ^2ρ & ... & ... & ... & ...\\\\\n     σ^2ρ^2 & ... & σ^2 & ... & σ^2ρ^2\\\\\n     ... & ... & ... & ... & σ^2ρ\\\\\n    σ^2ρ^{T-1} & ... & σ^2ρ^2 & σ^2ρ & σ^2\\\\\n  \\end{array} } \\right)\n\\]\nWe are going to use the BodyWeight dataset from the nlme package. In this dataset, weight is measured on 16 rats every 7 days during 64 days (which gives 11 measurements for each rats). Three diets are tested with 88 rats following diet 1 and 44 following diet 2 and 3.\nThe research question is: Is weight gain different depending on the diet? The model will be a mixed model with diet and time as fixed factor and individuals as random factor: weight ~ Diet * Time | Rat.\n\nhead(BodyWeight, 10)\n\nGrouped Data: weight ~ Time | Rat\n   weight Time Rat Diet\n1     240    1   1    1\n2     250    8   1    1\n3     255   15   1    1\n4     260   22   1    1\n5     262   29   1    1\n6     258   36   1    1\n7     266   43   1    1\n8     266   44   1    1\n9     265   50   1    1\n10    272   57   1    1\n\n\n\ntime_model = gls(weight ~ Diet * Time, data = BodyWeight, correlation = corAR1(form = ~ 1 | Rat))\nsummary(time_model)\n\nGeneralized least squares fit by REML\n  Model: weight ~ Diet * Time \n  Data: BodyWeight \n       AIC      BIC    logLik\n  1152.248 1177.334 -568.1239\n\nCorrelation Structure: AR(1)\n Formula: ~1 | Rat \n Parameter estimate(s):\n      Phi \n0.9895746 \n\nCoefficients:\n                Value Std.Error   t-value p-value\n(Intercept) 250.11069 13.327526 18.766475  0.0000\nDiet2       203.46561 23.083952  8.814158  0.0000\nDiet3       260.91151 23.083952 11.302723  0.0000\nTime          0.37351  0.090964  4.106160  0.0001\nDiet2:Time    0.62384  0.157554  3.959506  0.0001\nDiet3:Time    0.18780  0.157554  1.191998  0.2349\n\n Correlation: \n           (Intr) Diet2  Diet3  Time   Dt2:Tm\nDiet2      -0.577                            \nDiet3      -0.577  0.333                     \nTime       -0.222  0.128  0.128              \nDiet2:Time  0.128 -0.222 -0.074 -0.577       \nDiet3:Time  0.128 -0.074 -0.222 -0.577  0.333\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.44446363 -0.63038331  0.02804647  0.25238087  2.93319234 \n\nResidual standard error: 37.70413 \nDegrees of freedom: 176 total; 170 residual\n\n\nIn this example, the time correlation ρ has a value of 0.989. The sequence of weight values was highly correlated. The model allowed this temporal dependency to be taken into account. \n\n\n3.2 Case of spatial autocorrelation\nDependency structure We want to take into account the dependency due to the possible spatial proximity between the sites at which the measurements were taken.\nTo do this, \\(d(i, i')\\) is the distance separating sites \\(i\\) and \\(i'\\), and the following equation is used\n\\[Cov(Y_i, Y_i{'}) = e^{−δ.d(i,i')}\\]\nAs in the case of repeated measurements, there is no simple way of writing this in terms of random effects. Moreover, since all the measurements are dependent, the matrix Σ is no longer diagonal per block and is written as :\n\\[\n  Σ = \\left( {\\begin{array}{cc}\n     σ^2 + γ^2 & e^{−δ.d(i,i')}& ...& e^{−δ.d(i,i')}\\\\\n     e^{−δ.d(i,i')}& σ^2 + γ^2 & e^{−δ.d(i,i')} & \\vdots\\\\\n     \\vdots & e^{−δ.d(i,i')}  & σ^2 + γ^2 & e^{−δ.d(i,i')}\\\\\ne^{−δ.d(i,i')}& \\ldots & e^{−δ.d(i,i')} & σ^2 + γ^2\\\\\n  \\end{array} } \\right)\n\\]\n\ndata.spatialCor.glsExp &lt;- gls(y ~ x, data = data.spatialCor,\n    correlation = corExp(form = ~LAT + LONG, nugget = TRUE),\n    method = \"REML\")\n\n\nsummary(data.spatialCor.glsExp)\n\nGeneralized least squares fit by REML\n  Model: y ~ x \n  Data: data.spatialCor \n       AIC      BIC    logLik\n  974.3235 987.2484 -482.1618\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~LAT + LONG \n Parameter estimate(s):\n    range    nugget \n1.6956723 0.1280655 \n\nCoefficients:\n               Value Std.Error  t-value p-value\n(Intercept) 65.90018 21.824752 3.019516  0.0032\nx            0.94572  0.286245 3.303886  0.0013\n\n Correlation: \n  (Intr)\nx -0.418\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.6019483 -0.3507695  0.1608776  0.6451751  2.1331505 \n\nResidual standard error: 47.68716 \nDegrees of freedom: 100 total; 98 residual\n\n\nIn spatially correlated data, three coefficients are needed to describe the correlation structure. The variance increases with increasing distance up to a point the sill. The span of distances over which points are correlated is called the range. While we might expect the value of variance at a distance of zero to be zero, in reality we rarely have sampling units that approach such a small distance from one another.The value of variance when distance is equal to zero is the nugget.\nHere, in our example, the value of the sill, the range and the nugget are respectively 47.68, 1.69 and 0.12."
  },
  {
    "objectID": "Mixed_models.html#application",
    "href": "Mixed_models.html#application",
    "title": "Mixed models in ecology",
    "section": "4. Application",
    "text": "4. Application\nFor this example of a mixed model application, we will use a general linear mixed model. This is a special case of a general linear model, in which the response is quantitative and the predictor variables are both quantitative and qualitative, and the model includes random factors to take account of data dependency. Mixed models must respect the normality of residuals and the homogeneity of variances. (The structure and lines of code are inspired by (Outreman 2023)’s courses on linear mixed model.)\n\n4.1 Dataset presentation and objectives of the analysis\nFor this worked exercise, we will use data from a study performed on penguins. The aim of this study is to test whether species, sex and island influence the body mass of penguins. In the experimental design, the data are collected from one year to the next (from 2007 to 2009), which suggests that the penguin body mass data are dependent on each other from one year to the next. This dependency will be included in the model. The data contains:\n- species: three species of penguins (Chinstrap, Adelie, or Gentoo), categorical variable\n- island: island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica), categorical variable\n- sex: penguin sex (female, or male), categorical variable\n- year: years of data collection (2007, 2008, or 2009), continuous variable\n- body_mass_g: body mass of the penguins (in grams), continuous variable\n\nThe response variable is the ‘body_mass_g’, while ‘species’ and ‘island’ and ‘sex’ are assumed predictors. To include data dependency, ‘year’ will be included as random factor in model. The underlying question for this research is: do the species, island and sex drive the body mass of penguins?\nData import :\n\n# Data import\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n# Make sure that our variables 'species', 'island' and 'sex' are all factors in the choice.\ndf$species=as.factor(df$species)\ndf$island=as.factor(df$island)\ndf$sex=as.factor(df$sex)\n\n# Check for missing values\ncolSums(is.na(df))\n\n            rowid           species            island    bill_length_mm \n                0                 0                 0                 2 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                2                 2                 2                11 \n             year \n                0 \n\n\nWe can see that there are some missing values, including 2 for the response variable \\(Y\\) ‘body_mass_g’ and 11 for the explanatory variable \\(X\\) ‘sex’. We’re going to delete the rows with the missing values.\n\n# Rows with missing values are marked.\nwhich(is.na(df$body_mass_g), arr.ind=TRUE)\n\n[1]   4 272\n\nwhich(is.na(df$sex), arr.ind=TRUE)\n\n [1]   4   9  10  11  12  48 179 219 257 269 272\n\n# We delete the rows 4, 9, 10, 11, 12, 48, 179, 219, 257, 269, and 272.\ndf=df[-c(4,9,10,11,12,48,179,219,257,269,272), ]\n\n# Check for missing values\ncolSums(is.na(df))\n\n            rowid           species            island    bill_length_mm \n                0                 0                 0                 0 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                0                 0                 0                 0 \n             year \n                0 \n\n# Ok\n\n\n\n4.2 Data exploration\nBefore any statistical analysis, it is ESSENTIAL to explore the data in order to avoid any errors. Here is the list of explorations to be carried out before modelling:\n\nCheck for outliers in \\(Y\\) and the distribution of \\(Y\\) values.\nIf \\(X\\) is an independent quantitative variable, check for the presence of outliers in X and the distribution of the values of X. 2b. 2b. If \\(X\\) is a qualitative independent variable, analyse the number of levels and the number of individuals per level.\nAnalyse the potential relationships between \\(Y\\) and the \\(X_{s}\\).\nCheck for the presence of interactions between \\(X_{s}\\).\nCheck for collinearity between \\(X_{s}\\).\n\n\n4.2.1 Outliers and distribution of \\(Y\\)\n\npar(mfrow=c(2,2))\n# Boxplot\nboxplot(df$body_mass_g,col='blue',ylab='Masse corporel')\n# Cleveland plot\ndotchart(df$body_mass_g,pch=16,col='blue',xlab='Masse corporel')\n# Histogram\nhist(df$body_mass_g,col='blue',xlab=\"Masse corporel\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(df$body_mass_g,pch=16,col='blue',xlab='')\nqqline(df$body_mass_g,col='red')\n\n\n\n\nHere, the Boxplot and Cleveland Plot show no individuals with outliers. The Cleveland Plot shows us that there appears to be a group of individuals with a body mass between 5000 and 6000g, while the rest is between 3000 and 4000g. The Histogram and the QQ Plot show that \\(Y\\) hardly follows a Normal distribution… This is not very important, as the validity of the model is based, among other things, on the normality of the residuals, which we will demonstrate later.\n\n\n4.2.2 Outliers in \\(Xs\\)\n\nFor \\(Xs\\) which are quantitative: check for outliers and distribution\n\nNo quantitative predictor here.\n\nFor categorical \\(Xs\\): number of levels and number of individuals per level.\n\n\n# Factor Species\nsummary(df$species)\n\n   Adelie Chinstrap    Gentoo \n      146        68       119 \n\n# Factor Island\nsummary(df$island)\n\n   Biscoe     Dream Torgersen \n      163       123        47 \n\n# Factor Sex\nsummary(df$sex)\n\nfemale   male \n   165    168 \n\n\nThe ‘species’ variable has 3 levels: Adelie, Chinstrap and Gentoo. The number of individuals between the 3 levels is not balanced, with fewer individuals for the Chinstrap species.\nThe ‘island’ variable has 3 levels: Biscoe, Dream and Torgersen. The number of individuals between the 3 levels is not balanced, with fewer individuals for the Torgersen island.\nThe ‘sex’ variable has 2 levels: female and male. The number of individuals per level is close to equilibrium.\n\n\n4.2.3 Analysis of potential relationships Y vs Xs\nWe can graphically analyse the possible relationships between Y and X. Please note that this graphical analysis of the relationships between Y and X in no way predicts the importance of the relationship. Statistical modelling is the only way to identify relationships.\n\npar(mfrow=c(1,1))\n# Species\nplot(df$body_mass_g~df$species,pch=16,col='blue',xlab='Espèces',ylab='Masse corporel en g')\n\n\n\n# Islands\nplot(df$body_mass_g~df$island,pch=16,col='blue',xlab='Îles',ylab='Masse corporel en g')\n\n\n\n# Sex\nplot(df$body_mass_g~df$sex,pch=16,col='blue',xlab='Sexe',ylab='Masse corporel en g')\n\n\n\n\nIn terms of species, we can see that Gentoo has a higher body mass (between 5000 and 6000g) than the other two species (between 3000 and 4000g). About the islands, we can see that the individuals present on Biscoe have a higher body mass (between 5000 and 6000g) than the individuals present on the other two islands (between 3000 and 4000g). Finally, in terms of sex, males appear to have a slightly higher body mass than females.\n\n\n4.2.4 Analysis of possible interactions between the three independent variables\nHere, we will consider the interaction between the three factors studied. To estimate the presence of interactive effects, we develop a graphical approach. Remember that the interaction between factors can only be tested if the factors are crossed (i.e. all the levels of one treatment are represented in all the levels of the other treatment and vice versa = a factorial design). This point must be tested first.\n\n# The experimental design means that the factors are cross-tabulated (all the levels of each variable are represented in all the levels of the other variables). \n\n# Interaction table\ntable(df$species,df$sex,df$island)\n\n, ,  = Biscoe\n\n           \n            female male\n  Adelie        22   22\n  Chinstrap      0    0\n  Gentoo        58   61\n\n, ,  = Dream\n\n           \n            female male\n  Adelie        27   28\n  Chinstrap     34   34\n  Gentoo         0    0\n\n, ,  = Torgersen\n\n           \n            female male\n  Adelie        24   23\n  Chinstrap      0    0\n  Gentoo         0    0\n\n# Interaction table without island\ntable(df$species,df$sex)\n\n           \n            female male\n  Adelie        73   73\n  Chinstrap     34   34\n  Gentoo        58   61\n\n# Interaction Species:Sex\nboxplot(df$body_mass_g~df$species*df$sex, varwidth = TRUE, xlab = \"Species.Sex\", ylab = \"Body mass\", col='blue2', main = \"\",cex.axis=0.7)\n\n\n\n\nIn the interaction table, we can see that in every island, not all the species are represented. This can be complicated to analyse. We’ll see in the next section if we remove the “Island” variable. We can see that without the ‘island’ variable, there is the same number of female and male in each species, except for the Gentoo species, with 58 female and 61 male.\nFor the interaction graph (still without the ‘island’ variable), we can see that males of all 3 species appear to have a greater body mass than females. We can also see that males and females of the Gentoo species have a higher body mass than individuals of the other species.\n\n\n4.2.5 Check collinearity between X\nColinearity refers to the situation in which two or more predictors of collinearity are closely related to each other.The presence of collinearity can pose problems in the context of regression, as it can be difficult to separate the individual effects of collinear variables on the response.\nHere, we will test for collinearity between our 3 predictor variables:\n\n# ploting Species by Island\nggplot(df, aes(x=species, y=island)) +\n  geom_point() +\n  theme_bw() -&gt; p1\n\n# ploting Species by Sex\nggplot(df, aes(x=species, y=sex)) +\n  geom_point() +\n  theme_bw() -&gt; p2\n\n# ploting Island by Sex\nggplot(df, aes(x=island, y=sex)) +\n  geom_point() +\n  theme_bw() -&gt; p3\n\n# Ploting side-by-side\nmarrangeGrob(list(p1,p2,p3), nrow=1, ncol=3, top=NULL)\n\n\n\n\nIn our example, we can see that for the interaction between Species and Sex, there are two sex modalities per species, and for the interaction between Island and Sex, there are two sex modalities per island. However, for the interaction between Species and islands based on, not all the islands contain all the species, as we saw in the previous section! We cannot therefore test the influence of islands and species on the basis of this result. We therefore decided to remove the Island variable from our analysis. We will test the influence of species and sex on the body mass of penguins, always with years as a random effect.\n\n\n\n4.3 Statistical analysis\n\n4.3.1 Model construction\nFor statistical modelling, we first analyse the full model (model containing all the independent variables to be tested).\nTo obtain the candidate model (a model containing only the significant terms) from the full model, we will use the BACKWARD SELECTION METHOD, i.e. model selection based on the significance of the terms. In this approach, we start by creating the full model with all the variables of interest, then drop the least significant variable as long as it is not significant. We continue by successively fitting reduced models and applying the same rule until all the remaining variables are significant. The deletion of non-significant terms must follow the following two steps: - First, insignificant interactions are successively removed. - Secondly, the non-significant main effects are successively removed. A main effect is only removed if it is insignificant AND if it is not contained in a significant interaction.\nIn this example, we consider a measure of dependence at year level (e.g. a mass measurement made in 2009 depends on the measurement made in 2008, which in turn depends on the measurement made in 2007). The presence of the random effect of the year will be integrated not with the lm function, but lme (from the nlme package).\n\n# Full model\nmod1=lme(body_mass_g~species\n              + sex\n              + species:sex\n              ,random=~1|year\n              ,data=df)\n\nWe can see from the anova output of our full model that each interaction and each variable is significant (&lt;0.05). The full model is therefore the candidate model.\n\n\n4.3.2 Model’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\nLinear mixed-effects model fit by REML\n  Data: df \n       AIC      BIC    logLik\n  4718.236 4748.556 -2351.118\n\nRandom effects:\n Formula: ~1 | year\n        (Intercept) Residual\nStdDev:   0.0140241 309.3973\n\nFixed effects:  body_mass_g ~ species + sex + species:sex \n                            Value Std.Error  DF  t-value p-value\n(Intercept)              3368.836  36.21222 325 93.03036  0.0000\nspeciesChinstrap          158.370  64.24029 325  2.46528  0.0142\nspeciesGentoo            1310.906  54.42228 325 24.08767  0.0000\nsexmale                   674.658  51.21181 325 13.17387  0.0000\nspeciesChinstrap:sexmale -262.893  90.84950 325 -2.89372  0.0041\nspeciesGentoo:sexmale     130.437  76.43559 325  1.70650  0.0889\n Correlation: \n                         (Intr) spcsCh spcsGn sexmal spcsC:\nspeciesChinstrap         -0.564                            \nspeciesGentoo            -0.665  0.375                     \nsexmale                  -0.707  0.399  0.471              \nspeciesChinstrap:sexmale  0.399 -0.707 -0.265 -0.564       \nspeciesGentoo:sexmale     0.474 -0.267 -0.712 -0.670  0.378\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.67360404 -0.69157224  0.03564805  0.66744876  2.78292473 \n\nNumber of Observations: 333\nNumber of Groups: 3 \n\n\nFrom this table, we can determine the coefficients of the model such that:\nSpecies factor\n- \\(species_{Adelie}\\) = 0 (the baseline of the factor Habitat) - \\(Species_{Chinstrap}\\) = \\(158.370\\) - \\(Species_{Gentoo}\\) = \\(1310.906\\)\nSex factor\n- \\(Sex_{female}\\) = 0 (the baseline of the factor Habitat) - \\(Sex_{male}\\) = \\(674.658\\)\nInteraction\n- \\(Species_{Chinstrap}\\):\\(Sex_{male}\\) = \\(-262.893\\) - \\(Species_{Gentoo}\\):\\(Sex_{male}\\) = \\(130.437^{NS}\\)\nSo, the candidate model is: \\[  Species = 3369 + (Adelie = 0, Chinstrap = 158, Gentoo = 1311)  + (Female = 0,\\: Male = 675)\\] \\[       + (Adelie_{Male} = 0, \\:Chinstrap_{Male} = -263,\\: Gentoo_{Male} = 130^{NS}) \\]\nFor sake of simplicity, we can write the model depending on the sexe :\nThe model for the Female pinguin is : \\[ Sex_{Female} = 3369\\:  + (Adelie = 0,\\: Chinstrap = 158,\\: Gentoo = 1311)\\]\nThe model for the Male pinguin is : \\[Sex_{Male} = 4043\\: + (Adelie = 0,\\: Chinstrap = - 105,\\: Gentoo = 1441)\\]\nThus, sex, species, and the interaction of these two variables (except between Male and Gentoo) do have a significant impact on penguin body mass. For example, in Adelie penguins, the female will have a body mass of 3369g, whereas a male will have a body mass of 4043g and in Chinstrap penguins, the female will have a body mass of 3527g, whereas a male will have a body mass of 3938g.\nThese results are in line with the graph obtained in the “interactions between factors” section, which showed that males had a greater body mass than females. This also confirms that individuals of the Gentoo species have a greater body mass than individuals (males and females) of the other two species.\n\n\n\n4.4 Model validation\nTo validate the model, we need to :\n\nValidate the normality of the residuals\nHistogram and QQplot of the residuals\nValidate the homogeneity of the variances\nIn addition, check for the presence of observations which would have contributed too much to the model.\n\n\n4.4.1 Normality of the residuals\n\npar(mfrow=c(1,2))\n# Histogram\nhist(mod1$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality\")\n# Quantile-Quantile plot\nqqnorm(mod1$residuals,pch=16,col='blue',xlab='')\nqqline(mod1$residuals,col='red')\n\n\n\n\nWe can see that the histogram follows a normal distribution, and the quantile plot points follow the red line: the normality of the residuals is validated.\n\n\n4.4.2 Homogeneity of the variance\n\npar(mfrow=c(1,3))\n\n# residuals vs fitted\nplot(residuals(mod1)~fitted(mod1)\n      , col='blue'\n      , pch=16)\nabline(h = 0)\n\n# residuals against Species\nboxplot(residuals(mod1)~ df$species, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Species\",\n         main = \"\")\nabline(h = 0)\n\n# residuals against Sex\nboxplot(residuals(mod1)~ df$sex, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Sex\",\n         main = \"\")\nabline(h = 0)\n\n\n\n\nWe can see here that for each plot, the variance of the residuals is evenly distributed around the horizontal line. The homogeneity of the variance is validated.\n\n\n4.4.3 Look at influential observations\n\npar(mfrow = c(1, 1))\n\n\nCookD(mod1,newwd=TRUE)\n\nWe can see that individuals 314, 315 and 325 contribute slightly more to the model, but this is not an aberrant result."
  },
  {
    "objectID": "chapter_dependance.html",
    "href": "chapter_dependance.html",
    "title": "Chapter : statistical tests with dependent data",
    "section": "",
    "text": "This chapter is a simple example using R\nYou can import R package using the code\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "chapter_dependance.html#what-is-autocorrelation",
    "href": "chapter_dependance.html#what-is-autocorrelation",
    "title": "Chapter : statistical tests with dependent data",
    "section": "What is autocorrelation ?",
    "text": "What is autocorrelation ?\nStatistical analyses, including many regression methods, often assume that observations are independent of each other. So it is natural to make independence calculations between data X and Y. But there may still be a dependency intrinsic to each data item: this is known as autocorrelation. Autocorrelation indicates that the independence assumption is not satisfied, which can lead to biased parameter estimates, incorrect confidence intervals and a loss of statistical power. Autocorrelation in statistics is a measure of the linear dependence between the values of a single variable itself, at different times or locations. Autocorrelation is only detected on the variable being measured, i.e. the quantitative response variable. Checking for autocorrelation in a variable means analyzing the relationship between an observation and other previous observations in the series. This means analyzing the possible link between what is measured at time 1 and what is measured at time 2, to see if data 2 can be predicted with what we know about data 1, if they are close in space or time. Correlation between two observations is often expressed as a correlation coefficient, such as Pearson’s correlation coefficient. Values are correlated with an offset function in time or space, giving an autocorrelation coefficient which may be positive, indicating that the observations tend to be similar, or negative, indicating that the observations tend to be opposite to each other."
  },
  {
    "objectID": "chapter_dependance.html#what-types-of-autocorrelation",
    "href": "chapter_dependance.html#what-types-of-autocorrelation",
    "title": "Chapter : statistical tests with dependent data",
    "section": "What types of autocorrelation ?",
    "text": "What types of autocorrelation ?\nAutocorrelation is commonly used to analyze data where the order or location of observations has an important significance, such as time series (data taken at the same location at several points in time), or spatial data (data taken at a given point in time at several locations in the same geographical area, and therefore close in space).\n\nTime series\nIn time series, there may be seasonal autocorrelation, where values observed at the same time each year (seasons) are correlated. Temporal autocorrelation shows spatial similarities that depend on the scale used. Temporal autocorrelation in ecology is important for understanding the processes of reproduction, migration, population dispersal and species responses to seasonal environmental changes. Spatial dependencies can be linked to several factors: [Image]\n\nExample of temporal autocorrelation\nAn example of temporal autocorrelation in ecology concerns populations of animal or plant species, particularly when studying seasonal variations. Let’s imagine a study of annual fluctuations in the population of migratory birds in a given region. Temporal autocorrelation means that the number of birds observed at a given time of the year is correlated with the number of birds observed at the same time of the previous year. Suppose the data show positive temporal autocorrelation for a migratory bird species. This would mean that if, for example, in April of the current year, a large number of these birds are observed, it is highly likely that in April of the previous year, a large number of these same birds were also observed. This temporal dependence may be due to seasonal bird migrations, the availability of food resources or other cyclical environmental factors.\n\n\n\nSpatial dependence\nIn the case of spatial data, spatial autocorrelation refers to the correlation between observations in close spatial locations. This can be important for understanding the spatial distribution of populations, environmental phenomena and so on. Spatial autocorrelation shows temporal similarities that depend on the scale used. Spatial autocorrelation is important for understanding patterns of species distribution, population dispersal, biotic interactions and ecological processes at different spatial scales. - It can be due to various factors, such as : - Microclimate effects: (local characteristics of terrain, vegetation or topography can influence temperature on a small scale) - Dispersion phenomena: (the propagation of heat, wind, humidity or other environmental factors can cause spatial autocorrelation) - Biotic interactions: (interactions between plant, animal and micro-organism species in an ecosystem can also influence the spatial distribution of variables such as temperature).\n\nExample of spatial dependence\nAn example of spatial autocorrelation is the observation of temperature distribution in a large forest. Let’s imagine a large, dense forest with many weather stations measuring temperatures at different locations. Spatial autocorrelation would manifest itself in the fact that temperatures recorded at nearby locations in the forest are correlated, i.e. they tend to be similar. Suppose the data show positive spatial autocorrelation. This would mean that, if you have two weather stations located close to each other (say, a few meters apart), the temperatures measured at these two stations at any given time are highly positively correlated. In other words, when one of the stations records a high temperature, the other station will also tend to record a high temperature, and vice versa."
  },
  {
    "objectID": "chapter_dependance.html#why-is-this-important",
    "href": "chapter_dependance.html#why-is-this-important",
    "title": "Chapter : statistical tests with dependent data",
    "section": "Why is this important?",
    "text": "Why is this important?\n…"
  }
]