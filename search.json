[
  {
    "objectID": "r_chapter.linear_modelling.html",
    "href": "r_chapter.linear_modelling.html",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "lwskjglm This chapter is a simple example using R\nYou can import R package using the code\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nand then describe the purpose of your chapter as well as executing R command.\nFor example a basic summary of a dataset is given by\n\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n\nand produce a graph\n\ndf %&gt;% ggplot() +\n    aes(x=species, y = body_mass_g) +\n    geom_boxplot()  \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nA citation (bauer2023writing?)"
  },
  {
    "objectID": "chapter_dependance.html",
    "href": "chapter_dependance.html",
    "title": "Chapter : statistical tests with dependent data",
    "section": "",
    "text": "This chapter is a simple example using R\nYou can import R package using the code\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "chapter_dependance.html#what-is-autocorrelation",
    "href": "chapter_dependance.html#what-is-autocorrelation",
    "title": "Chapter : statistical tests with dependent data",
    "section": "What is autocorrelation ?",
    "text": "What is autocorrelation ?\nStatistical analyses, including many regression methods, often assume that observations are independent of each other. Most of the time, it seems natural to analyse two different datas, X and Y, assuming their independence. But there may still be a dependency intrinsic to each of them: this is known as autocorrelation. Autocorrelation indicates that the independence assumption is not satisfied, which can lead to biased parameter estimates, incorrect confidence intervals and a loss of statistical power. \nIn statistics, autocorrelation is a measure of the linear dependence between the values of one variable itself, at different times or locations. Checking for autocorrelation in a variable means analyzing the relationship between an observation and other previous observations in the series. In other words, this means analyzing the possible link between what is measured at time 1 and what is measured at time 2, to see if the data at time 2 can be predicted with what we know about the data at time 1, if they are close in space or time. \nCorrelation between two observations is often expressed as a correlation coefficient, such as Pearson’s correlation coefficient. Values are correlated with an offset function in time or space, giving an autocorrelation coefficient which may be: * positive: indicating that the observations tend to be similar,  * negative: indicating that the observations tend to be opposite to each other."
  },
  {
    "objectID": "chapter_dependance.html#what-types-of-autocorrelation",
    "href": "chapter_dependance.html#what-types-of-autocorrelation",
    "title": "Chapter : statistical tests with dependent data",
    "section": "What types of autocorrelation ?",
    "text": "What types of autocorrelation ?\nAutocorrelation is commonly used to analyze data for which the order or location of observations has an important significance, such as: * time series: data taken at the same location at several points in time,  * spatial data: data taken at a given point in time at several locations in the same geographical area, and therefore close in space.\n\nTime series\nA typical example of autocorrelation in a time series is a seasonal autocorrelation, where values are observed at a same time each year (seasons), and are therefore correlated with each other. \nTemporal autocorrelation shows spatial similarities that depend on the scale used.  \nTemporal autocorrelation in ecology is important for understanding the processes of reproduction, migration, population dispersal and species responses to seasonal environmental changes.\n\nExample of temporal autocorrelation\nAn example of temporal autocorrelation in ecology concerns populations of animal or plant species, particularly when studying seasonal variations. Let’s imagine a study of annual fluctuations in the population of migratory birds in a given region. Temporal autocorrelation means that the number of birds observed at a given time of the year is correlated with the number of birds observed at the same time of the previous year. \nSuppose the data show positive temporal autocorrelation for a migratory bird species. This would mean that if, for example, in April of the current year, a large number of these birds are observed, it is highly likely that in April of the previous year, a large number of these same birds were also observed. This temporal dependence may be due to seasonal bird migrations, the availability of food resources or other cyclical environmental factors.\n\n\n\nSpatial dependence\nSpatial autocorrelation refers to the correlation between observations in close spatial locations.  It shows temporal similarities that depend on the scale used. Spatial autocorrelation is important for understanding patterns of species distribution, population dispersal, biotic interactions and ecological processes at different spatial scales. \nSpatial autocorrelation can be due to various factors, such as :\n\nMicroclimate effects: local characteristics of terrain, vegetation or topography can influence temperature on a small scale,\nDispersion phenomena: the propagation of heat, wind, humidity or other environmental factors can cause spatial autocorrelation,\nBiotic interactions: interactions between plant, animal and micro-organism species in an ecosystem can also influence the spatial distribution of variables such as temperature.\n\n\nExample of spatial dependence\nAn example of spatial autocorrelation is the observation of temperature distribution in a large forest. Let’s imagine a large, dense forest with many weather stations measuring temperatures at different locations. Spatial autocorrelation would manifest itself in the fact that temperatures recorded at nearby locations in the forest are correlated, i.e. they tend to be similar. \nSuppose the data show positive spatial autocorrelation. This would mean that, if you have two weather stations located close to each other (say, a few meters apart), the temperatures measured at these two stations at any given time are highly positively correlated. In other words, when one of the stations records a high temperature, the other station will also tend to record a high temperature, and vice versa."
  },
  {
    "objectID": "chapter_dependance.html#why-is-this-important",
    "href": "chapter_dependance.html#why-is-this-important",
    "title": "Chapter : statistical tests with dependent data",
    "section": "Why is this important?",
    "text": "Why is this important?\nAutocorrelation is an essential concept in statistics, as it can have a significant impact on data analysis. In the presence of pseudoreplications, meaning that several different values are thought to be taken when in fact they are dependent on each other, it may appear that one variable has a significant effect on another, when in reality this effect is biased due to the temporal or spatial dependence of the data. It can therefore have a significant impact on statistical analysis, and often requires adjustments to ensure that results are reliable and unbiased. \nAutocorrelation checking is an essential process in statistics to guarantee the accuracy, reliability and validity of analyses. It enables appropriate statistical methods to be applied, biases to be corrected and informed modeling and forecasting decisions to be made. It is also essential for understanding patterns of variation in biological data."
  },
  {
    "objectID": "Mixed_models.html",
    "href": "Mixed_models.html",
    "title": "Mixed models in ecology",
    "section": "",
    "text": "library(nlme)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(predictmeans)\n\nLoading required package: glmmTMB\n\n\nWarning in checkMatrixPackageVersion(getOption(\"TMB.check.Matrix\", TRUE)): Package version inconsistency detected.\nTMB was built with Matrix version 1.6.4\nCurrent Matrix version is 1.6.1.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\n\nLoading required package: lmerTest\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(sp)"
  },
  {
    "objectID": "Mixed_models.html#introduction",
    "href": "Mixed_models.html#introduction",
    "title": "Mixed models in ecology",
    "section": "Introduction",
    "text": "Introduction\nGeneral Linear Models, such as linear regressions, ANOVA, and ANCOVA, are commonly employed to depict the relationships between a dependent variable, denoted as (Y), and one or more independent variables (\\(X_1, X_2, ..., X_n\\)).  These models are based on several assumptions, including homoscedasticity of the variance, non-collinearity of residuals, and normality of residuals. Generalized Linear Models (GLMs) can address homoscedasticity and normality assumptions by accommodating data from different distributions like Poisson, binomial, or Gamma distributions, which are often encountered in ecology.   However, it is crucial to validate the non-collinearity of residuals.\nIn biological and ecological experiments, the assumption of independence of measurements, necessary for non-collinearity of residuals, is frequently violated.  This is because measurements are often correlated within families, regions, repeated on the same individuals, or across time and sites. In such cases, it becomes necessary to employ mixed models. These models, extensions of both general and generalized linear models, consider the correlation of measurements by introducing individuals, regions, families, or other factors as random effects in the models. This incorporation allows for a more accurate representation of the complex dependencies present in the data.\nWhat is a random effect, and how do I determine if my effect is random or fixed ?\nTo clarify the distinction between fixed and random effects, let’s examine two examples:\n\nExample 1: Comparing individual cars\nAbdel, Antonio, Odeline, and Aela want to compare the oil consumption of their individual cars. They conduct a test by measuring oil consumption during a 30-kilometer drive, repeated five times in a day, with consistent traffic conditions and driving patterns.  The data set consists of one factor with four levels (representing the four cars) and five replicates each. Performing a one-way ANOVA allows them to determine which car is the most economical. In this scenario, the factor “car” is fixed, and the analysis provides conclusions specific to the four studied cars.\nExample 2: Assessing homogeneity within a car model\nA car constructor aims to evaluate the homogeneity of oil consumption within a car model, treating the model as a population of cars with expected heterogeneity in gas consumption.  Similar to Example 1, they measure oil consumption by driving each car 30 kilometers, five times in a day, resulting in a data set with one factor and four levels, each with five replicates. Unlike the first example, the cars in this case were sampled from a larger population, and the objective is to draw conclusions about the entire population, not just the sampled cars. Here, a mixed model with the factor ‘car’ as a random factor should be used.\n\nIn summary, a factor is designated as fixed when the experimenter intentionally chooses a limited number of levels for investigation, aiming to assess the impact of each level on the response variable. A factor is considered random when the selected levels represent only a sample from all possible levels. In this case, the objective is to understand the variability in the response variable attributed to this factor.\nFor example, let’s consider a researcher investigating the influence of the number of training sessions per week on the concentration of red blood cells in recreational athletes. The researcher collects data from 50 athletes in a local club who train between 1 and 5 times a week. Initially planning a simple ANOVA with the number of training sessions as the main factor, the researcher discovers that most athletes in the data set belong to only 10 families, leading to non-independent measurements. To address this issue, the researcher opts for a mixed model, treating the number of training sessions as a fixed factor and the family as a random factor. This approach allows the exploration of variability between families without the intention of directly comparing them.\nNow that we have a general understanding of what mixed models are, we can delve into the mathematical formalism of these models. In this chapter, you will discover how matrices can be employed to create mixed models, explore the various dependency structures that exist, and ultimately, find an implementation of mixed models in R."
  },
  {
    "objectID": "Mixed_models.html#formalization-of-the-linear-mixed-model",
    "href": "Mixed_models.html#formalization-of-the-linear-mixed-model",
    "title": "Mixed models in ecology",
    "section": "1. Formalization of the linear mixed model",
    "text": "1. Formalization of the linear mixed model\nThe linear mixed model can be formulated as follows:\n\\[\nY_{i} = \\beta X_{i} + \\gamma_i Z_{i} + \\varepsilon_{i}\n\\]\nwhere:\n\n\\(Y_i = n_i \\times 1\\) measurements for subject \\(i\\), where \\(n\\) is the number of observations\n\\(X_i = n_i \\times p\\) matrix of vectors for fixed effects, where \\(p\\) is the number of fixed parameters\n\\(\\beta_i= p \\times 1\\) parameters for fixed effects\n\\(Z_i = n_i \\times p\\) matrix of vectors for random effects\n\\(\\gamma_i = r \\times 1\\) parameters for random effects, where \\(p\\) is the number of random parameters\n\\(\\varepsilon_i = n_i \\times 1\\) residuals for individual \\(i\\)\n\n(Giorgi 2020)\nA mixed effects model incorporates random effects (\\(\\gamma_i\\)), or a combination of both random and fixed effects (\\(\\beta\\)), whereas a standard linear model includes only fixed effects.\nWhen it is clear that the researcher intends to compare particular, predefined levels of a treatment, those levels are considered fixed effects. Conversely, when the levels of the treatment are drawn from a larger population of possible levels, the treatment is treated as a random effect.\nIn addition, random effects are included in a model when there is a correlation or dependence among the observations that cannot be ignored.\nRANDOM VARIABLE = “something that could not be known before sampling/measurement/observation”.\nIn matrix form, the mixed model is written as:\n\\[\nY \\sim \\mathcal{N{n}}(X\\theta, \\Sigma)\n\\] where:\n\\(Y\\) is the response vector of the observations, \\(X\\theta\\) is the expectation of the response vector \\(Y\\) and \\(\\Sigma\\) is the variance matrix.\nWe can note that if the response vector \\(Y\\) is of dimension \\(n\\), the matrix \\(\\Sigma\\) is of dimensions \\(n \\times n\\). Since \\(\\Sigma\\) is symmetric, it comprises \\(n(n + 1)/2\\) parameters. This is because, in a symmetric matrix, the elements above (or below) the main diagonal are the same as those below (or above), reducing the total number of parameters needed to describe the matrix.\nHowever, the limitation of the available data prevents considering models where all these \\(n(n + 1)/2\\) parameters are free. This restriction arises from the need to have a significant amount of data to reliably estimate each parameter, which quickly becomes unrealistic with a limited dataset.\nTo address this issue, the linear mixed-effects model proposes an approach where a structure is imposed on the variance matrix \\(\\Sigma\\). This structure, governed by a limited number of parameters called “variance parameters,” denoted \\(\\psi\\), reduces the number of parameters needed to describe the covariance matrix. Consequently, the model can be realistically adapted even with a limited amount of data, while accounting for the correlation between observations within the framework of linear mixed-effects models. The model parameters include \\(\\theta\\) for the expectation and \\(\\psi\\) for the variance."
  },
  {
    "objectID": "Mixed_models.html#matrix-computation-in-mixed-models",
    "href": "Mixed_models.html#matrix-computation-in-mixed-models",
    "title": "Mixed models in ecology",
    "section": "2. Matrix computation in mixed models",
    "text": "2. Matrix computation in mixed models\nIt is possible to encounter (mixed) linear models written under their matrix form, for their concision. It is therefore natural to present this form in the context of mixed models.\nAs a reminder, a linear model, like linear regression, with \\(p\\) explanatory variables can be written \\[y_i = \\beta_0 + \\beta_1x_i^{(1)} + \\ldots + \\beta_px_i^{(p)} + \\varepsilon_i\\], where \\(y_i\\) represent an observation of the response variable \\(Y\\) for the individual \\(i\\), \\(\\beta_0\\) the intercept, \\(\\beta_1,...,\\beta_p\\) the coefficients associated to each explanatory variable \\(X_1,...,X_p\\), \\(x_i^{(1)},...,x_i^{(p)}\\) the \\(p\\) observations (for the \\(p\\) explanatory variables) for the individual \\(i\\), and \\(e_i\\) an error term associated to the individual \\(i\\).  We can see \\(e_i\\) as a realization of a random variable \\(E_i\\) distributed according to a normal law \\(\\mathcal{N}(0,\\sigma^2)\\). Noting \\[y=\\begin{pmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\\\\\n\\end{pmatrix}\\], \\[X=\\begin{pmatrix}\n1&x_1^{(1)} & \\ldots & x_1^{(p)}\\\\\n\\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & x_n^{(1)} & \\ldots & x_n^{(p)}\n\\end{pmatrix}\\], \\[\\theta=\\begin{pmatrix}\n\\beta_0\\\\\n\\vdots\\\\\n\\beta_p\\\\\n\\end{pmatrix}\\] and \\[e=\\begin{pmatrix}\n\\varepsilon_1\\\\\n\\vdots\\\\\n\\varepsilon_n\\\\\n\\end{pmatrix}\\], we can rewrite the previous model under the form \\[y=X\\theta+e\\]Here, \\(e\\) is a vector of \\(n\\) independent realizations or a random variable \\(E_i\\) following a normal distribution \\(\\mathcal{N}(0,\\sigma^2)\\).  Hence, \\(e\\) is a realization of a random variable \\(E\\) following the distribution \\(\\mathcal{N}_n(0,\\sigma^2I_n)\\) (\\(e_i\\) is an observation of the random variable \\(E_i\\) distributed according to a normal law \\(\\mathcal{N}(0,\\sigma^2)\\)).  Similarly, \\(y\\) is an observation of \\(Y=X\\theta+E\\) where \\(Y\\sim\\mathcal{N}_n(X\\theta,\\sigma^2I_n)\\) (\\(y_i\\) is an observation of \\(Y_i\\) distributed according to a normal law \\(\\mathcal{N}((X\\theta)_i,\\sigma^2)\\)). Hence, by introducing \\(Y\\) and \\(E\\), the previous model can be written \\(Y=X\\theta+E\\) where \\(\\mathrm{E}\\stackrel{iid}\\sim\\mathcal{N}_n(0,\\sigma^2I_n)\\).\nBy definition, the mean response is equal to \\(X\\theta\\), more or less an error term equals to 0 in average but that varies of \\(\\sigma^2I_n\\). Thus, we can write \\[Y\\sim\\mathcal{N}_n(X\\theta,\\sigma^2I_n)\\]We note that when writing \\(\\mathrm{E}\\stackrel{iid}\\sim\\mathcal{N}_n(0,\\sigma^2I_n)\\), each error has got the same variance (\\(\\sigma^2\\)) because every samples are independent. We will see subsequently that if this independence condition is not respected, all the errors do not have the same variance, we speak of a dependence structure. The dependence between the measurements determines the dependence structure (measurements repeated over time, individuals grouped by common ancestry, etc.). We will see here that in the context of mixed models, the previous equation is written \\[Y\\sim\\mathcal{N}_n(X\\theta,\\sum)\\]. When writing this, we note that the average response does not change. Generalization concerns errors. Now, let’s study study the components of variance \\(\\sum\\) by following an example. We decide to study the heritability of a trait (height for example). We want to see if individuals from one ascendant are more similar than those from another ascendant. We have \\(m\\) ascendants, numbered \\(i=1,...,m\\), from a larger population, each having \\(n\\) descendants numbered \\(j=1,...,n\\). We pose \\(Y_{ij}=\\) the trait value for the j-ème descendant of the i-ème ascendant. Individuals with the same ascendant are therefore grouped by their belonging to the same ascendant. The fact of sampling ancestors from a larger population (random effect) introduces a correlation between the traits measured on the descendants of the same ancestor. This correlation is uniform between individuals. The descendants between the groups (for different ancestors) are independent. Thus, we write \\[\n\\mathbb{Cov}(Y_{ij},Y_{i'j'}) = \\left\\{\n    \\begin{array}{ll}\n        \\gamma^2 & \\mbox{si } i=i'\\\\\n        0 & \\mbox{sinon.}\n    \\end{array}\n\\right.\n\\] This model therefore includes a variance associated with the ascendant effect \\(\\gamma^2\\) and a residual variance \\(\\sigma^2\\). We precise that \\(\\gamma^2\\) represent the variability between the ascendants. Starting from the linear model \\(Y=X\\theta+E\\) where \\(E\\) is still \\(\\sigma^2I_n\\), we just have to take into account this variance associated with the ascendant effect. To do this, we introduce the matrix \\(Z\\), of dimension \\(n\\times m\\), where \\[\nZ_{a,i} = \\left\\{\n    \\begin{array}{ll}\n        1 & \\mbox{l'individu } a=(i,j) \\mbox{ est le descendant de l'ascendant }i\\\\\n        0 & \\mbox{sinon.}\n    \\end{array}\n\\right.\n\\] and the vector \\(U\\) of dimension \\(m\\) including the random effects \\(\\gamma^2\\). Hence, the previous model can be rewritten \\(Y=X\\theta+ZU+E\\). We remind that \\(U\\) and \\(E\\) are independents, gaussiens centered, and that \\(\\mathbb{Var}(U)=\\gamma^2I_m\\). Consequently, \\(\\mathbb{E}(Y)=X\\theta\\) and \\(\\sum = \\mathbb{V}(Y)=Z\\mathbb{V}(U)Z'+\\mathbb{V}(E) = \\gamma^2ZZ'+\\sigma^2(I_n)\\). We get back to the matrix form \\(Y\\sim\\mathcal{N}(X\\theta,\\sum)\\) with \\[\\sum=\\begin{pmatrix}\nR&0 & \\ldots & 0\\\\\n0 & \\ddots & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & 0 \\\\\n0 & \\ldots & 0 & R\n\\end{pmatrix}\\], where \\[R=\\begin{pmatrix}\n\\sigma^2+\\gamma^2&\\gamma^2 & \\ldots & \\gamma^2\\\\\n\\gamma^2 & \\ddots & \\ddots & \\gamma^2 \\\\\n\\vdots & \\ddots & \\ddots & \\gamma^2 \\\\\n\\gamma^2 & \\ldots & \\gamma^2 & \\sigma^2+\\gamma^2\n\\end{pmatrix}\\]\\(R\\) corresponds to the representation of the variances due to the random effect and residuals. Note that the diagonal blocks are of the same dimensions if the number of descendants is identical for each ascendant."
  },
  {
    "objectID": "Mixed_models.html#dependency-structures",
    "href": "Mixed_models.html#dependency-structures",
    "title": "Mixed models in ecology",
    "section": "3. Dependency structures",
    "text": "3. Dependency structures\nThis part of the chapter is inspired by (L. Bel 2016)’s book. \n\n3.1 Case of repeated measurements\nWe want to evaluate the effect of different diet on weight gain in rats. Several animals (\\(j\\)) follow each diet (\\(i\\)), and they kept the same diet across all the experiment. Each week animal weight (\\(Y_{ij}\\)) is measured, during T weeks. In this case, measures are repeated across time, such measurements are called longitudinal data.  To analyse this data, the temporal dependency must be taken into account, for this, the following model can be used: \nModel The model proposed here takes into account the kinetic aspect of the experiment and predicts that the dependence between two measurements depends on the time interval between them.    We assume that the weights are Gaussian with the following expected values :\n\\[ E(Y_{ijt}) = µ + α_i + γ_t + (αγ)_{it}.\\]\nDependency structure We assume that all measurements have the same variance\n\\[ V(Y_{ijt} =  σ^2) \\]\nand the covariance between them is\n\\[Cov(Y_{ijt}, Y_{i'j't'}) = \\left\\{\n  \\begin{array}{ll}\n  σ^2ρ^{|t-t'|} \\ \\ si \\ \\ (i, j) = (i', j') \\\\\n     0 \\ sinon \\\n   \\end{array}\n  \\right.\\]\nThis structure assumes that measurements made on different animals are independent. It is also assumed that |ρ| &lt; 1, which implies that the longer the time interval, the less correlated the tests on the same animal. This form of covariance corresponds to an autoregressive process of order 1, generally denoted AR(1). This model has two variance parameters: the temporal correlation ρ and the variance of each observation \\(σ^2\\).\n\\[\n  ψ = \\left( {\\begin{array}{cc}\n    ρ \\\\\n    σ^2 \\\\\n  \\end{array} } \\right)\n\\]\nBecause of the independence between the measurements obtained on different animals, the variance matrix Σ also has the same diagonal block shape, but the block (R) differs.\n\\[  \nR = \\left( {\\begin{array}{cc}\n     σ^2 & σ^2ρ & σ^2ρ^2 & ... & σ^2ρ^{T-1}\\\\\n     σ^2ρ & ... & ... & ... & ...\\\\\n     σ^2ρ^2 & ... & σ^2 & ... & σ^2ρ^2\\\\\n     ... & ... & ... & ... & σ^2ρ\\\\\n    σ^2ρ^{T-1} & ... & σ^2ρ^2 & σ^2ρ & σ^2\\\\\n  \\end{array} } \\right)\n\\]\nWe are going to use the BodyWeight dataset from the nlme package. In this dataset, weight is measured on 16 rats every 7 days during 64 days (which gives 11 measurements for each rats). Three diets are tested with 88 rats following diet 1 and 44 following diet 2 and 3.\nThe research question is: Is weight gain different depending on the diet? The model will be a mixed model with diet and time as fixed factor and individuals as random factor: weight ~ Diet * Time | Rat.\n\nhead(BodyWeight, 10)\n\nGrouped Data: weight ~ Time | Rat\n   weight Time Rat Diet\n1     240    1   1    1\n2     250    8   1    1\n3     255   15   1    1\n4     260   22   1    1\n5     262   29   1    1\n6     258   36   1    1\n7     266   43   1    1\n8     266   44   1    1\n9     265   50   1    1\n10    272   57   1    1\n\n\n\ntime_model = gls(weight ~ Diet * Time, data = BodyWeight, correlation = corAR1(form = ~ 1 | Rat))\nsummary(time_model)\n\nGeneralized least squares fit by REML\n  Model: weight ~ Diet * Time \n  Data: BodyWeight \n       AIC      BIC    logLik\n  1152.248 1177.334 -568.1239\n\nCorrelation Structure: AR(1)\n Formula: ~1 | Rat \n Parameter estimate(s):\n      Phi \n0.9895746 \n\nCoefficients:\n                Value Std.Error   t-value p-value\n(Intercept) 250.11069 13.327526 18.766475  0.0000\nDiet2       203.46561 23.083952  8.814158  0.0000\nDiet3       260.91151 23.083952 11.302723  0.0000\nTime          0.37351  0.090964  4.106160  0.0001\nDiet2:Time    0.62384  0.157554  3.959506  0.0001\nDiet3:Time    0.18780  0.157554  1.191998  0.2349\n\n Correlation: \n           (Intr) Diet2  Diet3  Time   Dt2:Tm\nDiet2      -0.577                            \nDiet3      -0.577  0.333                     \nTime       -0.222  0.128  0.128              \nDiet2:Time  0.128 -0.222 -0.074 -0.577       \nDiet3:Time  0.128 -0.074 -0.222 -0.577  0.333\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.44446363 -0.63038331  0.02804647  0.25238087  2.93319234 \n\nResidual standard error: 37.70413 \nDegrees of freedom: 176 total; 170 residual\n\n\nIn this example, the time correlation ρ has a value of 0.989. The sequence of weight values was highly correlated. The model allowed this temporal dependency to be taken into account. \n\n\n3.2 Case of spatial autocorrelation\nDependency structure We want to take into account the dependency due to the possible spatial proximity between the sites at which the measurements were taken.\nTo do this, \\(d(i, i')\\) is the distance separating sites \\(i\\) and \\(i'\\), and the following equation is used\n\\[Cov(Y_i, Y_i{'}) = e^{−δ.d(i,i')}\\]\nAs in the case of repeated measurements, there is no simple way of writing this in terms of random effects. Moreover, since all the measurements are dependent, the matrix Σ is no longer diagonal per block and is written as :\n\\[\n  Σ = \\left( {\\begin{array}{cc}\n     σ^2 + γ^2 & e^{−δ.d(i,i')}& ...& e^{−δ.d(i,i')}\\\\\n     e^{−δ.d(i,i')}& σ^2 + γ^2 & e^{−δ.d(i,i')} & \\vdots\\\\\n     \\vdots & e^{−δ.d(i,i')}  & σ^2 + γ^2 & e^{−δ.d(i,i')}\\\\\ne^{−δ.d(i,i')}& \\ldots & e^{−δ.d(i,i')} & σ^2 + γ^2\\\\\n  \\end{array} } \\right)\n\\]\n\ndata.spatialCor.glsExp &lt;- gls(y ~ x, data = data.spatialCor,\n    correlation = corExp(form = ~LAT + LONG, nugget = TRUE),\n    method = \"REML\")\n\n\nsummary(data.spatialCor.glsExp)\n\nGeneralized least squares fit by REML\n  Model: y ~ x \n  Data: data.spatialCor \n       AIC      BIC    logLik\n  974.3235 987.2484 -482.1618\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~LAT + LONG \n Parameter estimate(s):\n    range    nugget \n1.6956723 0.1280655 \n\nCoefficients:\n               Value Std.Error  t-value p-value\n(Intercept) 65.90018 21.824752 3.019516  0.0032\nx            0.94572  0.286245 3.303886  0.0013\n\n Correlation: \n  (Intr)\nx -0.418\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.6019483 -0.3507695  0.1608776  0.6451751  2.1331505 \n\nResidual standard error: 47.68716 \nDegrees of freedom: 100 total; 98 residual\n\n\nIn spatially correlated data, three coefficients are needed to describe the correlation structure. The variance increases with increasing distance up to a point the sill. The span of distances over which points are correlated is called the range. While we might expect the value of variance at a distance of zero to be zero, in reality we rarely have sampling units that approach such a small distance from one another.The value of variance when distance is equal to zero is the nugget.\nHere, in our example, the value of the sill, the range and the nugget are respectively 47.68, 1.69 and 0.12."
  },
  {
    "objectID": "Mixed_models.html#application",
    "href": "Mixed_models.html#application",
    "title": "Mixed models in ecology",
    "section": "4. Application",
    "text": "4. Application\nFor this example of a mixed model application, we will use a general linear mixed model. This is a special case of a general linear model, in which the response is quantitative and the predictor variables are both quantitative and qualitative, and the model includes random factors to take account of data dependency. Mixed models must respect the normality of residuals and the homogeneity of variances. (The structure and lines of code are inspired by (Outreman 2023)’s courses on linear mixed model.)\n\n4.1 Dataset presentation and objectives of the analysis\nFor this worked exercise, we will use data from a study performed on penguins. The aim of this study is to test whether species, sex and island influence the body mass of penguins. In the experimental design, the data are collected from one year to the next (from 2007 to 2009), which suggests that the penguin body mass data are dependent on each other from one year to the next. This dependency will be included in the model. The data contains:\n- species: three species of penguins (Chinstrap, Adelie, or Gentoo), categorical variable\n- island: island name (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica), categorical variable\n- sex: penguin sex (female, or male), categorical variable\n- year: years of data collection (2007, 2008, or 2009), continuous variable\n- body_mass_g: body mass of the penguins (in grams), continuous variable\n\nThe response variable is the ‘body_mass_g’, while ‘species’ and ‘island’ and ‘sex’ are assumed predictors. To include data dependency, ‘year’ will be included as random factor in model. The underlying question for this research is: do the species, island and sex drive the body mass of penguins?\nData import :\n\n# Data import\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n# Make sure that our variables 'species', 'island' and 'sex' are all factors in the choice.\ndf$species=as.factor(df$species)\ndf$island=as.factor(df$island)\ndf$sex=as.factor(df$sex)\n\n# Check for missing values\ncolSums(is.na(df))\n\n            rowid           species            island    bill_length_mm \n                0                 0                 0                 2 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                2                 2                 2                11 \n             year \n                0 \n\n\nWe can see that there are some missing values, including 2 for the response variable \\(Y\\) ‘body_mass_g’ and 11 for the explanatory variable \\(X\\) ‘sex’. We’re going to delete the rows with the missing values.\n\n# Rows with missing values are marked.\nwhich(is.na(df$body_mass_g), arr.ind=TRUE)\n\n[1]   4 272\n\nwhich(is.na(df$sex), arr.ind=TRUE)\n\n [1]   4   9  10  11  12  48 179 219 257 269 272\n\n# We delete the rows 4, 9, 10, 11, 12, 48, 179, 219, 257, 269, and 272.\ndf=df[-c(4,9,10,11,12,48,179,219,257,269,272), ]\n\n# Check for missing values\ncolSums(is.na(df))\n\n            rowid           species            island    bill_length_mm \n                0                 0                 0                 0 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                0                 0                 0                 0 \n             year \n                0 \n\n# Ok\n\n\n\n4.2 Data exploration\nBefore any statistical analysis, it is ESSENTIAL to explore the data in order to avoid any errors. Here is the list of explorations to be carried out before modelling:\n\nCheck for outliers in \\(Y\\) and the distribution of \\(Y\\) values.\nIf \\(X\\) is an independent quantitative variable, check for the presence of outliers in X and the distribution of the values of X. 2b. 2b. If \\(X\\) is a qualitative independent variable, analyse the number of levels and the number of individuals per level.\nAnalyse the potential relationships between \\(Y\\) and the \\(X_{s}\\).\nCheck for the presence of interactions between \\(X_{s}\\).\nCheck for collinearity between \\(X_{s}\\).\n\n\n4.2.1 Outliers and distribution of \\(Y\\)\n\npar(mfrow=c(2,2))\n# Boxplot\nboxplot(df$body_mass_g,col='blue',ylab='Masse corporel')\n# Cleveland plot\ndotchart(df$body_mass_g,pch=16,col='blue',xlab='Masse corporel')\n# Histogram\nhist(df$body_mass_g,col='blue',xlab=\"Masse corporel\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(df$body_mass_g,pch=16,col='blue',xlab='')\nqqline(df$body_mass_g,col='red')\n\n\n\n\nHere, the Boxplot and Cleveland Plot show no individuals with outliers. The Cleveland Plot shows us that there appears to be a group of individuals with a body mass between 5000 and 6000g, while the rest is between 3000 and 4000g. The Histogram and the QQ Plot show that \\(Y\\) hardly follows a Normal distribution… This is not very important, as the validity of the model is based, among other things, on the normality of the residuals, which we will demonstrate later.\n\n\n4.2.2 Outliers in \\(Xs\\)\n\nFor \\(Xs\\) which are quantitative: check for outliers and distribution\n\nNo quantitative predictor here.\n\nFor categorical \\(Xs\\): number of levels and number of individuals per level.\n\n\n# Factor Species\nsummary(df$species)\n\n   Adelie Chinstrap    Gentoo \n      146        68       119 \n\n# Factor Island\nsummary(df$island)\n\n   Biscoe     Dream Torgersen \n      163       123        47 \n\n# Factor Sex\nsummary(df$sex)\n\nfemale   male \n   165    168 \n\n\nThe ‘species’ variable has 3 levels: Adelie, Chinstrap and Gentoo. The number of individuals between the 3 levels is not balanced, with fewer individuals for the Chinstrap species.\nThe ‘island’ variable has 3 levels: Biscoe, Dream and Torgersen. The number of individuals between the 3 levels is not balanced, with fewer individuals for the Torgersen island.\nThe ‘sex’ variable has 2 levels: female and male. The number of individuals per level is close to equilibrium.\n\n\n4.2.3 Analysis of potential relationships Y vs Xs\nWe can graphically analyse the possible relationships between Y and X. Please note that this graphical analysis of the relationships between Y and X in no way predicts the importance of the relationship. Statistical modelling is the only way to identify relationships.\n\npar(mfrow=c(1,1))\n# Species\nplot(df$body_mass_g~df$species,pch=16,col='blue',xlab='Espèces',ylab='Masse corporel en g')\n\n\n\n# Islands\nplot(df$body_mass_g~df$island,pch=16,col='blue',xlab='Îles',ylab='Masse corporel en g')\n\n\n\n# Sex\nplot(df$body_mass_g~df$sex,pch=16,col='blue',xlab='Sexe',ylab='Masse corporel en g')\n\n\n\n\nIn terms of species, we can see that Gentoo has a higher body mass (between 5000 and 6000g) than the other two species (between 3000 and 4000g). About the islands, we can see that the individuals present on Biscoe have a higher body mass (between 5000 and 6000g) than the individuals present on the other two islands (between 3000 and 4000g). Finally, in terms of sex, males appear to have a slightly higher body mass than females.\n\n\n4.2.4 Analysis of possible interactions between the three independent variables\nHere, we will consider the interaction between the three factors studied. To estimate the presence of interactive effects, we develop a graphical approach. Remember that the interaction between factors can only be tested if the factors are crossed (i.e. all the levels of one treatment are represented in all the levels of the other treatment and vice versa = a factorial design). This point must be tested first.\n\n# The experimental design means that the factors are cross-tabulated (all the levels of each variable are represented in all the levels of the other variables). \n\n# Interaction table\ntable(df$species,df$sex,df$island)\n\n, ,  = Biscoe\n\n           \n            female male\n  Adelie        22   22\n  Chinstrap      0    0\n  Gentoo        58   61\n\n, ,  = Dream\n\n           \n            female male\n  Adelie        27   28\n  Chinstrap     34   34\n  Gentoo         0    0\n\n, ,  = Torgersen\n\n           \n            female male\n  Adelie        24   23\n  Chinstrap      0    0\n  Gentoo         0    0\n\n# Interaction table without island\ntable(df$species,df$sex)\n\n           \n            female male\n  Adelie        73   73\n  Chinstrap     34   34\n  Gentoo        58   61\n\n# Interaction Species:Sex\nboxplot(df$body_mass_g~df$species*df$sex, varwidth = TRUE, xlab = \"Species.Sex\", ylab = \"Body mass\", col='blue2', main = \"\",cex.axis=0.7)\n\n\n\n\nIn the interaction table, we can see that in every island, not all the species are represented. This can be complicated to analyse. We’ll see in the next section if we remove the “Island” variable. We can see that without the ‘island’ variable, there is the same number of female and male in each species, except for the Gentoo species, with 58 female and 61 male.\nFor the interaction graph (still without the ‘island’ variable), we can see that males of all 3 species appear to have a greater body mass than females. We can also see that males and females of the Gentoo species have a higher body mass than individuals of the other species.\n\n\n4.2.5 Check collinearity between X\nColinearity refers to the situation in which two or more predictors of collinearity are closely related to each other.The presence of collinearity can pose problems in the context of regression, as it can be difficult to separate the individual effects of collinear variables on the response.\nHere, we will test for collinearity between our 3 predictor variables:\n\n# ploting Species by Island\nggplot(df, aes(x=species, y=island)) +\n  geom_point() +\n  theme_bw() -&gt; p1\n\n# ploting Species by Sex\nggplot(df, aes(x=species, y=sex)) +\n  geom_point() +\n  theme_bw() -&gt; p2\n\n# ploting Island by Sex\nggplot(df, aes(x=island, y=sex)) +\n  geom_point() +\n  theme_bw() -&gt; p3\n\n# Ploting side-by-side\nmarrangeGrob(list(p1,p2,p3), nrow=1, ncol=3, top=NULL)\n\n\n\n\nIn our example, we can see that for the interaction between Species and Sex, there are two sex modalities per species, and for the interaction between Island and Sex, there are two sex modalities per island. However, for the interaction between Species and islands based on, not all the islands contain all the species, as we saw in the previous section! We cannot therefore test the influence of islands and species on the basis of this result. We therefore decided to remove the Island variable from our analysis. We will test the influence of species and sex on the body mass of penguins, always with years as a random effect.\n\n\n\n4.3 Statistical analysis\n\n4.3.1 Model construction\nFor statistical modelling, we first analyse the full model (model containing all the independent variables to be tested).\nTo obtain the candidate model (a model containing only the significant terms) from the full model, we will use the BACKWARD SELECTION METHOD, i.e. model selection based on the significance of the terms. In this approach, we start by creating the full model with all the variables of interest, then drop the least significant variable as long as it is not significant. We continue by successively fitting reduced models and applying the same rule until all the remaining variables are significant. The deletion of non-significant terms must follow the following two steps: - First, insignificant interactions are successively removed. - Secondly, the non-significant main effects are successively removed. A main effect is only removed if it is insignificant AND if it is not contained in a significant interaction.\nIn this example, we consider a measure of dependence at year level (e.g. a mass measurement made in 2009 depends on the measurement made in 2008, which in turn depends on the measurement made in 2007). The presence of the random effect of the year will be integrated not with the lm function, but lme (from the nlme package).\n\n# Full model\nmod1=lme(body_mass_g~species\n              + sex\n              + species:sex\n              ,random=~1|year\n              ,data=df)\n\nWe can see from the anova output of our full model that each interaction and each variable is significant (&lt;0.05). The full model is therefore the candidate model.\n\n\n4.3.2 Model’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\nLinear mixed-effects model fit by REML\n  Data: df \n       AIC      BIC    logLik\n  4718.236 4748.556 -2351.118\n\nRandom effects:\n Formula: ~1 | year\n        (Intercept) Residual\nStdDev:   0.0140241 309.3973\n\nFixed effects:  body_mass_g ~ species + sex + species:sex \n                            Value Std.Error  DF  t-value p-value\n(Intercept)              3368.836  36.21222 325 93.03036  0.0000\nspeciesChinstrap          158.370  64.24029 325  2.46528  0.0142\nspeciesGentoo            1310.906  54.42228 325 24.08767  0.0000\nsexmale                   674.658  51.21181 325 13.17387  0.0000\nspeciesChinstrap:sexmale -262.893  90.84950 325 -2.89372  0.0041\nspeciesGentoo:sexmale     130.437  76.43559 325  1.70650  0.0889\n Correlation: \n                         (Intr) spcsCh spcsGn sexmal spcsC:\nspeciesChinstrap         -0.564                            \nspeciesGentoo            -0.665  0.375                     \nsexmale                  -0.707  0.399  0.471              \nspeciesChinstrap:sexmale  0.399 -0.707 -0.265 -0.564       \nspeciesGentoo:sexmale     0.474 -0.267 -0.712 -0.670  0.378\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.67360404 -0.69157224  0.03564805  0.66744876  2.78292473 \n\nNumber of Observations: 333\nNumber of Groups: 3 \n\n\nFrom this table, we can determine the coefficients of the model such that:\nSpecies factor\n- \\(species_{Adelie}\\) = 0 (the baseline of the factor Habitat) - \\(Species_{Chinstrap}\\) = \\(158.370\\) - \\(Species_{Gentoo}\\) = \\(1310.906\\)\nSex factor\n- \\(Sex_{female}\\) = 0 (the baseline of the factor Habitat) - \\(Sex_{male}\\) = \\(674.658\\)\nInteraction\n- \\(Species_{Chinstrap}\\):\\(Sex_{male}\\) = \\(-262.893\\) - \\(Species_{Gentoo}\\):\\(Sex_{male}\\) = \\(130.437^{NS}\\)\nSo, the candidate model is: \\[  Species = 3369 + (Adelie = 0, Chinstrap = 158, Gentoo = 1311)  + (Female = 0,\\: Male = 675)\\] \\[       + (Adelie_{Male} = 0, \\:Chinstrap_{Male} = -263,\\: Gentoo_{Male} = 130^{NS}) \\]\nFor sake of simplicity, we can write the model depending on the sexe :\nThe model for the Female pinguin is : \\[ Sex_{Female} = 3369\\:  + (Adelie = 0,\\: Chinstrap = 158,\\: Gentoo = 1311)\\]\nThe model for the Male pinguin is : \\[Sex_{Male} = 4043\\: + (Adelie = 0,\\: Chinstrap = - 105,\\: Gentoo = 1441)\\]\nThus, sex, species, and the interaction of these two variables (except between Male and Gentoo) do have a significant impact on penguin body mass. For example, in Adelie penguins, the female will have a body mass of 3369g, whereas a male will have a body mass of 4043g and in Chinstrap penguins, the female will have a body mass of 3527g, whereas a male will have a body mass of 3938g.\nThese results are in line with the graph obtained in the “interactions between factors” section, which showed that males had a greater body mass than females. This also confirms that individuals of the Gentoo species have a greater body mass than individuals (males and females) of the other two species.\n\n\n\n4.4 Model validation\nTo validate the model, we need to :\n\nValidate the normality of the residuals\nHistogram and QQplot of the residuals\nValidate the homogeneity of the variances\nIn addition, check for the presence of observations which would have contributed too much to the model.\n\n\n4.4.1 Normality of the residuals\n\npar(mfrow=c(1,2))\n# Histogram\nhist(mod1$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality\")\n# Quantile-Quantile plot\nqqnorm(mod1$residuals,pch=16,col='blue',xlab='')\nqqline(mod1$residuals,col='red')\n\n\n\n\nWe can see that the histogram follows a normal distribution, and the quantile plot points follow the red line: the normality of the residuals is validated.\n\n\n4.4.2 Homogeneity of the variance\n\npar(mfrow=c(1,3))\n\n# residuals vs fitted\nplot(residuals(mod1)~fitted(mod1)\n      , col='blue'\n      , pch=16)\nabline(h = 0)\n\n# residuals against Species\nboxplot(residuals(mod1)~ df$species, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Species\",\n         main = \"\")\nabline(h = 0)\n\n# residuals against Sex\nboxplot(residuals(mod1)~ df$sex, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Sex\",\n         main = \"\")\nabline(h = 0)\n\n\n\n\nWe can see here that for each plot, the variance of the residuals is evenly distributed around the horizontal line. The homogeneity of the variance is validated.\n\n\n4.4.3 Look at influential observations\n\npar(mfrow = c(1, 1))\n\n\nCookD(mod1,newwd=TRUE)\n\nWe can see that individuals 314, 315 and 325 contribute slightly more to the model, but this is not an aberrant result."
  },
  {
    "objectID": "Matrix.html",
    "href": "Matrix.html",
    "title": "Matrix models and applications in ecology",
    "section": "",
    "text": "Summary :\nI. Foundations of matric modelling in ecology A. Why using matric modelling in ecology B. Transform a population into a matrix C. How can we modify a simplified population matrix ?\n\nLeslie matrix A. Meerkat life cycle B. Leslie matrix\nEpidemiology matrices A. Introduction to epidemiology with the SIRS model B. Taking population dynamics and structure into account C. Setting up the model in R D. Visualizing results\nMarkov chain A- The Markov Chain B- The transition Matrix C- Evolution of the system D- Eigenvalues and eigenvectors\n\nI. Foundations of matrix modelling in ecology\nA. Why using matrix modelling in ecology ? \nMatrices are mathematical tools for modeling situations. Used in many fields, ecology is no exception . Matrices make it possible to represent populations and make them evolve according to different rules or events. The first uses of these models date back to 1941 by Harro Bernardelli which studied the oscillations of the structure of a population (Smith D. and Keyfitz N., 2012). Others use later matrices to represent populations, including Leslie in 1945 whose work marked a turning point in the use of matrices in ecology (Charles S., 2004). Subsequently, different models of matrix representation of populations were published and used in population dynamics.\nThe objective of a population matrix is, as we have already said, to represent the evolution of a population over time and its state at a particular time “t” for example, which will be represented as discreet in this chapter. This matrix is therefore based on population growth behaviours. Let’s recall the basics to better understand how population matrices were created :\nIn 1798, Malthus published his population growth model. If we consider N(t) as the population size at time t, and λ the rate of growth of this population, we can write that:\n\\[\nN(t+1) = λN(t)\n\\] If we want to write this equation for the population in a distant future, we can find by recurrence that :\n\\[\nN(t) = λ^tN(0)   \n\\] with N(0) the population at the beginning of our study.\nDepending on the sign of λ, our population will therefore increase or decrease over time (Hoppensteadt F.C., 1982). However, this model is largely simplified. Why? It considers that all individuals are identical, whether by their reproductive capacity or their ability to survive. This means that in this model, an elderly individual can reproduce and survive just as well as a young and vigorous individual who has just reached sexual maturity ((Frisman E.Ya. et al., 2021).\nBut then, how can we solve this problem? In the wild, each individual is different, but we can predict his qualities according to the average of his species and his position in the population: by position, we mean his place in different categories expressed according to various criteria such as age (young, adult, old), sex (male, female), or many others. These categories are commonly referred to as “classes”. Several individuals of the same class will have the same characteristics from the point of view of the criterion used for this class, and therefore will have the same impacts on the dynamics of their population. For example, they will have the same fertility or the same survival rate. Thus, we can determine the contribution of each class to another class and thus ultimately to the population, and this according to the size of the first class.\nThe size of a class “i” can thus be written:\n\\[\n\\begin{equation*}\nn_i(t+1) = \\sum_j (a_{ij} \\cdot n_j(t))\n\\end{equation*}\n\\]\nWhere “a” is the contribution and a_ij the contribution of a class “j” to class “i” at time t+1. We can then create a system considering all the classes, which can be translated into a matrix:\n\\[\n\\begin{equation}\nN(t+1) = A \\cdot N(t)\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\nN(t) = A^t \\cdot N(0)\n\\end{equation}\n\\] Where A is what we call a population projection matrix, representing our population in a matrix form, according to its different characteristics (classes, class parameters, etc.). N(t) is a vector dependent on the number of individuals present in our population, and N(0) is our initial population vector, from which we will be able to make this population evolve over time.\nPopulation dynamic is a concept that is sometimes difficult to study because it depends on many parameters. A population will vary in size and type of individuals through four main processes: - The birth of new individuals, depending on the fertility of those already present and their number; - mortality, which may depend on predation, disease or old age ; - immigration, the rates of which will depend on other populations, their distances from those studied and the dispersal capacity of individuals; - emigration, which will depend on the lifestyle of the species studied and its tendency to leave its original population.\nA matrix will allow to represent the population variation according to these four processes (Schaub M. and Abadi F., 2011 ; Abadi et al., 2010).\n\n\n\nScheme of the population dynamics\n\n\nThese representations can be very complete, including differences in age, gender, genetic origin and other individuals. Indeed, not all individuals will have the same impact on their population dynamics. Too young or too old individuals do not participate in reproduction and therefore in the future increase of their group. The number of males will not have the same impact on the number of offspring as the females. Finally, individuals of different ages will not have the same mortality rates. Matrices are thus widely used in epidemiology, where they will allow to model the spread of the disease in a population according to the virulence and infectivity of the disease. They may also focus on the population’s response to the disease, depending on individual mortality rates and resilience.\nIt is important to know that modeling matrices have limitations. They are based in particular on hypotheses that will allow their correct analysis and the validity of their results. Thus, interactions between individuals are not taken into account, and do not affect the dynamics of their population. Then, the caracteristics of individuals of the same class are identical, regardless of the individual, and constant over time. Different caracteristics are only found in different classes.\nBy allowing to model as correctly as possible the evolution of a population over time, according to the maximum of parameters that can positively or negatively impact it, we can obtain many very interesting results. Thus, population matrices can allow us to improve species conservation and management. Depending on the dynamics of its population, we can know if the species is threatened in the future and/or impacted by events (example: impact of fire (Hoffmann W.A., 1999)). We can have a more global and real vision of how populations evolve in their environment. However, matrix models may not be always necessary. As for each study, it is important to ask ourselves the right questions, to think about the interest of this mode of analysis compared to others and its relevance for the question/ population studied.\nB. Transform a population into a matrix\nWe were able to see what are the interests of making population matrices for our research. From now on, we will see together how to create these matrices.\nThe first thing to do before creating our matrix is to collect data. Scientists regularly conduct long-term monitoring and capture-recapture measurements to obtain maximum information on the population they wish to study. These processes also make it possible to know additional information such as the age of individuals and their gender for example.\nIt is necessary to know the different parameters that we will apply to our population dynamics. Thus, as we saw earlier, the population evolves according to four main concepts: births, deaths, immigration and emigration.\nIn order to correctly model the population that we study, we must be able to express these phenomena in the matrix. For this, we need to know how many individuals die, are born, immigrate or emigrate. These numbers will depend of the species but also on the individuals already present in the population. To represent births, we must know the fertility rate of the females already present: in short, how many females, how many young on average, which females are able to reproduce and how many females will be fertilized. Then, to represent the death of individuals we must know the mortality (or survival) rate of individuals in this population. It will depend on the composition of our population such as the age of its individuals, the average lifespan of the species and the rates of predation or death by disease for example. In terms of immigration rates, they will depend (as mentioned earlier in the first part) on the presence and distance of other populations around and the faculty/tendency of dispersal of the species. Finally, emigration rates will be based similarly on the same things, and also depend on the individuals already present.\nTo have all these parameters, we must already know a minimum of things about the species/population studied. These information can be retrieved during the population study project or in the literature, depending on what we want to study.\nObviously, depending on the question and the complexity of the study, these parameters may be more or less precise (for example, population of an invasive species with no predator, and therefore a mortality rate dependent on fewer factors).\nOnce all the parameters are known, we can formulate equations reflecting the evolution of the population according to these criteria. It is useful before making our matrix to realize a life cycle diagram of the studied population (Charles S., 2004).\nTo help you understand how matrix modeling of populations works, we will use an example more and more complex of a population matrix throughout our chapter.\nWe will take a population of meerkats as an example. Meerkats are a social species living in more or less numerous groups, with all kinds of individuals of different sex and age. They are also animals able to change groups by leaving theirs or by integrating a new one if the conditions oblige it. Finally, they are under pressure from different predators and potential diseases. Therefore, they will perfectly allow us to represent our population matrices with a maximum of criteria.\nTo start, we will begin with a very simplified situation:\nA serious epidemic decimated the original colony of a meerkat family, forcing them to immigrate to find a new one. The family is healthy and does not have the disease. It is composed of a male and a female, as well as their two young, a young male and a young female. If we look for the lifestyle of meerkats in the literature, we obtain in particular that females have 2 to 5 young per year. They can have young from one year old. In our example, we will consider an average of 3 cubs per year. In addition, individuals live on average 12 years (Réserve de Sigean, 2023).\nWe can represent our small, simplified population in a basic matrix form. Thus:\n\\[       \n\\begin{equation}   \n  P_1 = \\begin{bmatrix} M_1 & M_2 \\\\ F_1 & F_2 \\end{bmatrix}  \n\\end{equation}    \n\\]\nIn practice, it is rather rare to consider both sexes in population matrices. This is due to the greater complexity it would require to also consider gender interactions. For simplicity, the models are often based on females only (female-based model), mainly because of the importance of females for the number of descendants and the greater ease in calculating their number of descendants, compared to a male (Girel S., 2020 ; ) .\nWe will now see how to represent our population matrix on R, which will be our main tool in this chapter to manipulate and understand the dynamics of our population. To create a matrix similar to the one shown above, simply type this command:\nmatrix(nrow = number of rows desired , ncol = number of columns desired) thus in our case:\n\nmatrix(nrow = 2, ncol = 2)\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\n\nFor now, this matrix is empty. If we wanted to fill it directly, we could also use a vector as information:\n\nmatrix(data = c(1, 1, 1, 1) , nrow = 2)\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\n\nWe could also make a matrix with our abbreviations if we wanted:\n\nmatrix(data = c(\"M1 \"  , \"F1 \" , \"M2\" , \"F2 \" ), nrow = 2)\n\n     [,1]  [,2] \n[1,] \"M1 \" \"M2\" \n[2,] \"F1 \" \"F2 \"\n\n\nFinally, we can create a matrix from a list, which can be very convenient if our data set, our population, is represented in this form. In our current case, our list would be very simple since we only have four values. But this makes it easier to write our matrix if our list is much longer.\n\nliste &lt;- list(1, 1, 1, 1)    \n#ou alors\nliste2 &lt;- list(\"M1 \"  , \"F1 \" , \"M2 \" , \"F2 \" )\nmatrix &lt;- matrix(data = liste, nrow = 2)\nmatrix\n\n     [,1] [,2]\n[1,] 1    1   \n[2,] 1    1   \n\n#ou\nmatrix2 &lt;- matrix(data = liste2, nrow = 2)\nmatrix2\n\n     [,1]  [,2] \n[1,] \"M1 \" \"M2 \"\n[2,] \"F1 \" \"F2 \"\n\n\nHere, our model represents a stable population, at the moment t. It does not allow us to observe its evolution for the moment. In order to use it in population dynamics, we will have to make it more complex.\nC. How can we modify a simplified population matrix ?\nBefore making our matrix suitable for the study of population dynamics, we will start by seeing how simple matrix manipulations can allow us to modify the matrix representation of our population if the latter is modified. Let’s take our family of meerkats: during the journey, the young male is captured by a predator, which removes a member from the population and therefore changes the matrix. Similarly, later on, a lone male, lost, is welcomed by the family. Thus, our matrix will be successively modified in these ways:\n\\[\n\\begin{equation}\n  P_2 = \\begin{bmatrix} M_1 & M_2 \\\\ F_1 & F_2 \\end{bmatrix} - \\begin{bmatrix} 0 & M_2 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} M_1-0 & M_2-M_2 \\\\ F_1-0 & F_2-0 \\end{bmatrix} = \\begin{bmatrix} M_1 & 0 \\\\ F_1 & F_2 \\end{bmatrix}\n\\end{equation}\n\\] \\[\n\\begin{equation}\n  P_3 = \\begin{bmatrix} M_1 & 0 \\\\ F_1 & F_2 \\end{bmatrix} + \\begin{bmatrix} M_1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} M_1+M_1 & 0 \\\\ F_1+0 & F_2+0 \\end{bmatrix} = \\begin{bmatrix} 2M_1 & 0 \\\\ F_1 & F_2 \\end{bmatrix}\n\\end{equation}\n\\]\nThus, our final matrix represents our final population : two adult males, one young female and one adult female, with zero young males. The matrix does not represent each individual personally but the size of each group (adult male, adult female, young male, young female). It is important to remember that we can only add or subtract matrices with the same dimension.\nSubsequently, our new family finally joined a large group of meerkats residing in the area. This colony has 30 individuals, whose composition is revealed in the following matrix:\n\\[\n\\begin{equation}\n  P_n = \\begin{bmatrix} 11M_1 & 3M_2 \\\\ 14F_1 & 2F_2 \\end{bmatrix}\n\\end{equation}\n\\]\nIf we compile our two populations, we get our final population which is composed of 34 individuals:\n\\[\n\\begin{equation}\n  P = \\begin{bmatrix} 13M_1 & 3M_2 \\\\ 15F_1 & 3F_2 \\end{bmatrix}\n\\end{equation}\n\\]\nIt is from this population that we will develop the rest of our chapter, which will explain how we can study the evolution of this group over time, according to different parameters.\nIf we want to perform these calculations on R, the commands are quite simple. We have seen in the previous part how to create matrices on the software. If we want to compile them, just use the operation «+» or «-» as numbers. Of course, it is also necessary here to have matrices of the same dimension.\n\nP1 &lt;- matrix(data = c(1, 1, 1, 1) , nrow = 2)\nM &lt;- matrix(data = c(1, 1, 0, 1) , nrow = 2)\nP2 = P1 - M\nP2\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    0    0\n\nN &lt;- matrix(data = c(1, 0, 0, 0) , nrow = 2)\nP3 &lt;- P2 + N\nP3\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    0    0\n\nPn &lt;- matrix(data = c(11, 14, 3, 2) , nrow = 2)\nP = P3 + Pn\nP\n\n     [,1] [,2]\n[1,]   12    4\n[2,]   14    2\n\n\nSubsequently, we will need to multiply our population matrix with other matrices in order to make it evolve over time. So we will take advantage of this chapter to show you now how to do this. Once again, we will start by reviewing the calculation rules of matrices, then their applications in R. First, to multiply two matrices, they do not have to have the same dimension, unlike addition and subtraction operations. However, the number of columns in the first matrix must be equal to the number of rows in the second. Indeed, the multiplication of the two matrices will be a cross of the columns of one with the rows of the other, so it is important to have an equivalent number between the two. Let’s take the example of Pn, with a lambda B matrix for the moment. We will see in the next chapter which matrix is useful to multiply in reality with our population matrix.\n\\[\n\\begin{equation}\n  P_n \\mathbf{B} = \\mathbf{P'}\\mathbf{B} = \\begin{bmatrix} 11M_1 & 3M_2 \\\\ 14F_1 & 2F_2 \\end{bmatrix} * \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n  \\mathbf{P'}\\mathbf{B} = \\begin{bmatrix} 11M_1 \\ast a + 3M_2 \\ast b & 11M_1 \\ast c + 3M_2 \\ast d \\\\ 14F_1 \\ast a + 2F_2 \\ast b & 14F_1 \\ast c + 2F_2 \\ast d \\end{bmatrix}\n\\end{equation}\n\\]\nAs mentioned just before, matrices are not necessarily of the same dimension:\n\\[\n\\begin{equation}\n  P_n \\mathbf{N} = \\mathbf{P'}\\mathbf{N} = \\begin{bmatrix} 11M_1 & 3M_2 \\\\ 14F_1 & 2F_2 \\end{bmatrix} * \\begin{bmatrix} a \\\\ b \\end{bmatrix}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n  P_n \\mathbf{N} = \\begin{bmatrix} 11M_1 \\ast a + 3M_2 \\ast b \\\\ 14F_1 \\ast a + 2F_2 \\ast b \\end{bmatrix}\n\\end{equation}\n\\]\nIt is important to note that most of the time, multiplications are not commutative: we do not get the same result by doing Pn x B as by doing B x Pn. The order of multiplication is important, on the contrary of the multiplication of numbers that can be reversed as desired.\n\\[\n\\begin{equation}\n  \\mathbf{B} \\times P_n = \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix} \\times \\begin{bmatrix} 11M_1 & 3M_2 \\\\ 14F_1 & 2F_2 \\end{bmatrix} = \\begin{bmatrix} a \\ast 11M_1 + c \\ast 14F_1 & a \\ast 3M_2 + c \\ast 2F_2 \\\\ b \\ast 11M_1 + d \\ast 14F_1 & b \\ast 3M_2 + d \\ast 2F_2 \\end{bmatrix}\n\\end{equation}\n\\]\nNow that the matrix product calculations necessary for our future models are generally acquired, we will see how to perform them on R. If we simply want to multiply our matrix with a number, the sign “*” is enough:\n\\[\n\\begin{equation}\n  P_m = 2 \\times P_n\n\\end{equation}\n\\]\nIf we want to multiply several matrices together, we must add the symbol ‘%’ for each matrix, in addition to ’*’. Thus:\n\nN &lt;- matrix(data = c(2, 4) , nrow = 2)\nPf = Pn %*% N \n\n\nLeslie matrix\nA. Meerkat life cycle\n\nWe’re now back with our meerkat family 10 years later. During this decade, our population has evolved and different age classes have formed. First of all, we have the class of juveniles, noted J, which are between 0 and 1 year old and correspond to individuals not yet sexually mature. Only a small proportion of these juveniles reach adulthood, following a rate of passage noted TPa. Then we move on to the adult age class, noted A, which can reproduce with a fecundity rate noted TFa and which are all between 1 and 10 years old. Adult meerkat females survive from one year to the next with a survival rate of TSa, and only a small proportion of these females make it to the next age class. reach the senior age class, following a passage rate noted TPs.\nFor the sake of our story, we’ll consider that from the age of 10 our adult meerkats belong to the senior age class, noted S, and that they have a lower fecundity rate than adults, noted TFs. Senior female meerkats survive from one year to the next with a survival rate noted TSs.\nWith all this information, we can draw the following tree of life:\n\n\n\nFigure 2 : Meerkat life cycle\n\n\nTo complete this cycle , we search for data corresponding the most to reality in the wild. Thus, we estimate that :\n\nan adult female meerkat will have an average of 3 offspring per year\naround 90% of adult meerkat females survive from one year to the next, until their tenth year.\na senior female meerkat will have an average of 1 young per year\n40% of senior meerkat females still alive at the end of their eleventh year will survive, but not beyond their twelfth year.\none juvenile out of two does not survive its first year of life\n30% of adult female meerkats reach the senior stage.\n\nWith all this information, we can complete our life cycle:\n\n\n\nFigure 3 : Meerkat life cycle\n\n\nIf we note, for any instant t (time being counted in years, so that t is an integer), Jt the number of juvenile meerkats, At the number of adults and St the number of seniors, we get :\n\\[\nN_t = J_t + A_t + S_t\n\\]\nThis corresponds to the total number of meerkats in the study population.\nThe above assumptions translate into the following equations:\n\\[\n\\begin{align*}\nJ_{t+1} &= 0 \\cdot J_t + TF_a \\cdot A_t + TF_s \\cdot S_t \\\\\nA_{t+1} &= TP_a \\cdot J_t + TS_a \\cdot A_t + 0 \\cdot S_t \\\\\nS_{t+1} &= 0 \\cdot J_t + TP_s \\cdot A_t + TS_s \\cdot S_t\n\\end{align*}\n\\]\nWe can then replace them with our values:\n\\[\n\\begin{align*}\nJ_{t+1} &= 0 \\cdot J_t + 3 \\cdot A_t + 1 \\cdot S_t \\\\\nA_{t+1} &= 0.5 \\cdot J_t + 0.9 \\cdot A_t + 0 \\cdot S_t \\\\\nS_{t+1} &= 0 \\cdot J_t + 0.3 \\cdot A_t + 0.4 \\cdot S_t\n\\end{align*}\n\\]\nB. Leslie matrix\nThis second system allows us to establish a matrix. The use of a matrix will enable us to determine various parameters, such as the growth rate, to understand and predict the evolution of the population over time. This type of matrix has a name : the Leslie matrix. The term “Leslie matrix” refers to the British economist and demographer Patrick Leslie, who introduced this method of demographic modeling in the 1940s. Initially developed to study the growth of insect populations, it was later adapted and extended to human and animal demographic modeling.\nSo, we can create this matrix:\n\\[\n\\begin{bmatrix}\n0 & TF_a & TF_s \\\\\nTP_a & TS_a & 0 \\\\\n0 & TP_s & TS_s \\\\\n\\end{bmatrix}\n\\]\nThis matrix is constructed in such a way that we have Jt+1, At+1 and St+1 in column order, and Jt, At and St in row order. You will notice that on the first row we have the indicators concerning the fertility of our individuals, noted TF. On the diagonal we have the parameters concerning survival and on the diagonal below those concerning the probability of passing from one age stage to another.\nSo, if we replace by our values, we obtain the following matrix :\n\\[\n\\begin{bmatrix}\n0 & 3 & 1 \\\\\n0.5 & 0.9 & 0 \\\\\n0 & 0.3 & 0.4 \\\\\n\\end{bmatrix}\n\\]\nNow that we have our matrix, we can enter it into the R software in order to perform various calculations, which we’ll detail later:\nTo enter and create our matrix in R, we’ll write the following lines:\n\nTF_a = 3\nTF_s = 1\nTP_a = 0.5\nTS_a = 0.9\nTP_s = 0.3\nTS_s = 0.4\nleslie&lt;-c(0,TF_a,TF_s,TP_a,TS_a,0,0,TP_s,TS_s) \n# With this line of code we have created our list named leslie, which includes all the parameters present in our life cycle\nmatrice&lt;-matrix(data=leslie, nrow = 3,ncol = 3, byrow=TRUE, dimnames = ) \n#  The matrix() function transforms this list into a matrix\n\nThen, in order to perform all the calculations related to our matrix, we need to install a package called popbio\n\n#install.packages(\"popbio\")\nlibrary(popbio)\n\nThanks to this package, we can calculate various parameters:\nThe first parameter we’ll calculate is the growth rate. The growth rate is a demographic indicator that shows the evolution of a population at a given time. In our example, we obtain it as follows:\n\ngrowth_rate = lambda(matrice)\n\nThis gives us a growth rate of 1.795362. Our growth rate is greater than 1, indicating that our population is increasing. This type of rate is generally associated with factors such as a high birth rate, low mortality, positive net migration, or a combination of these. It indicates that our population is dynamic and growing. If the growth rate had been less than 1, it would have indicated that our population was in decline. Typically, these types of growth rates are associated with aging populations, with negative demographic trends. This is not the case in our meerkat population.\nNow that we know our population is growing, let’s look at generation time. Generation time is the average time it takes for each individual to be replaced in the population. So we’re going to use the generation.time() function available in the popbio package.\n\ngeneration.time(matrice)\n\n[1] 4.890923\n\n# 4.890923\n\nIn our case, the time step we have taken is 1 year, so we have a generation time of 4.8 years. Generation time represents the average length of time between the birth of a parent generation and the birth of the next generation, giving us a perspective on how demographic characteristics are transmitted across generations.\nWe’re now going to calculate the stable age structure, which will be in the form of a proportion per age class, and which represents the expected distribution of the different age groups in a stationary population. age groups in a stationary population. To obtain these proportions, it is generally assumed that the population is stable in the sense that the number of births, deaths and migrants are balanced, and that the age distribution remains constant.\nTo calculate them, we do this :\n\nstable.stage(matrice)\n\n[1] 0.59577168 0.33269876 0.07152956\n\n# 0.59577168 0.33269876 0.07152956\n\nSo, assuming a stable population, we would need: 60% juveniles, 33% adults and 0.07% seniors.\nThis stable age structure can be represented as follows:\n\nbarplot(stable.stage(matrice),names.arg=c(\"juvenile\",\"adult\",\"senior\"),ylim = c(0,0.7))\n\n\n\n\nWe can also calculate the reproductive values for each age class, enabling us to assess the contribution of each age group to reproduction and population growth. The reproductive value of an age class is calculated by taking into account the survival rates and reproductive rates specific to each age class. It represents the average number of offspring that an individual of a particular age class can expect to produce during its lifetime.\nWe can therefore write :\n\nreproductive.value(matrice)\n\n[1] 1.0000000 3.5907241 0.7166599\n\n# 1.0000000 3.5907241 0.7166599\n\nWe thus obtain: 1 corresponding to the basic index (juvenile class), 3.59 (adult class), 0.71 (senior class). Since juveniles can’t reproduce, we don’t take this class into account, hence the 1. We can see that it’s the adults who contribute the most to reproduction, and that the seniors contribute just a little.\nWe can then calculate how long it will take our population to recover from a disturbance. This concept is important because it is often linked to the notion of demographic resilience. Demographic resilience refers to the ability of a population to recover recover from disruptions such as natural disasters, epidemics, environmental changes, or other events likely to affect population structure or size.\nTo do this, we write this :\n\ndamping.ratio(matrice)\n\n[1] 2.226831\n\n# we obtain 2.23 which we will multiplicate by the generation time \ndamping.ratio(matrice)*generation.time(matrice)\n\n[1] 10.89126\n\n# we obtain 10.89126\n\nWe can see that it would take about 11 years for our population to recover from a disruption such as a natural disaster, environmental change etc…\nNow that we’ve seen how our population behaves, let’s look at projections for different population sizes.\nFirst, when the population is in a state close to stable, i.e. 60% juveniles, 33% adults and 7% seniors. We’ll start with a time step of 20.\n\npop.projection(matrice,c(60,33,7),20) #proche de la stucture stable\n\n$lambda\n[1] 1.795362\n\n$stable.stage\n[1] 0.59577168 0.33269876 0.07152956\n\n$stage.vectors\n      0     1      2       3        4         5         6         7         8\n[1,] 60 106.0 191.80 343.180 617.0860 1107.1270 1988.3121 3569.2419 6408.4831\n[2,] 33  59.7 106.73 191.957 344.3513  618.4592 1110.1768 1993.3151 3578.6046\n[3,]  7  12.7  22.99  41.215  74.0731  132.9346  238.7116  428.5377  769.4096\n             9        10        11        12        13        14        15\n[1,] 11505.223 20656.302 37085.330 66581.763 119538.23 214614.51 385310.66\n[2,]  6424.986 11535.099 20709.740 37181.431  66754.17 119847.87 215170.34\n[3,]  1381.345  2480.034  4452.543  7993.939  14352.00  25767.05  46261.18\n            16        17        18        19\n[1,] 691772.20 1241981.5 2229806.4 4003309.7\n[2,] 386308.63  693563.9 1245198.2 2235581.6\n[3,]  83055.57  149114.8  267715.1  480645.5\n\n$pop.sizes\n [1]     100.000     178.400     321.520     576.352    1035.510    1858.521\n [7]    3337.200    5991.095   10756.497   19311.554   34671.435   62247.613\n[13]  111757.133  200644.407  360229.435  646742.179 1161136.405 2084660.164\n[19] 3742719.735 6719536.860\n\n$pop.changes\n [1] 1.784000 1.802242 1.792585 1.796663 1.794787 1.795622 1.795246 1.795414\n [9] 1.795339 1.795373 1.795357 1.795364 1.795361 1.795362 1.795362 1.795362\n[17] 1.795362 1.795362 1.795362\n\n# we enter the name of the matrix, a vector with the initial size of each class and the number of time steps\n# we get population sizes per year (popsize) and growth rates (popchanges)\n# $pop.sizes\n# [1]     100.000     178.400     321.520     576.352    1035.510    1858.521    3337.200    5991.095   10756.497\n# [10]   19311.554   34671.435   62247.613  111757.133  200644.407  360229.435  646742.179 1161136.405 2084660.164\n# [19] 3742719.735 6719536.860\n\n# $pop.changes\n# [1] 1.784000 1.802242 1.792585 1.796663 1.794787 1.795622 1.795246 1.795414 1.795339 1.795373 1.795357 1.795364\n# [13] 1.795361 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n\nThus, we can see with the pop.sizes that after 19 years, in a stable situation, we would have a population of 6719536.860 individuals and that the growth rate from 1.784000 to 1.795362.\nAnd if we run this simulation over 100 time steps, we get :\n\npop.projection(matrice,c(60,33,7),100)\n\n$lambda\n[1] 1.795362\n\n$stable.stage\n[1] 0.59577168 0.33269876 0.07152956\n\n$stage.vectors\n      0     1      2       3        4         5         6         7         8\n[1,] 60 106.0 191.80 343.180 617.0860 1107.1270 1988.3121 3569.2419 6408.4831\n[2,] 33  59.7 106.73 191.957 344.3513  618.4592 1110.1768 1993.3151 3578.6046\n[3,]  7  12.7  22.99  41.215  74.0731  132.9346  238.7116  428.5377  769.4096\n             9        10        11        12        13        14        15\n[1,] 11505.223 20656.302 37085.330 66581.763 119538.23 214614.51 385310.66\n[2,]  6424.986 11535.099 20709.740 37181.431  66754.17 119847.87 215170.34\n[3,]  1381.345  2480.034  4452.543  7993.939  14352.00  25767.05  46261.18\n            16        17        18        19        20       21       22\n[1,] 691772.20 1241981.5 2229806.4 4003309.7 7187390.3 12903968 23167294\n[2,] 386308.63  693563.9 1245198.2 2235581.6 4013678.3  7206006 12937389\n[3,]  83055.57  149114.8  267715.1  480645.5  862932.7  1549277  2781512\n           23       24        25        26        27        28         29\n[1,] 41593679 74675712 134069938 240704075 432150956 775867417 1392962898\n[2,] 23227297 41701407  74869122 134417179 241327498 433270227  777876912\n[3,]  4993822  8965718  16096709  28899420  51884922  93152218  167241955\n             30         31         32          33          34          35\n[1,] 2500872692 4489971866 8061124990 14472637702 25983624162 46650012146\n[2,] 1396570670 2507349949 4501600887  8082003293 14510121815 26050921714\n[3,]  300259856  539075143  967835042  1737614283  3119646701  5600895225\n              36           37           38           39           40\n[1,] 83753660368 150368141452 269965251242 484685360705 870185691679\n[2,] 46770835616  83970582239 150757594741 270664460888 485940695152\n[3,] 10055634604  18053504527  32412576482  58192309015 104476261872\n               41           42           43           44           45\n[1,] 1.562298e+12 2.804891e+12 5.035795e+12 9.041075e+12 1.623200e+13\n[2,] 8.724395e+11 1.566345e+12 2.812156e+12 5.048838e+12 9.064492e+12\n[3,] 1.875727e+11 3.367609e+11 6.046078e+11 1.085490e+12 1.948847e+12\n               46           47           48           49           50\n[1,] 2.914232e+13 5.232102e+13 9.393517e+13 1.686476e+14 3.027836e+14\n[2,] 1.627404e+13 2.921780e+13 5.245653e+13 9.417846e+13 1.690844e+14\n[3,] 3.498886e+12 6.281768e+12 1.127805e+13 2.024818e+13 3.635281e+13\n               51           52           53           54           55\n[1,] 5.436061e+14 9.759698e+14 1.752219e+15 3.145868e+15 5.647971e+15\n[2,] 3.035678e+14 5.450140e+14 9.784975e+14 1.756757e+15 3.154015e+15\n[3,] 6.526645e+13 1.171769e+14 2.103750e+14 3.776992e+14 6.781069e+14\n               56           57           58           59           60\n[1,] 1.014015e+16 1.820525e+16 3.268501e+16 5.868142e+16 1.053544e+17\n[2,] 5.662599e+15 1.016642e+16 1.825240e+16 3.276966e+16 5.883340e+16\n[3,] 1.217447e+15 2.185759e+15 3.924228e+15 7.045410e+15 1.264906e+16\n               61           62           63           64           65\n[1,] 1.891493e+17 3.395914e+17 6.096896e+17 1.094613e+18 1.965227e+18\n[2,] 1.056273e+17 1.896392e+17 3.404710e+17 6.112686e+17 1.097449e+18\n[3,] 2.270965e+16 4.077204e+16 7.320057e+16 1.314215e+17 2.359492e+17\n               66           67           68           69           70\n[1,] 3.528295e+18 6.334566e+18 1.137284e+19 2.041837e+19 3.665836e+19\n[2,] 1.970317e+18 3.537433e+18 6.350973e+18 1.140230e+19 2.047125e+19\n[3,] 4.236142e+17 7.605409e+17 1.365446e+18 2.451470e+18 4.401277e+18\n               71           72           73           74           75\n[1,] 6.581502e+19 1.181618e+20 2.121432e+20 3.808738e+20 6.838064e+20\n[2,] 3.675330e+19 6.598548e+19 1.184678e+20 2.126926e+20 3.818603e+20\n[3,] 7.901885e+18 1.418674e+19 2.547034e+19 4.572849e+19 8.209919e+19\n               76           77           78           79           80\n[1,] 1.227680e+21 2.204130e+21 3.957212e+21 7.104628e+21 1.275538e+22\n[2,] 6.855775e+20 1.230860e+21 2.209839e+21 3.967461e+21 7.123029e+21\n[3,] 1.473978e+20 2.646324e+20 4.751109e+20 8.529960e+20 1.531437e+21\n               81           82           83           84           85\n[1,] 2.290052e+22 4.111473e+22 7.381582e+22 1.325261e+23 2.379324e+23\n[2,] 1.278841e+22 2.295983e+22 4.122122e+22 7.400700e+22 1.328694e+23\n[3,] 2.749483e+21 4.936318e+21 8.862478e+21 1.591136e+22 2.856664e+22\n               86           87           88           89           90\n[1,] 4.271747e+23 7.669333e+23 1.376923e+24 2.472075e+24 4.438270e+24\n[2,] 2.385486e+23 4.282811e+23 7.689197e+23 1.380489e+24 2.478478e+24\n[3,] 5.128747e+22 9.207957e+22 1.653162e+23 2.968024e+23 5.328677e+23\n               91           92           93           94           95\n[1,] 7.968301e+24 1.430599e+25 2.568442e+25 4.611284e+25 8.278924e+25\n[2,] 4.449765e+24 7.988939e+24 1.434304e+25 2.575095e+25 4.623227e+25\n[3,] 9.566904e+23 1.717606e+24 3.083724e+24 5.536401e+24 9.939844e+24\n               96           97           98           99\n[1,] 1.486367e+26 2.668566e+26 4.791042e+26 8.601655e+26\n[2,] 8.300366e+25 1.490216e+26 2.675478e+26 4.803451e+26\n[3,] 1.784562e+25 3.203935e+25 5.752222e+25 1.032732e+26\n\n$pop.sizes\n  [1] 1.000000e+02 1.784000e+02 3.215200e+02 5.763520e+02 1.035510e+03\n  [6] 1.858521e+03 3.337200e+03 5.991095e+03 1.075650e+04 1.931155e+04\n [11] 3.467143e+04 6.224761e+04 1.117571e+05 2.006444e+05 3.602294e+05\n [16] 6.467422e+05 1.161136e+06 2.084660e+06 3.742720e+06 6.719537e+06\n [21] 1.206400e+07 2.165925e+07 3.888619e+07 6.981480e+07 1.253428e+08\n [26] 2.250358e+08 4.040207e+08 7.253634e+08 1.302290e+09 2.338082e+09\n [31] 4.197703e+09 7.536397e+09 1.353056e+10 2.429226e+10 4.361339e+10\n [36] 7.830183e+10 1.405801e+11 2.523922e+11 4.531354e+11 8.135421e+11\n [41] 1.460603e+12 2.622311e+12 4.707997e+12 8.452559e+12 1.517540e+13\n [46] 2.724534e+13 4.891525e+13 8.782059e+13 1.576697e+14 2.830743e+14\n [51] 5.082208e+14 9.124403e+14 1.638161e+15 2.941092e+15 5.280324e+15\n [56] 9.480093e+15 1.702020e+16 3.055742e+16 5.486163e+16 9.849649e+16\n [61] 1.768369e+17 3.174862e+17 5.700026e+17 1.023361e+18 1.837304e+18\n [66] 3.298625e+18 5.922226e+18 1.063254e+19 1.908926e+19 3.427213e+19\n [71] 6.153088e+19 1.104702e+20 1.983340e+20 3.560814e+20 6.392950e+20\n [76] 1.147766e+21 2.060655e+21 3.699622e+21 6.642161e+21 1.192508e+22\n [81] 2.140984e+22 3.843842e+22 6.901088e+22 1.238995e+23 2.224445e+23\n [86] 3.993684e+23 7.170108e+23 1.287294e+24 2.311159e+24 4.149367e+24\n [91] 7.449615e+24 1.337476e+25 2.401253e+25 4.311118e+25 7.740018e+25\n [96] 1.389614e+26 2.494859e+26 4.479176e+26 8.041742e+26 1.443784e+27\n\n$pop.changes\n [1] 1.784000 1.802242 1.792585 1.796663 1.794787 1.795622 1.795246 1.795414\n [9] 1.795339 1.795373 1.795357 1.795364 1.795361 1.795362 1.795362 1.795362\n[17] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[25] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[33] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[41] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[49] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[57] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[65] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[73] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[81] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[89] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[97] 1.795362 1.795362 1.795362\n\n\nWe would then go from an initial population of 1.000000e+02 individuals to a population of 1.443784e+27 individuals. In terms of growth rate, we’d go from 1.784000 to 1.795362.\nWe can thus observe a very significant growth in this population.\nNevertheless, this stable population situation is not representative of reality. So let’s try with other initial effectics, with an unbalanced structure.\n\npop.projection(matrice,c(1,4,2),20) #unbalanced structure\n\n$lambda\n[1] 1.795363\n\n$stable.stage\n[1] 0.59577175 0.33269869 0.07152956\n\n$stage.vectors\n     0    1     2      3       4         5         6         7         8\n[1,] 1 14.0 14.30 34.100 54.3320 103.07060 180.58526 327.81192 585.64093\n[2,] 4  4.1 10.69 16.771 32.1439  56.09551 102.02126 182.11176 327.80655\n[3,] 2  2.0  2.03  4.019  6.6389  12.29873  21.74815  39.30564  70.35578\n             9        10        11        12        13       14        15\n[1,] 1053.7754 1890.0233 3394.7959 6093.6624 10941.318 19642.83 35266.634\n[2,]  587.8464 1055.9494 1895.3662 3403.2275  6109.736 10969.42 19693.894\n[3,]  126.4843  226.9476  407.5639  731.6354  1313.622  2358.37  4234.174\n            16        17        18        19\n[1,] 63315.857 113675.30 204087.99 366412.09\n[2,] 35357.822  63479.97 113969.62 204616.65\n[3,]  7601.838  13648.08  24503.22  43992.18\n\n$pop.sizes\n [1]      7.0000     20.1000     27.0200     54.8900     93.1148    171.4648\n [7]    304.3547    549.2293    983.8033   1768.1061   3172.9204   5697.7260\n[13]  10228.5253  18364.6763  32970.6213  59194.7022 106275.5168 190803.3528\n[19] 342560.8318 615020.9209\n\n$pop.changes\n [1] 2.871429 1.344279 2.031458 1.696389 1.841435 1.775027 1.804570 1.791243\n [9] 1.797215 1.794531 1.795736 1.795194 1.795437 1.795328 1.795377 1.795355\n[17] 1.795365 1.795361 1.795363\n\npop.projection(matrice,c(1,4,2),100)\n\n$lambda\n[1] 1.795362\n\n$stable.stage\n[1] 0.59577168 0.33269876 0.07152956\n\n$stage.vectors\n     0    1     2      3       4         5         6         7         8\n[1,] 1 14.0 14.30 34.100 54.3320 103.07060 180.58526 327.81192 585.64093\n[2,] 4  4.1 10.69 16.771 32.1439  56.09551 102.02126 182.11176 327.80655\n[3,] 2  2.0  2.03  4.019  6.6389  12.29873  21.74815  39.30564  70.35578\n             9        10        11        12        13       14        15\n[1,] 1053.7754 1890.0233 3394.7959 6093.6624 10941.318 19642.83 35266.634\n[2,]  587.8464 1055.9494 1895.3662 3403.2275  6109.736 10969.42 19693.894\n[3,]  126.4843  226.9476  407.5639  731.6354  1313.622  2358.37  4234.174\n            16        17        18        19        20        21        22\n[1,] 63315.857 113675.30 204087.99 366412.09 657842.14 1181065.0 2120439.1\n[2,] 35357.822  63479.97 113969.62 204616.65 367361.03  659546.0 1184123.9\n[3,]  7601.838  13648.08  24503.22  43992.18  78981.87  141801.1  254584.2\n            23        24       25       26       27       28        29\n[1,] 3806955.9 6834863.9 12271055 22030986 39553597 71013025 127494089\n[2,] 2125931.0 3816815.9  6852566 12302837 22088047 39656040  71196949\n[3,]  457070.9  820607.6  1473288  2645085  4748885  8525968  15307199\n            30        31        32         33         34         35         36\n[1,] 228898046 410954861 737812751 1324640997 2378210146 4269748188 7665743761\n[2,] 127824299 229490892 412019233  739723686 1328071815 2384369707 4280806830\n[3,]  27481964  49340075  88583298  159039089  285532741  512634641  920364768\n              37          38          39          40           41           42\n[1,] 13762785258 24709182039 44361927153 79645719458 142992900331 256724023394\n[2,]  7685598028 13798430854 24773178788 44476824486  79852001766 143363251755\n[3,]  1652387956  2966634591  5326183093  9562426874  17168018095  30822807768\n               43           44           45           46           47\n[1,] 460912563034 827504913463 1.485671e+12 2.667317e+12 4.788800e+12\n[2,] 257388938277 462106325966 8.296482e+11 1.489519e+12 2.674225e+12\n[3,]  55338098634  99351920936 1.783727e+11 3.202435e+11 5.749530e+11\n               48           49           50           51           52\n[1,] 8.597629e+12 1.543586e+13 2.771295e+13 4.975478e+13 8.932785e+13\n[2,] 4.801203e+12 8.619897e+12 1.547584e+13 2.778473e+13 4.988365e+13\n[3,] 1.032249e+12 1.853260e+12 3.327273e+12 5.973660e+12 1.072488e+13\n               53           54           55           56           57\n[1,] 1.603758e+14 2.879327e+14 5.169434e+14 9.281005e+14 1.666276e+15\n[2,] 8.955921e+13 1.607912e+14 2.886784e+14 5.182823e+14 9.305043e+14\n[3,] 1.925505e+13 3.456978e+13 6.206527e+13 1.114296e+14 2.000565e+14\n               58           59           60           61           62\n[1,] 2.991569e+15 5.370950e+15 9.642800e+15 1.731232e+16 3.108188e+16\n[2,] 1.670592e+15 2.999318e+15 5.384861e+15 9.667775e+15 1.735716e+16\n[3,] 3.591739e+14 6.448472e+14 1.157734e+15 2.078552e+15 3.731753e+15\n               63           64           65           66           67\n[1,] 5.580322e+16 1.001870e+17 1.798719e+17 3.229352e+17 5.797856e+17\n[2,] 3.116238e+16 5.594775e+16 1.004465e+17 1.803378e+17 3.237716e+17\n[3,] 6.699848e+15 1.202865e+16 2.159579e+16 3.877225e+16 6.961023e+16\n               68           69           70           71           72\n[1,] 1.040925e+18 1.868837e+18 3.355239e+18 6.023869e+18 1.081503e+19\n[2,] 5.812872e+17 1.043621e+18 1.873677e+18 3.363929e+18 6.039471e+18\n[3,] 1.249756e+17 2.243764e+17 4.028368e+17 7.232380e+17 1.298474e+18\n               73           74           75           76           77\n[1,] 1.941689e+19 3.486034e+19 6.258693e+19 1.123662e+20 2.017380e+20\n[2,] 1.084304e+19 1.946718e+19 3.495063e+19 6.274903e+19 1.126572e+20\n[3,] 2.331231e+18 4.185403e+18 7.514314e+18 1.349091e+19 2.422108e+19\n               78           79           80           81           82\n[1,] 3.621928e+20 6.502672e+20 1.167465e+21 2.096022e+21 3.763119e+21\n[2,] 2.022605e+20 3.631309e+20 6.519513e+20 1.170489e+21 2.101451e+21\n[3,] 4.348560e+19 7.807240e+19 1.401682e+20 2.516527e+20 4.518077e+20\n               83           84           85           86           87\n[1,] 6.756161e+21 1.212975e+22 2.177730e+22 3.909814e+22 7.019531e+22\n[2,] 3.772865e+21 6.773659e+21 1.216117e+22 2.183370e+22 3.919940e+22\n[3,] 8.111584e+20 1.456323e+21 2.614627e+21 4.694202e+21 8.427792e+21\n               88           89           90           91           92\n[1,] 1.260260e+23 2.262623e+23 4.062227e+23 7.293168e+23 1.309388e+24\n[2,] 7.037712e+22 1.263524e+23 2.268483e+23 4.072748e+23 7.312058e+23\n[3,] 1.513094e+22 2.716551e+22 4.877192e+22 8.756326e+22 1.572078e+23\n               93           94           95           96           97\n[1,] 2.350825e+24 4.220582e+24 7.577473e+24 1.360431e+25 2.442466e+25\n[2,] 1.312779e+24 2.356914e+24 4.231513e+24 7.597098e+24 1.363954e+25\n[3,] 2.822448e+23 5.067317e+23 9.097668e+23 1.633361e+24 2.932474e+24\n               98           99\n[1,] 4.385110e+25 7.872860e+25\n[2,] 2.448792e+25 4.396467e+25\n[3,] 5.264852e+24 9.452316e+24\n\n$pop.sizes\n  [1] 7.000000e+00 2.010000e+01 2.702000e+01 5.489000e+01 9.311480e+01\n  [6] 1.714648e+02 3.043547e+02 5.492293e+02 9.838033e+02 1.768106e+03\n [11] 3.172920e+03 5.697726e+03 1.022853e+04 1.836468e+04 3.297062e+04\n [16] 5.919470e+04 1.062755e+05 1.908034e+05 3.425608e+05 6.150209e+05\n [21] 1.104185e+06 1.982412e+06 3.559147e+06 6.389958e+06 1.147229e+07\n [26] 2.059691e+07 3.697891e+07 6.639053e+07 1.191950e+08 2.139982e+08\n [31] 3.842043e+08 6.897858e+08 1.238415e+09 2.223404e+09 3.991815e+09\n [36] 7.166753e+09 1.286692e+10 2.310077e+10 4.147425e+10 7.446129e+10\n [41] 1.336850e+11 2.400129e+11 4.309101e+11 7.736396e+11 1.388963e+12\n [46] 2.493692e+12 4.477079e+12 8.037978e+12 1.443108e+13 2.590902e+13\n [51] 4.651606e+13 8.351317e+13 1.499364e+14 2.691901e+14 4.832936e+14\n [56] 8.676871e+14 1.557812e+15 2.796837e+15 5.021335e+15 9.015115e+15\n [61] 1.618539e+16 2.905864e+16 5.217078e+16 9.366544e+16 1.681634e+17\n [66] 3.019142e+17 5.420452e+17 9.731674e+17 1.747188e+18 3.136835e+18\n [71] 5.631754e+18 1.011104e+19 1.815297e+19 3.259116e+19 5.851292e+19\n [76] 1.050519e+20 1.886062e+20 3.386163e+20 6.079389e+20 1.091470e+21\n [81] 1.959585e+21 3.518164e+21 6.316377e+21 1.134018e+22 2.035974e+22\n [86] 3.655310e+22 6.562604e+22 1.178225e+23 2.115340e+23 3.797802e+23\n [91] 6.818429e+23 1.224155e+24 2.197801e+24 3.945849e+24 7.084227e+24\n [96] 1.271875e+25 2.283477e+25 4.099667e+25 7.360387e+25 1.321456e+26\n\n$pop.changes\n [1] 2.871429 1.344279 2.031458 1.696389 1.841435 1.775027 1.804570 1.791243\n [9] 1.797215 1.794531 1.795736 1.795194 1.795437 1.795328 1.795377 1.795355\n[17] 1.795365 1.795361 1.795363 1.795362 1.795362 1.795362 1.795362 1.795362\n[25] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[33] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[41] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[49] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[57] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[65] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[73] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[81] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[89] 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362 1.795362\n[97] 1.795362 1.795362 1.795362\n\n\nIn this case, we also observe a very significant growth in our population.\nTo differentiate between the two, we use graphical representations.\n\n# over 20 years for population size\nplot((pop.projection(matrice,c(60,33,7),20)$pop.sizes),type=\"l\",ylim=c(100,100000),ylab=\"population size\",xlab=\"year\")\nlines(pop.projection(matrice,c(1,4,2),20)$pop.sizes,type=\"l\",col=\"red\")\nlegend(\"topright\",legend=c(\"balanced\",\"unbalanced\"),text.col=c(\"black\",\"red\")) \n\n\n\n# over 100 years for population size\nplot((pop.projection(matrice,c(60,33,7),100)$pop.sizes),type=\"l\",xlim = c(0,30), ylim=c(100,10000000),ylab=\"population size\",xlab=\"year\")\nlines(pop.projection(matrice,c(1,4,2),100)$pop.sizes,type=\"l\",col=\"red\")\nlegend(\"topright\",legend=c(\"balanced\",\"Unbalanced\"),text.col=c(\"black\",\"red\"))  \n\n\n\n# over 20 years for growth rate \nplot((pop.projection(matrice,c(60,33,7),20)$pop.changes),type=\"l\",xlim = c(0,10),ylim=c(1,3),ylab=\"growth rate\",xlab=\"year\")\nlines(pop.projection(matrice,c(1,4,2),20)$pop.changes,type=\"l\",col=\"red\")\nlegend(\"bottomright\",legend=c(\"balanced\",\"unbalanced\"),text.col=c(\"black\",\"red\"))\n\n\n\n# over 100 years on growth rate\nplot((pop.projection(matrice,c(60,33,7),100)$pop.changes),type=\"l\",xlim = c(0,10),ylim=c(1,3),ylab=\"growth rate\",xlab=\"year\")\nlines(pop.projection(matrice,c(1,4,2),100)$pop.changes,type=\"l\",col=\"red\")\nlegend(\"bottomright\",legend=c(\"balanced\",\"unbalanced\"),text.col=c(\"black\",\"red\")) \n\n\n\n\nFinally, using the elasticity() function, we can see that the highest rate will contribute the most to the evolution of our population:\n\nelasticity(matrice)\n\n          [,1]       [,2]        [,3]\n[1,] 0.0000000 0.30186317 0.021633323\n[2,] 0.3234965 0.32517220 0.000000000\n[3,] 0.0000000 0.02163332 0.006201494\n\n# we obtain this new matrix\n#           [,1]       [,2]        [,3]\n# [1,] 0.0000000 0.30186317 0.021633323\n# [2,] 0.3234965 0.32517220 0.000000000\n# [3,] 0.0000000 0.02163332 0.006201494\n\nWith this new matrix, we can see that : - There are 0s where there were 0s in the basic matrix. - The largest number, 0.32517220, corresponds to the adult survival rate, which means that this rate will have the greatest impact on lambda. - The second, 0.3234965, corresponds to the juvenile-to-adult transition rate, followed by the adult fertility rate, etc.\nFinally, it’s also possible to simulate perturbations directly with the basic matrix, and we’d have this:\n\n# If we reduce adult survival by 30%, for example\nTF_a = 3\nTF_s = 1\nTP_a = 0.5\nTS_a = 0.9*0.3\nTP_s = 0.3\nTS_s = 0.4\nleslie&lt;-c(0,TF_a,TF_s,TP_a,TS_a,0,0,TP_s,TS_s) \nmatrice&lt;-matrix(data=leslie, nrow = 3,ncol = 3, byrow=TRUE, dimnames = ) \nlambda(matrice)\n\n[1] 1.425171\n\n\nIn this case, the lambda decreases by 0.36 etc…\n\nEpidemiology matrices\n\nA. Introduction to epidemiology with the SIRS model\nEpidemiology is the study of disease dynamics within a population. Let’s imagine that a sick meerkat arrives in a new, healthy colony after having had to leave its previous colony, which has been decimated by disease. The dynamics of the epidemic can be modeled using a basic epidemiological model, the SIRS model.\n\n\n\nThe SIRS model\n\n\nIn this model, susceptible individuals (S) become infected, and thus pass into the infectious class (I), then either leave the system by dying of the disease, or heal, and thus become cured/immune individuals (R). Finally, immunized individuals can lose their immunity and become susceptible again (S).\n\ntrans is the transmission rate, so trans.S.I/N represents the number of individuals infected per day.\nrec is the recovery rate, so rec.I represents the number of individuals recovering per day.\nmadd is the mortality rate, so madd.I represents the number of individuals dying from the disease per day.\nloss is the rate of immunity loss, so loss.R represents the number of individuals losing immunity per day.\n\nTo make it easier to relate the parameters to reality, it is useful to calculate the values of 1/rec and 1/loss, which are respectively the duration of the infectious period and the duration of immunity in days.\n\n#Initialization of parameters related to epidemic dynamics\n\n# Pathogen parameters  \ntrans = 0.2;\nrec = 1/10; # Infectious period lasts 10 days\nloss = 1/40; # immunity lasts 40 days\nmadd = 0.05; # 5% chance of dying per infected day\n\nFor the time being, the following system of differential equations can be solved to track the evolution of the compartments S, I and R over time. But we’re going to complexify the model to make it more realistic, and carry out discrete-time tracking using a matrix to store population evolution.\n\\[\\begin{aligned}& \\frac{d S}{d t}= - trans \\cdot S \\cdot \\frac{I}{N} + loss \\cdot R  \\\\\n& \\frac{d I}{d t}= - madd \\cdot I + trans \\cdot S \\cdot \\frac{I}{N} - rec \\cdot I \\\\\n& \\frac{d R}{d t}= rec \\cdot I - loss \\cdot R\n\\end{aligned}\\]\nB. Taking population dynamics and structure into account\nThis model takes no account of population dynamics or structure. To make it more realistic, we will therefore separate the population into two categories: juvenile (j) and adult (a), each with an associated mortality rate (m1 and m2). In addition, adult females (sex ratio : sr) can reproduce with a daily reproduction rate f and give birth to 7 young at a time (portee). It’s important to note that the young are always born healthy. The young grow to adulthood with a rate t, i.e. a duration of 1/t days corresponding to the time spent as a youngster. In order to avoid the case of exponential population growth, we take into account K the capacity of the environment, which will allow us to adjust the number of births in the model by decreasing it as the total population approaches K.\n\n# Initialization of population dynamics parameters\n\n# Population parameters\nK = 50;         \nsr = 0.5;       \nm1 = 0.0008;        \nm2 = 0.0003; #They live about 13 years      \nf = 0.006;  \nportee = 7; #   \nt1 = 1/365; # Females mature at one year old    \n\nThese population dynamics can also be expressed in the form of differential equations.\n\\[\\begin{aligned}& N = j + a \\\\\n& \\frac{d j}{d t}= -m_1 \\cdot j + sr \\cdot portee \\cdot f \\cdot a - t_1 \\cdot j  \\\\\n& \\frac{d a}{d t}= t_1 \\cdot j - m_2 \\cdot a \\\n\\end{aligned}\\]\nBy combining population dynamics and epidemiological dynamics, we end up with a much more complex model:\n\n\n\nThe whole model\n\n\nThis model can also be expressed as a system of differential equations:\n\\[\\begin{aligned}& \\frac{d S_j}{d t}=-S_j \\cdot(m_1+t_1+trans\\cdot \\frac{I}{N})+loss\\cdot R_j + sr\\cdot portee\\cdot N_a\\cdot f \\cdot (1- \\frac{N}{K}) \\\\\n& \\frac{d I_j}{d t}=-I_j \\cdot (m_1+madd+t_1+rec)+trans\\cdot \\frac{I}{N} \\cdot S_j  \\\\\n& \\frac{d R_j}{d t}= -R_j*(m_1+t_1+loss) + rec*I_j\n\\end{aligned}\\]\n\\[\\begin{aligned}& \\frac{d S_a}{d t}=S_j \\cdot t_1 - S_a \\cdot (m_2+trans \\cdot \\frac{I}{N}) + loss \\cdot R_a \\\\\n& \\frac{d I_a}{d t}=I_j \\cdot t_1 - I_a \\cdot (m_2+madd+rec)+trans \\cdot S_a \\cdot \\frac{I}{N}  \\\\\n& \\frac{d R_a}{d t}= R_j \\cdot t_1 - R_a \\cdot (m_2+loss) + rec \\cdot I_a\n\\end{aligned}\\]\nHowever, instead of trying to solve this system in continuous time, we’ll work in discrete time, using a matrix to track meerkat population numbers.\nC. Setting up the model in R\nThe first step is to create the matrix that will store the population numbers over time. We will therefore create a matrix in 3 dimensions: - The first dimension (rows) corresponds to the structure of the population into juveniles and adults, with an additional row for the total population. - The second dimension (columns) corresponds to the different states of the individuals (S, I and R). - The third dimension is the simulation time in days.\nOur matrix allows us to store in a single object, the state of the population in the form of a row/column matrix for each day of the simulation.\nMATRIX REMINDER: In R, objects created with matrix() are two-dimensional. To work with matrices of more than 2 dimensions, use array(), which allows you to create n-dimensional arrays in R.\nMatrix creation (MAT) in R.\n\n# Creation of a matrix of 0 with the right dimensions (3 rows, 3 columns, the number of days in the simulation)\n\ntemps = 4*365; #Simulation time = 4 years\n\nMAT &lt;- array(0, dim=c(3,3,temps)); \n\nAt the very beginning of the epidemic, we imagine a single infectious adult arriving in a colony entirely susceptible to the disease, made up of 19 adults and 10 juveniles.\n\n# starting conditions \nMAT[1,1,1] &lt;- 6;                  # number of healthy juveniles at initial conditions\nMAT[2,1,1] &lt;- 43;                 # number of healthy adults at initial conditions\nMAT[2,2,1] &lt;- 1;                  # number of sick adults at initial conditions\n\nFrom these initial conditions, we can calculate the total population by health status.\nMATRIX REMINDERS :\n\nAccess a part of a matrix that is not a complete row or column:\n\nTo access several consecutive rows, simply replace the row number with an indication of the form : starting_row:ending_row.\nHere’s an example of how to access only the first two rows of our matrix for the healthy state (first column) at t = 1 (initial conditions).\n\nMAT[1:2,1,1]\n\n[1]  6 43\n\n\nIt works in the same way for columns and the time dimension:\n\nMAT[1,1:2,1] #Healthy and infected juveniles at starting conditions\n\n[1] 6 0\n\nMAT[1,1,1:5] #Healthy juveniles during the first 5 days (for the moment, the numbers are 0 after t=1 as the simulation has not yet been run).\n\n[1] 6 0 0 0 0\n\n\n\nAccess all rows, columns or time\n\nTo access all the rows in the matrix, you could use the method described above, but there’s a better way: simply leave the row, column or time number blank. Here’s an example with the rows:\n\nMAT[1:3,1,1] # Method seen above for accessing lines 1 to 3 for healthy state at initial conditions\n\n[1]  6 43  0\n\nMAT[,1,1] # New method\n\n[1]  6 43  0\n\n\n\nComplexe example\n\n\nMAT[1:2,,1:2] # Visualization of juvenile and adult numbers for all health conditions over the first two days.\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    6    0    0\n[2,]   43    1    0\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n\n\nSumming a matrix\n\nSumming the values contained in all or part of a matrix is very straightforward. Simply use the sum() function, setting as argument the matrix or the part of the matrix whose coefficients you wish to sum.\nExample:\n\nsum(MAT[1:2,1,1])  # Sum of the number of healthy juveniles and adults at initial conditions \n\n[1] 49\n\n\nLet’s now apply this reminder to fill in the line corresponding to the total meerkat population by health status.\n\n# Population size by health status at initial conditions\n\nMAT[3,1,1] &lt;- sum(MAT[1:2,1,1]); #Total number of healthy meerkats \nMAT[3,2,1] &lt;- sum(MAT[1:2,2,1]); #Total number of infectious meerkats\nMAT[3,3,1] &lt;- sum(MAT[1:2,3,1]); #Total number of immunized meerkats\n\nNow that the initial conditions have been set, it’s time to simulate the model. To do this, we simply convert the differential equations obtained in part two into discrete-time equations, which give :\n\\[\\begin{aligned}& S_j[t+1]=S_j[t]-S_j[t] \\cdot(m_1+t_1+trans\\cdot \\frac{I[t]}{N[t]})+loss\\cdot R_j[t] + sr\\cdot portee\\cdot N_a[t]\\cdot f \\cdot (1- \\frac{N[t]}{K}) \\\\\n& I_j[t+1]=I_j[t]-I_j[t] \\cdot (m_1+madd+t_1+rec)+trans\\cdot \\frac{I[t]}{N[t]} \\cdot S_j[t]  \\\\\n& R_j[t+1]= R_j[t]-R_j[t]*(m_1+t_1+loss) + rec*I_j[t]\n\\end{aligned}\\]\n\\[\\begin{aligned}& S_a[t+1]= S_a[t] + S_j[t] \\cdot t_1 - S_a[t] \\cdot (m_2+trans \\cdot \\frac{I[t]}{N[t]}) + loss \\cdot R_a[t] \\\\\n& I_a[t+1]=I_a[t] + I_j[t] \\cdot t_1 - I_a[t] \\cdot (m_2+madd+rec)+trans \\cdot S_a[t] \\cdot \\frac{I[t]}{N[t]}  \\\\\n& R_a[t+1]=R_a[t]+ R_j[t] \\cdot t_1 - R_a[t] \\cdot (m_2+loss) + rec \\cdot I_a[t]\n\\end{aligned}\\]\nThen, starting from the initial conditions, we simulate each time step in a loop. The simulation time here is 2 years.\n\nNevo &lt;- rep(0,temps) # Vector initialization to store total population over time\nNevo[1] &lt;- sum(MAT[3,,1])\n\n# SIMULATIONS\nfor (t in 1:(temps-1)){ \n  # For the juveniles 0-1 year (see equations above)\n  # Note: births are positive, as newborns are in a healthy state.\n  N &lt;- sum(MAT[3,,t]); # size of total living population\n \n  MAT[1,1,t+1] &lt;- MAT[1,1,t]*(1-m1-t1-trans*MAT[3,2,t]/N) + loss*MAT[1,3,t] + max(0, sr*portee*(sum(MAT[2,,t])*f ) * (1 - N/K)); # Healthy juveniles the next day\n  MAT[1,2,t+1] &lt;- MAT[1,2,t]*(1-m1-madd-t1-rec) + trans*MAT[1,1,t]*MAT[3,2,t]/N; # Sick juveniles the next day\n  MAT[1,3,t+1] &lt;- MAT[1,3,t]*(1-m1-t1-loss) + rec*MAT[1,2,t]; # Immunized juveniles the next day\n\n  # For the adult category 1-13 years (see equations above)\n \n  MAT[2,1,t+1] &lt;- MAT[2,1,t]*(1-m2-trans*MAT[3,2,t]/N) + MAT[1,1,t]*t1 + loss*MAT[2,3,t]; \n  MAT[2,2,t+1] &lt;- MAT[2,2,t]*(1-m2-madd-rec) + MAT[1,2,t]*t1 + trans*MAT[2,1,t]*MAT[3,2,t]/N; \n  MAT[2,3,t+1] &lt;- MAT[2,3,t]*(1-m2-loss) + MAT[1,3,t]*t1 + rec*MAT[2,2,t];\n  \n  # Calculation of population numbers by state of health\n  MAT[3,1,t+1] &lt;- sum(MAT[1:2,1,t+1]);\n  MAT[3,2,t+1] &lt;- sum(MAT[1:2,2,t+1]);\n  MAT[3,3,t+1] &lt;- sum(MAT[1:2,3,t+1]);              \n \n  \n  Nevo[t+1] &lt;- sum(MAT[3,,t+1])\n  \n}# fin boucle temps\n\nD. Visualizing results\nNow that the simulation is done, it’s easy to visualize the evolution of meerkat numbers by health status over time.\nHere’s a code to do just that:\n\n#Conversion of total numbers by health status over time into dataframes\n\nEffectifs_sains&lt;-as.data.frame(MAT[3,1,])        # Healthy\nEffectifs_infectieux&lt;-as.data.frame(MAT[3,2,])   # Infectious\nEffectifs_immunises&lt;-as.data.frame(MAT[3,3,])    # Immunized\n\n# Putting it in the form of a single dataframe\nEffectifs&lt;-cbind(Effectifs_sains,Effectifs_infectieux,Effectifs_immunises)\n\n# Graphic\nlibrary(ggplot2)\nEffectifsplot &lt;- ggplot(Effectifs,aes(x=seq(1,temps,1))) +\n  geom_line(aes(y=Effectifs[,1]),color=\"blue\") +    \n  geom_line(aes(y=Effectifs[,2]),color=\"darkred\") + \n  geom_line(aes(y=Effectifs[,3]),color=\"darkgreen\" ) + \n  geom_line(aes(y=Nevo),color=\"black\") +\n  labs(title=\"Epidemic dynamics\",x=\"Time (Days)\",y=\"Number of S (blue), I (red), R (green) and N (black)\")\n\nEffectifsplot  \n\n\n\n\nAn endemic equilibrium is reached with a stable percentage of each health state over time. If you want to have fun with this model, you can change the parameters of the epidemic dynamics to observe other scenarios, such as the extinction of the population or the extinction of the disease.\n\nMarkov chain\nA. The Markov Chain\n\n\n\n\nChangement de colonie\n\n\nAfter this latest blow and epidemic, the survivors of our meerkat family are once again forced to look for a habitat and join a new colony. There are several options and colonies to choose from, each with a different probability of being reached depending on the difficulty of the journey and the resources it contains. In other words, the arrows linking the habitats represent the probability of reaching them. When the habitats are reached, a new arrow comes into play to determine the probability of being accepted into the colony; when this is the case, the meerkats settle permanently and stop moving. On the other hand, if they are not accepted in a colony, they can try their luck in a new one or try again to be accepted in a previous colony. This information is used to construct the diagram above.\nFor example, our poor meerkats are initially in position O and cannot stay there. However, they can move to a small nearby colony (colony A) with a probability of 0.9 of arriving and 0.6 of being accepted and settle permanently in the A1 burrow, or to a more distant colony (colony C) with a probability of 0.1 of arriving and 0.8 of being accepted and settle permanently in the C1 burrow.\nThe diagram above is what is known as a “Markov Chain” in which the various probabilities are numerically anotated. A Markov chain can be defined as a discrete-time or continuous-time stochastic process with a discrete state space, defined as being “memoryless” because it depends only on the present state and not on previous states.\nB. The transition Matrix \nThese different elements and this “Markov Chain” allow us to produce a rather special matrix T, which we call the transition matrix of a “Markov Chain”. This defines the transition probabilities from each state at time (t) vertical to each state at time (t+1) horizontal and is shown below:\n\\[\nT=\n\\left(\\begin{array}{cc}\nP(O|O) & P(A|O) & P(A1|O) & P(B|O) & P(B2|O) & P(C|O) & P(C3|O) & P(D|O) & P(D4|O)\\\\\nP(O|A) & P(A|A) & P(A1|A) & P(B|A) & P(B2|A) & P(C|A) & P(C3|A) & P(D|A) & P(D4|A)\\\\\nP(O|A1) & P(A|A1) & P(A1|A1) & P(B|A1) & P(B2|A1) & P(C|A1) & P(C3|A1) & P(D|A1) & P(D4|A1)\\\\\nP(O|B) & P(A|B) & P(A1|B) & P(B|B) & P(B2|B) & P(C|B) & P(C3|B) & P(D|B) & P(D4|B)\\\\\nP(O|B2) & P(A|B2) & P(A1|B2) & P(B|B2) & P(B2|B2) & P(C|B2) & P(C3|B2) & P(D|B2) & P(D4|B2)\\\\\nP(O|C) & P(A|C) & P(A1|C) & P(B|C) & P(B2|C) & P(C|C) & P(C3|C) & P(D|C) & P(D4|C)\\\\\nP(O|C3) & P(A|C3) & P(A1|C3) & P(B|C3) & P(B2|C3) & P(C|C3) & P(C3|C3) & P(D|C3) & P(D4|C3)\\\\\nP(O|D) & P(A|D) & P(A1|D) & P(B|D) & P(B2|D) & P(C|D) & P(C3|D) & P(D|D) & P(D4|D)\\\\\nP(O|D4) & P(A|D4) & P(A1|D4) & P(B|D4) & P(B2|D4) & P(C|D4) & P(C3|D4) & P(D|D4) & P(D4|D4)\\\\\n\\end{array}\\right)\n\\]\nBy replacing the previous algebraic matrix with the values from the Markov chain diagram, we obtain the following matrix T :\n\\[\nT=\n\\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\\\\n0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{array}\\right)\n\\]\nAs we can see, this matrix works in discrete time because of the transition from time t to time t+1 and the absence of a transition period between the different compartments. However, it could also work in continuous time with discrete state spaces, but for the sake of simplicity we will work in discrete time. For example, in the matrix above, we can read that the probability of moving from position O (the poor camping tent of our family of beloved meerkats) to colony A is 0.9, or 90%.\nC. Evolution of the system\nIf we take into account the fact that at time 0, the meerkats are in their makeshift tent, here is the initial condition:\n\\[\nT_0 =\\left(\\begin{array}{cc}\n1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\\\\n\\end{array}\\right)\n\\]\nAt the end of the first time step, we will have :\n\\[\nT_1 = \\left(\\begin{array}{cc}\n1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\\\\n\\end{array}\\right).\\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\\\\n0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{array}\\right) = T_0.T\n\\]\nSo :\n\\[\nT_1 =\\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0\\\\\n\\end{array}\\right)\n\\]\nIn other words, at the end of the first time step the meerkats will have a 90% probability of being in colony A and a 10% probability of being in colony C.\nAt the end of the second time step, we will have :\n\\[\nT_2 = \\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0\\\\\n\\end{array}\\right).\\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\\\\n0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{array}\\right)=T_1.T\n\\]\nSo :\n\\[\nT_2 =\\left(\\begin{array}{cc}\n0.0025 & 0 & 0.54 & 0.3625 & 0 & 0 & 0.08 & 0.015 & 0\\\\\n\\end{array}\\right)\n\\]\nAt the end of the second time step, the meerkat family will have a 0.25% probability of having returned to its initial camp (position 0), a 54% probability of having been accepted by colony A, a 36.25% probability of having ended up in colony B, an 8% probability of having been accepted in colony C and a 1.5% probability of having reached colony D with the largest quantities of resources.\nGenerally speaking, we can agree on the following matrix product:\n\\[\nT_t = \\left(\\begin{array}{cc}\n1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\\\\n\\end{array}\\right).\\left(\\begin{array}{cc}\n0 & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\\\\n0 & 0 & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0.1 & 0 & 0.7 & 0.2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0.025 & 0 & 0 & 0.025 & 0 & 0 & 0.8 & 0.15 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 & 0.9\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{array}\\right)^t=T_0.T^t\n\\]\nD. Eigenvalues and eigenvectors\nMarkov chain theory tells us that, after a certain time, the probability distribution will be independent of the initial distribution and we will see convergence towards different points in the chain.\nReminder: The properties of a matrix stipulate the presence of eigenvalues and eigenvectors. We say that λ is an eigenvalue of the matrix T if there is an eigenvector V such that T.V = λ.V. We then say that λ and V are an eigenpair of the matrix. The eigenvalues are obtained by solving the characteristic polynomial of the matrix T, which is calculated as follows:\n\\[\nP(λ) = det|T-λI|\n\\] In which I is the “Identity” matrix of the T matrix :\n\\[\nI =\n\\left(\\begin{array}{cc}\n1 & 0 & 0 & 0 & 0 & 0 & 0  & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n\\end{array}\\right)\n\\]\nand λ is an eigenvalue of the matrix T. P(λ) will therefore be the determinant of the matrix resulting from the difference between the matrix T and the product of λ and the identity matrix of T, i.e. :\n\\[\nP(λ)=det|\\left(\\begin{array}{cc}\n-λ & 0.9 & 0 & 0 & 0 & 0.1 & 0  & 0 & 0\\\\\n0 & -λ & 0.6 & 0.4 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1-λ & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0.1 & -λ & 0.7 & 0.2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & -λ & 0 & 0 & 0 & 0\\\\\n0.025 & 0 & 0 & 0.025 & 0 & -λ & 0.8 & 0.15 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1-λ & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0.1 & 0 & -λ & 0.9\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1-λ\\\\\n\\end{array}\\right)|\n\\]\nBy solving the equation P(λ) = 0, the solutions λ will be the different eigenvalues associated with the matrix T. Since the determinant of a 9*9 matrix is particularly long and difficult to calculate manually, we will determine the eigenvalues and their associated eigenvectors directly on R after creating the matrix and using the following commands:\n\nT&lt;-matrix(c(0,0.9,0,0,0,0.1,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0.1,0,0.7,0.2,0,0,0,0,0,0,0,1,0,0,0,0,0.025,0,0,0.025,0,0,0.8,0.15,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.1,0,0,0.9,0,0,0,0,0,0,0,0,1),ncol=9,nrow=9,byrow=TRUE)\n\nVP&lt;-eigen(T)\n\nVP\n\neigen() decomposition\n$values\n[1]  1.000000e+00+0.000000e+00i  1.000000e+00+0.000000e+00i\n[3]  1.000000e+00+0.000000e+00i  1.000000e+00+0.000000e+00i\n[5] -2.348247e-01+0.000000e+00i  2.348247e-01+0.000000e+00i\n[7]  0.000000e+00+1.806727e-01i  0.000000e+00-1.806727e-01i\n[9]  3.801265e-49+0.000000e+00i\n\n$vectors\n                [,1]           [,2]          [,3]           [,4]           [,5]\n [1,] 0.436719812+0i 0.199897485+0i 0.10721831+0i 0.017469986+0i -0.94832724+0i\n [2,] 0.483792565+0i 0.219993592+0i 0.04986898+0i 0.008125575+0i  0.22983190+0i\n [3,] 0.754292755+0i 0.000000000+0i 0.00000000+0i 0.000000000+0i  0.00000000+0i\n [4,] 0.078042281+0i 0.549983979+0i 0.12467246+0i 0.020313938+0i -0.13492550+0i\n [5,] 0.000000000+0i 0.780253535+0i 0.00000000+0i 0.000000000+0i  0.00000000+0i\n [6,] 0.013065028+0i 0.019032524+0i 0.62336228+0i 0.101569688+0i  0.15841918+0i\n [7,] 0.000000000+0i 0.000000000+0i 0.76026822+0i 0.000000000+0i  0.00000000+0i\n [8,] 0.001306503+0i 0.001903252+0i 0.06233623+0i 0.670833935+0i -0.06746275+0i\n [9,] 0.000000000+0i 0.000000000+0i 0.00000000+0i 0.734085518+0i  0.00000000+0i\n               [,6]                    [,7]                    [,8]\n [1,] 0.94832724+0i -0.96997979+0.00000000i -0.96997979+0.00000000i\n [2,] 0.22983190+0i  0.00000000-0.20396827i  0.00000000+0.20396827i\n [3,] 0.00000000+0i  0.00000000+0.00000000i  0.00000000+0.00000000i\n [4,] 0.13492550+0i  0.09212874+0.00000000i  0.09212874-0.00000000i\n [5,] 0.00000000+0i  0.00000000+0.00000000i  0.00000000+0.00000000i\n [6,] 0.15841918+0i  0.00000000+0.08322575i  0.00000000-0.08322575i\n [7,] 0.00000000+0i  0.00000000+0.00000000i  0.00000000+0.00000000i\n [8,] 0.06746275+0i  0.04606437+0.00000000i  0.04606437-0.00000000i\n [9,] 0.00000000+0i  0.00000000+0.00000000i  0.00000000+0.00000000i\n                  [,9]\n [1,] -9.863939e-01+0i\n [2,] -1.234413e-17+0i\n [3,]  0.000000e+00+0i\n [4,]  4.081261e-17+0i\n [5,]  0.000000e+00+0i\n [6,] -4.564977e-18+0i\n [7,]  0.000000e+00+0i\n [8,]  1.643990e-01+0i\n [9,]  0.000000e+00+0i\n\n\nThe “eigen” function therefore returns the eigenvalues and eigenvectors of the matrix T. Note that the eigenvalues of a matrix can also contain an imaginary part and be defined by complex numbers, but in this case we will only be interested in the ‘Real’ part, which can be obtained using the Re function.\nThe particularity of the eigenvalues of a transition matrix is that they are linked to the eigenvectors associated with the stationary distribution, corresponding to the limit probability of occupying each state in a Markov chain after a large number of iterations. When eigenvalues are equal to 1, their associated eigenvectors are the so-called “stationary distributions” indicating the probabilities of being in each state at equilibrium.\nThe eigenvalues of a transition matrix are related to the eigenvectors associated with the stationary distribution. The stationary distribution is the limiting probability of occupying each state in a Markov chain after a large number of iterations. The eigenvectors corresponding to the eigenvalues of 1 are the stationary distributions. They indicate the probabilities of being in each state at equilibrium.\nHowever, this property only applies if the dominant eigenvalue is the only one to have this value. In the particular case where it is not the only one, as is the case for the T matrix, we simply need to simulate a very large number of iterations to approximate the stationary distribution. Here is an example for 10000 iterations (remark, in R, T^t doesn’t work!) :\n\nT0&lt;-matrix(c(1,0,0,0,0,0,0,0,0),ncol=9,nrow=1)\n\nTn&lt;-T0%*%T\n\nfor (t in 1:10000){\n  Tn&lt;-Tn%*%T\n}\n\nThis result tells us that 58% of the time the meerkat family will end up settling in burrow A1, 26% of the time in burrow B2, 14% of the time in burrow C3 and 2% of the time in burrow D4.\nV. Bibliography\nN. Bacaër, Histoire de mathématiques et de populations, Cassini, Paris, (2008)\nJ. Bair, La matrice de Leslie et la dynamique des populations, Tangente Hors-série no 42, Mathématiques et biologie, éditions Pole, Paris, Pages 90-94, (2012).\nJ. Bair, J. Mawhin, Modèles de type proie – prédateur, Tangente Hors-série no°42, Mathématiques et biologie, éditions Pole, Paris, Pages 56-59, (2012)\nHoffman, W.A., Fire and population dynamics of woody plants in a neotropical savanna : matrix model projections, Ecology, Volume 80, Pages 1354-1369, (1999), https://doi.org/10.1890/0012-9658(1999)080[1354:FAPDOW]2.0.CO;2\nAbadi, F., Gimenez, O., Ullrich, B., Arlettaz, R. and Schaub, M., Estimation of immigration rate using integrated population models, Journal of Applied Ecology, Volume 47, Pages 393-400, (2010), https://doi.org/10.1111/j.1365-2664.2010.01789.x\nSchaub, M., Abadi, F., Integrated population models: a novel analysis framework for deeper insights into population dynamics, J Ornithol, Volume 152 (Suppl 1), Pages 227–237, (2011), https://doi.org/10.1007/s10336-010-0632-7\nFrisman, E.Y., Zhdanova, O.L., Kulakov, M.P. et al., Mathematical Modeling of Population Dynamics Based on Recurrent Equations: Results and Prospects, Part I. Biol Bull Russ Acad Sci, Volume 48, Pages 1–15, (2021), https://doi.org/10.1134/S1062359021010064\nCharles S., Les modèles matriciels de type Leslie : Une approche couplée Mathématiques et Statistiques, (2004)\nSmith D. and Keyfitz N., Mathematical Demography, Springer Science & Business Media, (2012)\nHoppensteadt F.C., Mathematical Methods pf Population Biology, Cambridge University Press, (1982)\nRéserve de Sigean, Animaux : Suricates, (2023)\nGirel S., Chapitre 1 : Modèles matriciels de population, (2020)"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "How to contribute",
    "section": "",
    "text": "Create a specific branch for your chapter named an_explicit_name_for_the_branch and switch on this branch\n\ngit checkout -b an_explicit_name_for_the_branch\nIn the development of the project, you might find necessary to have several branches per chapter.\n\nCreate a quarto file for the chapter chapter0.qmd in your branch\n\ngit add chapter0.qmd\ngit commit -m \"first commit in the chapter branch\"\n\nPush everything on the remote repo on Github\n\nFor the first push after the branch creation, you have to specify the name of the branch on the remote repo and you can use git  push --set-upstream origin an_explicit_name_for_the_branch or\nFor the push to come after the first one, you will simply push by\ngit  push\nOnce you are quite happy with your production, you will be willing to integrate your production on the main branch. A good practice is to ask the permission to push on the main branch, which is named a pull request (PR).\n\nAsk for a PR on Github\nIf needed, specify in the PR message the package you need and mention MarieEtienne as reviewer of the PR.\n\nA mock rendering of the qmd file will start when you request a PR using the Github Action mechanism (called runner on gitlab). If the action passes (green signal), you can go to the next step. If not, you will have to fix the issue (again you can ask assistance if you don’t understand the error).\n\nOnce the PR is checked, mention one of your colleage as reviewer."
  },
  {
    "objectID": "instructions.html#one-chapter-corresponds-at-least-to-one-branch",
    "href": "instructions.html#one-chapter-corresponds-at-least-to-one-branch",
    "title": "How to contribute",
    "section": "",
    "text": "Create a specific branch for your chapter named an_explicit_name_for_the_branch and switch on this branch\n\ngit checkout -b an_explicit_name_for_the_branch\nIn the development of the project, you might find necessary to have several branches per chapter.\n\nCreate a quarto file for the chapter chapter0.qmd in your branch\n\ngit add chapter0.qmd\ngit commit -m \"first commit in the chapter branch\"\n\nPush everything on the remote repo on Github\n\nFor the first push after the branch creation, you have to specify the name of the branch on the remote repo and you can use git  push --set-upstream origin an_explicit_name_for_the_branch or\nFor the push to come after the first one, you will simply push by\ngit  push\nOnce you are quite happy with your production, you will be willing to integrate your production on the main branch. A good practice is to ask the permission to push on the main branch, which is named a pull request (PR).\n\nAsk for a PR on Github\nIf needed, specify in the PR message the package you need and mention MarieEtienne as reviewer of the PR.\n\nA mock rendering of the qmd file will start when you request a PR using the Github Action mechanism (called runner on gitlab). If the action passes (green signal), you can go to the next step. If not, you will have to fix the issue (again you can ask assistance if you don’t understand the error).\n\nOnce the PR is checked, mention one of your colleage as reviewer."
  },
  {
    "objectID": "instructions.html#as-a-reviewer",
    "href": "instructions.html#as-a-reviewer",
    "title": "How to contribute",
    "section": "As a reviewer",
    "text": "As a reviewer\nYour role is essential as you are responsible for the quality of the submission you were assigned to. Read carefully the production and ask for correction/clarification if needed. One you are happy with the correction you can accept the PR."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The book we dreamt we had",
    "section": "",
    "text": "This website is produced by the M2 MODE student uring year 2023-2024 as part of their assignment in the Online Collaborative Ressources course.\nAfter a short introduction regarding the principles and the tools of a reproducible science, they have been collaborating to write a M1 companion book for the next student generation.\nTable of Contents\n\nMultivariate analysis\nLinear modelling in ecology\nMixed models in Ecology\nDependent Data\nMatrix models and applications in ecology\nOrdinary Differential Equations and their application in Ecology\nResampling methods\nBayesian models"
  },
  {
    "objectID": "Chap_ODE.html",
    "href": "Chap_ODE.html",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "",
    "text": "Welcome to you, young Master 1 student!\nIt’s now been a few chapters since you plunged into this adventure full of numbers, formulas and other mathematical oddities (🤢). At the moment, you’re probably feeling a strong need for a change of orientation. I mean, we’re doing ecology, not maths! And raising goats in Auvergne sounds pretty cool…kinds of…🐐\nBut don’t give up! The first (really interesting 😜) chapter is right there in front of you!\nAre you interested in population and epidemiological trends and dynamics? Would you like to be able to predict the future (and the past, no kidding) from the present?\nThen this is the chapter for you! Work it as much as you can! You’ll end up shining in modelling (and maybe even in society).\nAt the end, you’ll:\n- know how to define an ordinary differential equation\n- be able to detect a problem involving the need to use differential equations\n- be able to carry out modelling involving an ODE system in an ecological context"
  },
  {
    "objectID": "Chap_ODE.html#ii-a.-the-exponential-model-or-malthus-model",
    "href": "Chap_ODE.html#ii-a.-the-exponential-model-or-malthus-model",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "II-A. The exponential model or Malthus model",
    "text": "II-A. The exponential model or Malthus model\nThe biological hypothesis behind this first model is as follows: The population grows in proportion to its present size.\nLet r be defined as follows: \\[r \\in \\mathbb{N}^+\\]\nThis means that a population of N individuals at time t will gain r*N individuals at the next time.\nIt could be illustrated as follows (Figure 3):\n\n\n\n\n\n\n\n\n\nWe’re going to define a discrete and a continuous model here to make things very clear!\n\nII-A-1. Discrete model\nLet\n\\[n \\in \\mathbb{N}^+\\]\nAs mentioned above, the population grows by r*N(n) per unit time.\nIn other words, between N(n+1) and N(n), the population increases/decreases by r*N(n).\nWe therefore have: \\[N(n+1)-N(n) = r*N(n)\\]\nThis gives the following model: \\[N(n+1) = r*N(n)+N(n)\\]\nWhich gives us the following solution: \\[N(n) = (1+r)^n * N(0)\\]\nWe’re going to graphically represent this model using r studio. Here are the lines of code to achieve this (Figure 4):\n\nr = 0.7 # r corresponds to the population growth rate; it ranges from -1 to 1; here, it's positive, so the population is growing.\nN0 = 10 # N0 corresponds to the initial number of individuals. There are 10 here.\nt = seq(0,10,1) # We simply define the duration of the simulation. Here, we start from time 0, up to time 10, with a step of 1.\npop = c() # We create a list to store the values of N(t)\nfor(n in t){\n  N = ((1+r)^n) * N0 # Model solution\n  pop = append(pop,N) # Storing results in the 'pop' list\n}\n\nplot(t, pop, main = \"\",\n     xlab = \"t\",\n     ylab = \"N(t)\") # Display it all in a beautiful graph (feel free to make prettier ones using ggplot2 ^^) \n\ntitle(main = \"Exponential evolution of population as a function of discrete time\", adj=0.75)\ntitle(sub = \"Figure 4\")\n\n\n\n\n\n\n\n\n\n\nII-A-2. Continuous model\nLet \\[t \\in \\mathbb{R}^+\\]\nWe had: \\[N(n+1)-N(n) = r*N(n)\\]\nWhich gives:\n\\[\\begin{aligned}[t]\n        & \\phantom{=}\\left(N(n+1)-N(n) = r*N(n)\\right)\\\\\n        & \\Leftrightarrow N(t+\\Delta t)-N(t) = r * \\Delta t * N(t)\\\\\n        & \\Leftrightarrow \\frac{N(t+\\Delta t)-N(t)}{\\Delta t} = r * N(t) \\\\\n        \\end{aligned}\\]\nThis gives us: \\[\\lim_{\\Delta t \\to 0} \\frac{N(t+\\Delta t)-N(t)}{\\Delta t} = \\frac{dN(t)}{dt} = N'(t) = r*N(t)\\]\nThis gives the following model: \\[\\frac{dN(t)}{dt} = r * N(t)\\]\nThe solution of this model is: \\[N(t) = \\exp(r * t) * N_0\\]\nWe’re going to try to simulate this model using r studio. We’re going to use a package you’ve seen or will no doubt be seeing very soon: the ‘deSolve’ package.\nInstall and load it:\n\n#install.packages(\"deSolve\")\n#library(deSolve)\n\nThere are two important points to bear in mind when using this package:\n\nThe main function you’ll be using is the ‘ode()’ function. It follows a very specific syntax that you need to remember:\n\node(y=y0, times=time, func=function, parms=parameters)\nwith:\n\n‘y0’ = initial value of the variable of interest y (for the first time value)\n‘times’ = the times you wish to work on\n‘function’ = the function defining the system; this function follows a very specific syntax that you’ll also need to remember (see below).\n‘parms’ = parameter values\nconcerning the ‘function’ argument of the ‘ode()’ function. This corresponds to a function which, although specific to each model, follows a very particular syntax, as explained above. The syntax is as follows:\n\nname_of_function&lt;- function(t,y,parameters){ instructions list(flux)} with:\n\n‘t’ = corresponds to the ‘times’ argument of the ‘ode()’ function\n\n‘y’ = corresponds to the variable of interest\n\n‘parameters’ = corresponds to the parameters of the ‘parms’ argument of the ‘ode()’ function\n\nKnowing all this, we can simulate the exponential model with the following script (Figure 5):\n\nexpo &lt;- function(t, N, r){\n  list(r*N)}\nN0 = 10\nr = 0.7\ntemps &lt;- seq(from = 0, to = 10, by = 0.01)\nsol &lt;- ode(y = N0, times = temps, func = expo, parms = r)\nplot(sol,main = \"\")\ntitle(main = \"Exponential evolution of population as a function of continuous time\", adj=0.65)\ntitle(sub = \"Figure 5\")\n\n\n\n\n\n\n\n\n\n\nII-A-2. Summary\nWe can see that, whether using the discrete or continuous model, the population is growing. And it’s growing exponentially!\nTry changing the parameters to vary the trajectories! Try, for example, r = 0; r &lt; 0; N0 = 0, etc. This will enable you to fully understand the model!"
  },
  {
    "objectID": "Chap_ODE.html#ii-b.-the-logistic-model-or-verhulst-model",
    "href": "Chap_ODE.html#ii-b.-the-logistic-model-or-verhulst-model",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "II-B. The logistic model or Verhulst model",
    "text": "II-B. The logistic model or Verhulst model\nThe biological assumptions behind this second model are:\n\nThe population grows in proportion to its present size.\nA density-dependency factor (noted K) is introduced to take account of the limiting effect of resources (which are not infinite): this is intraspecific competition!\n\nLet defined: \\[r \\in \\mathbb{N}^+ ; K \\in \\mathbb{R}^+ ; t \\in \\mathbb{R}^+  \\]\nThis means that a population of N individuals at time t will gain r*(1-(N/K))*N individuals at the next time.\nThis gives the following model: \\[\\frac{dN(t)}{dt} = r * (1-\\frac{N(t)}{K}) * N(t)\\]\nThe solution of this model is:\n\\[N(t) = \\frac{K}{1 + \\frac{K - N_0}{N_0}\\exp(-rt)}\\]\nUsing the explanations about ‘deSolve’, you can illustrate this as follows (Figure 6):\n\nlogi &lt;- function(t, N, param){\n  r = param[1]\n  K = param[2]\n  list(r*N*(1-N/K))}\nN0 = 100\nr = 0.7\nK = 100\ntemps &lt;- seq(from = 0, to = 10, by = 0.01)\nsol &lt;- ode(y = N0, times = temps, func = logi, parms = c(r, K))\nplot(sol, main = \"\",\n     xlim = c(0,10),\n     ylim = c(0,200),\n     col = 'green')\ntitle(main = \"Logistic evolution of population as a function of continuous time\", adj=0.65)\ntitle(sub = \"Figure 6\")\nN0 = 200\nsol &lt;- ode(y = N0, times = temps, func = logi, parms = c(r,K))\nlines(sol, col = 'red')\nN0 = 2\nsol &lt;- ode(y = N0, times = temps, func = logi, parms = c(r,K))\nlines(sol, col = 'blue')\n\n\n\n\n\n\n\n\nThere are a number of different situations here, which can be summarized as follows:\n\nIf K &gt; N0 &gt; 0, then ∀t &gt; 0, N(t) tends towards K (increase; blue trajectory on Figure 6)\nIf N0 = K, then for ∀t &gt;0, N(t) = N0 (green trajectory on Figure 6)\nIf N0 &gt; K, then for ∀t &gt;0, N(t) tends towards K (decrease; red trajectory on Figure 6)\n\nOnce again, try changing the parameters to vary the trajectories! This will enable you to fully understand the model!"
  },
  {
    "objectID": "Chap_ODE.html#lotka-volterra",
    "href": "Chap_ODE.html#lotka-volterra",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "Lotka-Volterra",
    "text": "Lotka-Volterra\nA LV prey-predator model represents both equations over time. Although we will mostly focus on the cyclic patterns between those equations, keep in mind that there can be a certain point in time where both equations are in equilibrium and the values that are depending on the time remain approximately constant.\nWhy LV is useful: - To see that the interactions between populations can create periodic dynamics over time;\n- To study how biodiversity interacts with the parameters put into the model, such as the interaction coefficients between populations.\nAssumptions for LV:\n- The prey and predator live in a limited space;\n- The encounters are random;\n- The prey is only limited by the predator;\n- A predator can consume an infinite number of preys\nBiological intuition:\nIn this model, the predator population thrives when prey are plentiful, but eventually exhaust their resources, and the population declines. When the predator population has declined sufficiently, the prey can reproduce more, and the population increases again. This dynamic continues in a cycle of growth and decline.\nTypically, here’s what these equations can look like:\n\\[ PP = \\begin{cases} \\frac{d_x}{d_t}= a x - b x y &\\text{Prey} \\\\ \\frac{d_y}{d_t}= -c y + h x y &\\text{Predator} \\end{cases}\\] ****\nIt’s important to really understand how equations are constructed and what they do before getting started.\nDon’t be afraid by the looks of all those letters, we’ll gonna break it down to you quite simply:\nThe main distinction to make in those equation is with “x” and “y”: make sure to recognize that “x” is referring to the prey population (as written in the \\[ \\frac{d_x}{d_t} \\] at the beginning of the line) and that “y” is referring to the predator population (as written in the \\[ \\frac{d_y}{d_t} \\] ). ax: Growth term for the prey population: constant a * current prey population. bxy: Death term for the prey population. Depends on the size of the prey population and the size of the predator’s population. The death of the prey population depends partly on the predator eating them. It’s the same logic the other way around for the second equation because it includes the growth rate of the predator population with cy and the death rate of the predator population with hxy.\nIn other terms: The prey grows at a linear rate (a) and gets eaten by the predator at the rate of (b). Indeed, the rate of predation on prey is assumed to be proportional to the frequency of encounters between predators and prey; it is equal to bx(t)y(t).\nThe predator gains a certain amount of vitality by eating the prey at a rate (c), while dying off at another rate (d).\nMore precisely, x(t) and y(t) functions follow a malthusian model:\nThe prey population follows an exponential growth in the absence of predators. This growth is represented in the equation with the term ax(t).\nThe term cy(t) is the natural death of the predators, it is an exponential decrease of the population, in absence of preys.\n\nDemonstration\nLet’s dive into R, to simulate and plot the evolution of the two populations, using the Lotka-Volterra prey-predator model.\nTo simulate this population dynamic, we will use given coefficients, initial levels of populations and also a given simulation time and given time step size. Indeed, We will use the so-called Euler method, as we will be moving through time, and incrementing new values of population levels at each time steps, to keep track of the evolution of the two populations.\nWhat you will have to keep in mind: In this Euler method, we will think of dt as \\[\\Delta t\\]\nThus, we can make this approximation: \\[ \\frac{d_x}{d_t} = ax - bxy \\\\ =&gt; dx = (ax - byx) * \\Delta t \\\\ and \\\\ \\frac{d_y}{d_t} = -cy + hxy \\\\ =&gt; dy = (-cy + hxy) * \\Delta t \\]\n\nDefining the terms\nDisclaimer: terms’ values are not intended to be ecologically relevant, but to serve as an example.\nTo make it more telling, let’s say that our prey population is a zebra population, whereas the predator population are a group of lions. Obviously, we would think that the zebra population starts at a higher population size than a simple group of lions. That’s what we tell to our model:\n\nrm(list=ls()) #to clear all the environment\n\nX_init&lt;- 5     #starting value for x\nY_init&lt;- 3     #starting value for y\n\nThe initial values for the populations’ sizes have been defined. Now, since LV is a dynamic model, evolving over time, and since we do not want our simulation to run endlessly, it’d be a good idea to set a time limit, wouldn’t it be? Let’s say we want to follow the population dynamics throughout 100 months.\nFor our example, we will talk about numbers of individuals (Xinit=5 individuals, Yinit=3 individuals), and we will consider the time unit to be months. But we could have chosen days or years, and then chose consistent coefficient values:\n\ntend&lt;-100    #how long the simulation is going to run for\ndelta_t&lt;-0.01    #the size of the time step, as we will be moving through time with discrete steps\n\n# Model coefficients\na&lt;-0.8 #prey\nb&lt;-0.4 #prey\nc&lt;-0.6 #predator\nh&lt;-0.2 #predator\n\n\n\nDefining the vectors\nWe create some vectors to keep track of the variables, as we are moving through time. Then those vectors will prove useful when we will want to plot the dynamics:\n\nX&lt;-c(X_init)    #we will append to this vector, it is going to be filled up with the successive levels of X, during all the time of the simulation\nY&lt;-c(Y_init)    #idem with Y\n\nt&lt;-c(0)   #this vector will be filled up with all the time steps\n\n\n\nSimulation\n\nwhile (t[length(t)]&lt;tend) \n{\n  current_X&lt;-X[length(X)]   #we define current_X as the most recent value of X\n  current_Y&lt;-Y[length(Y)]    #idem for Y\n  current_t&lt;-t[length(t)]\n  \n  delta_X&lt;-(a*current_X-b*current_X*current_Y)*delta_t\n  next_X&lt;-current_X+delta_X\n  \n  delta_Y&lt;-(-c*current_Y + h*current_X*current_Y)*delta_t\n  next_Y&lt;-current_Y+delta_Y\n  \n  next_t&lt;-current_t+delta_t\n  \n  #We append the successive values to the vectors\n  X&lt;-append(X,next_X)\n  Y&lt;-append(Y,next_Y)\n  t&lt;-append(t,next_t)\n\n}\n\n\n\nPlotting\n\nlibrary(ggplot2)\n\nLV &lt;- data.frame(time = t, prey = X, predator = Y)\n\nggplot(LV, aes(x = time)) +\n  geom_line(aes(y = prey, color = \"Prey\"), size = 1.5) +\n  geom_line(aes(y = predator, color = \"Predator\"), size = 1.5) +\n  labs(x = \"Time (months)\", y = \"Number of Individuals\") +\n  scale_color_manual(values = c(Prey = \"blue\", Predator = \"red\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\nYou can see the cyclic nature of this dynamic as explained earlier: the prey thrives when the predator declines, the predator thrives when there’s a lot of prey…\nThis dynamic can be seen in a phase space plot i.e. the level of predator as a function of the prey level:\n\nggplot(LV, aes(x = prey, y = predator)) +\n  geom_point() +\n  labs(x = \"Prey (nb of individuals)\", y = \"Predator (nb of individuals)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nAdding a parameter to make in more realistic\nAt that point, the model doesn’t contain a lot of terms. It is a great mathematical setting, we can understand how it works and what is its point. However, in nature, there are a lot more parameters controlling our populations. Adding some of these parameters would allow the model to be more realistic.\nTo cite only one parameter, we can talk about the carrying capacity of the environment. It defines how the environment can tolerate the species, according to the resources it contains. The more individuals from a species there is, the less the environment can keep up, until a certain point where there is too much individuals for a limited resource so the population declines…\nActually, this notion of carrying capacity (K) can bring us to an unaddressed point: reaching an equilibrium for both prey and predator populations, instead of endlessly going in circles.\nThe equations become: \\[ PP = \\begin{cases} \\frac{d_x}{d_t}= a x \\ (1 - \\frac{x}{K}) - b x y &\\text{Prey} \\\\ \\frac{d_y}{d_t}= -c y + h x y &\\text{Predator} \\end{cases}\\]\n\n\nExercises to understand:\n\nSupposing that you want to depict the possible seasonal variation of the coefficient “a”, rewrite the code to simulate a model that includes the variability of “a” among time. We can consider a variation of “a” over a time period of one year, with an amplitude of 0.6. Moreover, we consider a mean value of “a” of 0.8.\n\nAnswer:\n\nrm(list=ls())\n\n####DEFINING THE COEFFICIENTS AND VARIABLES####\nX_init&lt;-5\nY_init&lt;-3\ntend&lt;-100\ndelta_t&lt;-0.01\n\n\n####DEFINING THE VECTORS####\nX&lt;-c(X_init)   \nY&lt;-c(Y_init)\n\nt&lt;-c(0)   #this vector will be filled up with all the time steps\n\n###coefficients of the model\n#function to describe the variation of a among time\na_seasonal_variation &lt;- function(t) {\n  amplitude &lt;- 0.6  #amplitude of the seaonal variation\n  period &lt;- 12      #seasonal period\n  mean &lt;- 0.8     #mean value of the growth rate\n  return(mean + amplitude * sin(2 * pi * t / period))\n}\n\nb&lt;-0.4\nc&lt;-0.6\nh&lt;-0.2\n\n####SIMULATION####\n\nwhile (t[length(t)]&lt;tend) \n{\n  current_X&lt;-X[length(X)]   #we define current_X as the most recent value of X\n  current_Y&lt;-Y[length(Y)]    #idem for Y\n  current_t&lt;-t[length(t)]\n  \n  delta_X&lt;-(a_seasonal_variation(current_t)*current_X-b*current_X*current_Y)*delta_t\n  next_X&lt;-current_X+delta_X\n  \n  delta_Y&lt;-(-c*current_Y + h*current_X*current_Y)*delta_t\n  next_Y&lt;-current_Y+delta_Y\n  \n  next_t&lt;-current_t+delta_t\n  \n  #We append the successive values to the vectors\n  X&lt;-append(X,next_X)\n  Y&lt;-append(Y,next_Y)\n  t&lt;-append(t,next_t)\n  \n}\n\n####PLOTTING#####\n\nlibrary(ggplot2)\n\nLV_exercice &lt;- data.frame(time = t, prey = X, predator = Y)\n\nggplot(LV_exercice, aes(x = time)) +\n  geom_line(aes(y = prey, color = \"Prey\"), size = 1.5) +\n  geom_line(aes(y = predator, color = \"Predator\"), size = 1.5) +\n  labs(x = \"Time (months)\", y = \"Number of Individuals\") +\n  scale_color_manual(values = c(Prey = \"blue\", Predator = \"red\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nWhat factors, other than the carrying capacity of the environment, could be added that would have ecological significance?\n\nAnswer:\n\nsǝʇɐɹ uoᴉʇɔnpoɹdǝɹ ɟo ʎʇᴉlɐuosɐǝS\nǝʇɐɹ uoᴉʇɐpǝɹd ɟo ʎʇᴉlɐuosɐǝS\nuoᴉʇɐɹɓᴉꟽ\nuoᴉʇᴉʇǝdɯoɔ ɔᴉɟᴉɔǝdsɐɹʇuI\n.ɔʇǝ\n\nYou can use a online generator to flip the text back up, or just take a picture and turn it!"
  },
  {
    "objectID": "Chap_ODE.html#sir-model-and-resolution-by-hand",
    "href": "Chap_ODE.html#sir-model-and-resolution-by-hand",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "1) SIR model and resolution by hand",
    "text": "1) SIR model and resolution by hand\nThe most used models in epidemiology are mathematical models of infectious diseases such as the SIR model. These models are based on compartmentalization that divide the population into various possible disease states. The SIR model is composed of 3 compartments: initially healthy or susceptible individuals (S), infected individuals (I) and cured individuals or resistent (R).\n\\[SIR=\\begin{cases}\\frac{d_S}{d_t}=- \\alpha SI+\\gamma R &\\text{Susceptible} \\\\ \\frac{d_I}{d_t}=\\alpha SI-\\beta I&\\text{Infected} \\\\ \\frac{d_R}{d_t}=\\beta I- \\gamma R &\\text{Resistent}\\end{cases}\n\\\\\n\\\\ \\begin{align} With : & - \\alpha \\text{: disease transmission rate}\n\\\\ & - \\beta \\text{: cure rate}\n\\\\ & - \\gamma \\text{: coefficient of loss of immunity} \\end{align}\\]\nWe assume a closed population, so: \\[ S+I+R = 1 \\Leftrightarrow R=1-S-I \\]\nThus, we can express R from S and I, so we can remove the equation from our system and we end up with a system with 2 equations.\n\nBalance points\nWe look for balance points such as:\n\\[ SIR=\\begin{cases}\\frac{d_S}{d_t}=0 &\\text{Susceptible} \\\\ \\frac{d_I}{d_t}=0&\\text{Infected} \\end{cases} \\]\nTo do this, we solve:\n\\[ \\begin{align} & \\begin{cases} - \\alpha SI+\\gamma R = 0 &\\text{Susceptible} \\\\ \\alpha SI-\\beta I =0 &\\text{Infected} \\end{cases}\n\\\\ \\Leftrightarrow & \\begin{cases} -\\alpha SI+\\gamma (1-S-I) = 0 \\\\ \\alpha SI-\\beta I =0 \\end{cases} \\end{align} \\]\nWe first focus on solving:\n\\[\\begin{align} & \\alpha SI - \\beta I=0\n\\\\ \\Leftrightarrow \\ & I(\\alpha S- \\beta) =0 \\\\ \\Leftrightarrow \\ & I=0\\ or\\ S= \\frac{\\beta}{\\alpha} \\end{align}\\]\nIf I = 0, we integrate I=0 into our “susceptible” equation to obtain the solution for S:\n\\[\\begin{align} & \\ \\ \\ \\ \\ \\begin{cases} -\\alpha S *0 + \\gamma (1-S-0)=0 \\\\ I=0 \\end{cases} \\\\ & \\Leftrightarrow \\begin{cases} \\gamma (1-S)=0 \\\\ I=O \\end{cases} \\\\ &\\text{We thus obtain our first solution:} \\\\ &\\Leftrightarrow \\begin{cases} S=1 \\\\ I=0 \\end{cases} \\end{align} \\]\nIt’s the disease-free equilibrium (DFE).\nIf S = b/a, we integrate S into our “susceptible” equation to obtain the solution for I:\n\\[\\begin{align} & \\ \\ \\ \\ \\  \\begin{cases} - \\alpha \\frac{\\beta}{\\alpha} I + \\gamma (1- \\frac{\\beta}{\\alpha} - I) =0\\\\\nS = \\frac{\\beta}{\\alpha} \\end{cases}\\\\\n& \\Leftrightarrow \\begin{cases} -\\beta I + \\gamma- \\frac{\\beta \\gamma}{\\alpha} -\\gamma I = 0 \\\\ S = \\frac{\\beta}{\\alpha} \\end{cases} \\\\\n& \\Leftrightarrow \\begin{cases} \\beta I + \\gamma I = \\gamma (1 - \\frac{\\beta}{\\alpha}) \\\\ S= \\frac{\\beta}{\\alpha} \\end{cases}\\\\\n& \\text{We thus obtain our first solution:}\\\\\n& \\Leftrightarrow \\begin{cases} I= \\frac{\\gamma (1- \\frac{\\beta}{\\alpha})}{\\beta + \\gamma} \\\\\nS=\\frac{\\beta}{\\alpha} \\end{cases} \\end{align} \\]\nIt’s the endemic equilibrium (EE).\n\n\nJacobian matrix\nNow, let’s perform linear stability analysis for each equilibrium point by computing the Jacobian matrix:\n\\[ J=\\begin{bmatrix} \\frac{\\delta S}{\\delta S} & \\frac{\\delta S}{\\delta I} \\\\ \\frac{\\delta I}{\\delta S} & \\frac{\\delta I}{\\delta I}  \\end{bmatrix} \\Leftrightarrow \\begin{bmatrix} - \\alpha I- \\gamma & - \\alpha S- \\gamma \\\\ \\alpha I & \\alpha S- \\beta \\end{bmatrix} \\]\nAnalyze the stability of each equilibrium point by examining the signs of the real parts of the eigenvalues. If the real parts are all negative, the equilibrium is stable; if any are positive, the equilibrium is unstable. The stability will depend on the specific values of the parameters α and β.\n\n\nFor the disease-free equilibrium:\n\\[ J(1,0)= \\begin{bmatrix} -\\gamma & - \\alpha -\\gamma \\\\ 0 & \\alpha - \\beta \\end{bmatrix} \\]\nThe Jacobian matrix for the pair (1,0) is an upper triangular matrix, so the eigenvalues are the diagonal elements: \\[ \\lambda _1 = - \\gamma \\ \\text{and} \\ \\lambda _2 = \\alpha - \\beta \\]\nγ is a coefficient representing loss of immunity, so γ&gt;0.\nSo: \\[ \\begin{align} & \\lambda _1 &lt;0\n\\\\ & \\text{If:} \\ - \\alpha &gt; \\beta , \\ \\lambda _2 &gt; 0 \\ \\text{Unstable equilibrium point}\n\\\\& \\ \\ \\ \\  - \\alpha &lt; \\beta , \\ \\lambda _2 &lt; 0 \\ \\text{Asymptotically stable point}  \\end{align} \\]\nIn order to better understand the behavior of the unstable point, we look for the sign of the determinant of the matrix:\n\\[ det(J(1,0))= - \\gamma (\\alpha - \\beta ) \\\\\n\\Leftrightarrow \\ \\gamma (\\beta - \\alpha)\n\\\\ \\gamma &gt; 0 \\ and \\ \\alpha &gt; \\beta\n\\\\ det(J(1,0)) &lt; 0 \\Rightarrow \\text{Saddle point} \\]\nSaddle points exhibit a combination of stable and unstable behavior in different directions in the state space.\n\n\nFor the endemic equilibrium\n\\[ J(\\frac{\\beta}{\\alpha},\\frac{\\gamma (1-\\frac{\\beta}{\\alpha})}{\\beta + \\gamma})= \\begin{bmatrix} \\frac{\\gamma (\\beta- \\alpha)}{\\beta + \\gamma} - \\gamma & - \\beta -\\gamma \\\\ \\frac{\\gamma (\\alpha - \\beta)}{\\beta + \\gamma} & 0 \\end{bmatrix} \\]\nHere, we do not have a characteristic matrix so to determine the eigenvalues, it will be necessary to solve the characteristic polynomial. We will first look at the sign of the determinant of the matrix to study the behavior of the system.\nSo, we calculate the determinant: \\[\\begin{align} & det(J(\\frac{\\beta}{\\alpha},\\frac{\\gamma (1-\\frac{\\beta}{\\alpha})}{\\beta + \\gamma})) = - (- \\beta - \\gamma) * \\frac{\\gamma (\\alpha - \\beta)}{\\beta + \\gamma} = \\gamma (\\alpha - \\beta) \\end{align}\\]\nIf α &lt; β, det(J) &lt; 0: a saddle point.\nIf α &gt; β, det(J) &gt; 0: look the trace of the matrix.\n\\[Tr(J(S,I)) = \\frac{\\gamma (\\beta - \\alpha)}{\\beta - \\gamma} - \\gamma\n\\\\\n\\\\ \\gamma &gt; 0 \\ \\text{and} \\ \\alpha &gt; \\beta\n\\\\\n\\\\ \\text{So:} \\ Tr(J(S,I)) &lt;0 \\]\nIn this case we have a stable point. The specific type of stability depends on the signs and nature of the eigenvalues.\nNow, we solve the characteristic polynomial to obtain the eigenvalues:\n$$ \\[\\begin{align} & det(J- \\lambda Id) = 0\\\\\n& \\text{with Id : identity matrix}\\\\\n\n&\\Leftrightarrow \\begin{bmatrix} \\frac{\\gamma (\\beta - \\alpha)}{\\beta + \\gamma}-\\gamma -\\lambda\n& - \\beta - \\gamma\\\\\n\\frac{\\gamma (\\alpha - \\beta)}{\\beta + \\gamma} & - \\lambda \\end{bmatrix} = 0\\\\\n&\\Leftrightarrow - \\lambda (\\frac{\\gamma (\\beta - \\alpha)}{\\beta + \\gamma}-\\gamma -\\lambda)\n- (- \\beta - \\gamma )(\\frac{\\gamma (\\alpha - \\beta)}{\\beta + \\gamma}\n) = 0 \\\\\n&\\Leftrightarrow \\lambda^2 + \\lambda (\\gamma +  \\frac{\\gamma (\\alpha- \\beta)}{\\beta + \\gamma})+ \\gamma (\\alpha - \\beta) = 0 \\end{align}\\]$$\nThe number of solutions is indicated by the value of the discriminant : \\[ \\begin{align} & \\Delta = (\\gamma +  \\frac{\\gamma (\\alpha - \\beta)}{\\beta + \\gamma})^2 - 4 (\\gamma (\\alpha - \\beta))\n\\\\ &\\text{With} \\ \\alpha &gt; \\beta\n\\\\\n\\\\\n\\text{If} \\ & \\Delta &lt; 0:\\ \\text{you will have two complex conjugate roots.} \\\\ &\\Delta = 0:\\ \\text{you will have one real repeated root.} \\\\ &\\Delta&gt;0: \\ \\text{you will have two distinct real roots.} \\end{align} \\]\nFrom now,we will assign numerical values: \\[ \\alpha = 4, \\ \\beta = 2 \\ and \\ \\gamma = 1\n\\\\\n\\Delta = (1 + \\frac{1(4-2)}{1+2})^2 - 4(1(4-2))= -5.22\n\\\\ So \\ \\Delta &lt; 0 \\]\n\\[ \\begin{align} &\\text{Solutions are given by the following formula:}\n\\\\ & \\lambda= \\frac{-(\\gamma + \\frac{\n\\gamma (\\alpha - \\beta)}{\\beta + \\gamma}) \\pm \\sqrt \\Delta}{2}i\n\\\\ & \\frac{\\sqrt \\Delta}{2}i: \\ \\text{imaginary part.}\n\\\\ & \\frac{-(\\gamma + \\frac{\\gamma (\\alpha - \\beta)}{\\beta +\\gamma})}{2}: \\ \\text{real part.}\n\\\\\n\\\\ & \\text{Real part}= -\\frac{5}{3}\n\\end{align} \\]\nThe stability of a linear system is often determined by the real parts of the eigenvalues. In this case, the real part is negative which indicates a stable attractive node.\nThis sounds complicated, but don’t panic! In ecology we are lucky to have a great tool, R, which allows us to resolve this type of system. You can see how to do it on R at the end of Lotka Volterra 3 species.\n\n\nSolving SIR model with R\nUse R to study the behavior of our model as double interest.\n\nVerify if our theoretical calcul is okay\nHelp you to have an idea of the behavior of the model\n\nTo show how you can use phaseR and deSolve to solve ODE we will use the SIR we just solved. We hope our calcul is good. In this chapter, we will show what’s the most common function you can use, but not all the parameters you can add. I know you are probably sobbing “wHyYyYyy?” (Who has never dreamed of knowing all the difference between the Euler and Runge Kunta order 4?) but we let you make your search if you need more information.\n\nStep 1: Prepare you environnement\nLet’s start by erasing all the data in our environment in order to don’t bother us when we work. Then we load the 3 packages we will use.\n\nThe most famous library to solve and study ODE is deSolve you can also compute yourself if you want but it will be a harsh task. A great point of deSolve it’s its simplicity to use and we hope you will agree with us a the end of this chapter\nphaseR Performs a qualitative analysis of one- and two-dimensional autonomous ordinary differential equation systems, using phase plane methods\nWe will also use ggplot to make a beautiful graph. It’s also a little more intuitive way to create and manipulate plot than using a basic ploting system in r\nrgl is a package which give you tools to creat 3d plot and objects\ntidyr is like ggplot but for data manipulation. It doesn’t bring very special tools but able us to transform easily our table\n\nAs we talk about programmation we have to talk about technical issues you can have. To prevent you from committing this, we put during the presentation some technical issues to think of before coding. If you want to do the same things as us, you will probably find yourself in some problems: don’t be afraid R is widely used and you will easily find people who have defeated the same problem. Don’t be ashamed to search for help!\n\n#We erase all data use before\nrm(list = ls())\n#load library we need\nlibrary(ggplot2)\nlibrary(deSolve)\nlibrary(phaseR)\nlibrary(rgl)\nlibrary(tidyr)\n\n\n\nStep 2: Define your model\nIt’s time to start modeling! The first step is to create our function which gives us the evolution of our population according to time. This function takes a step of time, the initial population value, and the value of different parameters. The output of this function needs to be a list of the new populations. To have the behavior of our function we have to use the same hypothesis as in the calculation part so we will not code how the Recover compartment evaluates (R). If we need the Recover evolution we will use: \\[1 = S + I + R \\Leftrightarrow R = 1 - S - I \\] Technical consideration: When you build your function you are free to make some modification BUT the order of the input has to be time, population, and parameter value because the ODE function want you to do this!\n\nmod_SIR &lt;- function(t , dy , parameters){\n#we take our population and give give to to different variable\n  S &lt;- dy[1]\n  I &lt;- dy[2]\n\n#We give the value of parameters to the variable (there is greater way to do this but i let you find out)\n  alpha &lt;- parameters[1]\n  beta &lt;- parameters[2]\n  gamma &lt;- parameters[3]\n\n#sol is the vector where we stock our solution\n  sol &lt;- numeric(2)\n\n#We compute the equation need\n  sol[1] &lt;- - alpha * S * I + gamma * (1 - S - I)\n  sol[2] &lt;- alpha * S * I  - beta * I\n  \n#And list solution \n  list(sol)\n}\n\n\n\nStep 3: Study how the system evaluates among time\nNow we have done the heart of our model let’s study it with ODE. It’s from the deSolve package to solve our model. To do this we define some quantities. For the time we use the seq function which creates a vector that goes from 0 to 600 by 0.01 step. For parameter values, we create a vector in the same order as in our function. And value we start to form\nThen we use ODE to simulate how our population evolves at each time in our vectors. We take the output from ODE and manipulate it a little bit to make visualization easier.\n\n#Definition of parameters we will need for the suite\nparam &lt;- c(alpha = 0.2, beta = 0.1, gamma = 0.2)\ntimes = seq(from = 0 , to = 600, by = 0.01)\ndepart = c(0.99 , 0.001 )\n\n#Calculation of the evolution of our population \nresolnum &lt;- ode(y = depart, times = times, func = mod_SIR , parms = param)\n\n#Transfortmation of data in order to be use easely\nresolnum &lt;- as.data.frame(resolnum)\nresolnum$R&lt;- 1-  resolnum[,2] + resolnum[,3] \n\ncolnames(resolnum) &lt;- as.factor(c(\"Times\" , \"Susceptible\" ,\"Infectious\" ,\"Recovered\"))\n\n\n\n#Plotting time\nggplot(resolnum,aes(x=Times))+\n  geom_line(aes(x = Times, y = Susceptible),color =\"chartreuse\" ,size =1.5)+\n  geom_line(aes(x = Times,y = Infectious),color = \"red\" ,size =1.5)+\n  geom_line(aes(x = Times,y = Recovered),color = \"darkviolet\" ,size =1.5)+\n  labs(title = \"Population evolution among times\", y = \"Population\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow we have an idea of how our population evaluates thought time we can try to find the behavior\n\n\nStep 4: Equilibrium point and behavior\nLet’s study the behavior of our model and its equilibrium points. We’re going to study S as a function of I. First function displays model behavior. Second draws the lines where the models cancel out.\nTechnical consideration: To have all nullclines you should extend the area you want to explore. For example, we only have interest in the area where S and I go to 0 at 1(we made the hypothesis the sum of all compartments should be equal to 1) but in our simulation, we go from -0.1 to 1.1.\n\n#Plot the velocity field\n  field&lt;-flowField(mod_SIR ,xlim=c(-0.1,1.1),ylim=c(-0.1,1.1),parameters = param,add = F,xlab = \"Susceptible\" , ylab = \"Infectious\")\n  #Draw the isoclines of our model\nnullc&lt;-nullclines(mod_SIR ,xlim=c(-0.1,1.1),ylim=c(-0.1,1.1),parameters = param,points = 100, add = T,add.legend = F,size =2)\n\n\n\n\n\n\n\n\nAs we can see at crossing of nullclines we have two equilibrium points. The first is at the center of our phase diagram, and the arrows seem to show that we’re tending towards this point. A second is present where I = 0 and S = 1 on the contrary, we seem to be moving away from it. If this graph allows us to observe 2 points of equilibrium, it is not easy to determine their behavior.\nTo look at the behavior near these points, we can use the trajectory function, which will plot the trajectories of our system as a function of a starting point. Here we repeat the trajectory functions in order to have a 51 scenario.\n\n#we define our initials conditions\nX0 &lt;- sample(x = seq(0,1,0.01),size =1)\nY0 &lt;- sample(x = seq(0,1-X0,0.01),size =1)\ncond_init = c(X0, Y0)\n#We simulate what's happened for one trajectory\n  t&lt;-trajectory(mod_SIR ,cond_init,tlim= c(0,50),parameters = param,col = \"black\",add = F,xlim=c(0,1),ylim = c(0,1),xlab = \"Susceptible\" , ylab = \"Infectious\")\n  \n#We repeat the same process for 50 other trajectory\nfor(i in 1:50){\n  X0 &lt;- sample(x = seq(0,1,0.01),size =1)\nY0 &lt;- sample(x = seq(0,1-X0,0.01),size =1)\n\ncond_init = c(X0, Y0)\n  traj&lt;-trajectory(mod_SIR ,cond_init,tlim= c(0,50),parameters = param,col = i,add = T)\n}\n\n\n\n\n\n\n\n\nWhat the vector field showed us seems to be confirmed by this graph. Let’s check our observations with new functions (promise they’re the last)\nWe’ll check the behavior of the center point using the stability function. We could also have used the findEquilibrum function to find all the equilibrium points, but we’ll let you have your fun.\n\n# Checking stability of the point we see\nstability(SIR,ystar = c(0.5,0.5),parameters = param )$classification[1]\n\ntr = -0.1, Delta = 0.01, discriminant = -0.03, classification = Stable focus\n\n\n[1] \"Stable focus\"\n\n\nThe two functions confirm our observations and the calculations made just before - what more could you ask for?\nThere are still other functions in the deSolve and pahse R packages that could be of interest, but which are useful in more specific cases, so we won’t look at them here."
  },
  {
    "objectID": "Chap_ODE.html#lotka-volterra-a-three-species-model",
    "href": "Chap_ODE.html#lotka-volterra-a-three-species-model",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "2) Lotka-Volterra: A three-species model",
    "text": "2) Lotka-Volterra: A three-species model\nBiotic interactions are complex, requiring the interaction of numerous parameters. If we take the Lotka-Volterra example, it only considers the interactions between two species, prey and predator. It assumes a constant mortality rate among predators, regardless of prey. However, predators are themselves the prey of other super-predators. Their mortality rate should therefore depend on the super-predators they encounter. Thus, the prey-predator-superpredator extension of the Lotka-Volterra model would be a good representation of natural trophic interactions.\nIn this case, we considerer a superpredator, a predator an a prey.\nA superpredator is an organism that reaches adulthood at the top of the food chain. Thus, by definition, a superpredator is not the prey of any other species. The predator is consider to be in the middle of the food chain and the prey at the bottom.\nThe model can be written as the following system of differential equations:\n\\[ PPS=\\begin{cases} \\frac{d_x}{d_t}=ax-bxy &\\text{Prey} \\\\ \\frac{d_y}{d_t}=-cy+hxy-eyz&\\text{Predator} \\\\ \\frac{d_z}{d_t}=-fz+gyz &\\text{Superpredator} \\end{cases} \\]\nThe model parameters are:\n\na: x natural growth rate\n-f and -c: natural death rate of y and z\nThe growth rate of y and z depends on the number of prey caught. It is modeled by hxy for y and by gyz for z.\nThe death rate of x and y depends on the number of top predators y for x and z for y. It is modeled by -bxy for x and by -eyz for y.\n\nThis model is only useful for chains of predators. It doesn’t take into account the presence of several predators with different dynamics on the same prey.\nLet’s try to solve this system: To do this, the first step is to find the balance of the model, that is to say the values (x,y,z) so that:\n\\[\\frac{d_x}{d_t}=\\frac{d_y}{d_t}=\\frac{d_z}{d_t}=0\\]\nWe take our previous systems that we factored:\n\\[ PPS=\\begin{cases}\\frac{d_x}{d_t}=x(a-by) &\\text{Prey} \\\\ \\frac{d_y}{d_t}=y(hxy-c-ez)&\\text{Predator} \\\\ \\frac{d_z}{d_t}=z(gy-f) &\\text{Superpredator}\\end{cases} \\]\nThe obvious solution is (0,0,0) but is not the only one. Indeed, for the derivatives to be equal to 0, then we must find the following solution:\n\\[(a-by)=(gy-f)=(hx-c-ez)=0\\]\nWe first solve: \\[a-by=0\\Leftrightarrow a=by \\Leftrightarrow y=\\frac{a}{b}\\] \\[gy-f=0\\Leftrightarrow f=by \\Leftrightarrow y=\\frac{f}{g}\\]\nSo, a solution for a yeq is: \\[y_{eq}=\\frac{a}{b}=\\frac{f}{g}\\] Therefore, there is an equilibrium when: \\[ga=fb\\]\nIn order to determine the other solutions, we must solve the following jacobian matrix: \\[ J=\\begin{bmatrix}a-by&-bx&0\\\\hy&-c+hx-ez&-ey\\\\0&gz&gy-f\\end{bmatrix}\\]\nThe steps are the same as for the SIR model but here the matrix is 3x3. We invite you to look at the chapter on matrix calculation to determine the solutions.\n\nSolving Lotka-Volterra with R\nHey R is back! You loved last part with how to solve 2 equation ODE in R ? Now let’s find out how to solve a 3 equation system! Unfortunately, trying to solve this equation by hand leads to problems: it’s not easy to find all the equilibrium points. As mentioned in the previous section, phaseR is not suitable for solving three-equation systems. We can compute our function but unless you a nerd or MODE you don’t want to do this. the solution we choose in this paragraph it’s to only study numercally system. So let’s start again what we just do! We use the same library as in last part.\nWe create a function with the same limitation as in our first example.\n\nrm(list = ls())\nPPS &lt;- function(t , dy , parameters){\n\n  x &lt;- dy[1]\n  y &lt;- dy[2]\n  z &lt;- dy[3]\n  \n  r1 &lt;- parameters[1]\n  r2 &lt;- parameters[2]\n  r3 &lt;- parameters[3]\n  b1 &lt;- parameters[4]\n  b2 &lt;- parameters[5]\n  c1 &lt;- parameters[6]\n  c2 &lt;- parameters[7]\n  \n  sol &lt;- numeric(3)\n  \n  sol[1] &lt;- r1 * x - b1 * y * x\n  sol[2] &lt;- - r2 * y + b2 * x * y  - c1 * y * z \n  sol[3] &lt;- - r3 * z + c2 * z * y \n \n  list(sol)\n}\n\nIn order to determine the behavior of our system we should use the ode function to make many graph and try to understand numerically which parameter ar important. In the following example we pass this step and present you 2 cases we can have in our system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis first case is a very special one, since we observe a runaway system, with the number of prey increasing exponentially without being limited by predators. While mathematically this model makes sense, it is not very common in nature, due to the absence of infinite resources.\n\nparam &lt;- c(r1 = 0.01, r2 = 0.2 , r3 = 0.1, b1 = 0.04, b2 = 0.04, c1 = 0.09, c2 = 0.01)\ntemps = seq(from = 0 , to = 1000, by = 0.01)\ndepart = c(20 , 10 , 5)\nresolnum &lt;- ode(y = depart, times = temps, func = PPS, parms = param)\n\nresolnum &lt;- as.data.frame(resolnum)\ncolnames(resolnum) &lt;- c(\"Times\" , \"Prey\" ,\"Predators\" ,\"SuperPredators\")\n\n\n\nggplot(resolnum)+\n  geom_line(aes(x = Times, y = Prey),color =\"chartreuse\" ,size =1.5)+\n  geom_line(aes(x = Times,y = Predators),color = \"red\" ,size =1.5)+\n  geom_line(aes(x = Times,y = SuperPredators),color = \"darkviolet\",size =1.5)+\n  theme_classic()+\n  labs(title = \"Case 2: Balance system\")\n\n\n\n\n\n\n\nggplot(resolnum)+\n   geom_point(aes(x = Prey, y = Predators),color =\"black\" ,size =1.5)+\ntheme_classic()\n\n\n\n\n\n\n\nggplot(resolnum)+\n   geom_point(aes(x = Prey, y = SuperPredators),color =\"black\" ,size =0.2)+\ntheme_classic()\n\n\n\n\n\n\n\nggplot(resolnum)+\n   geom_point(aes(x =  SuperPredators, y = Predators),color =\"black\" ,size =1.5)+\ntheme_classic()\n\n\n\n\n\n\n\nplot3d(resolnum$Prey,resolnum$Predators,resolnum$SuperPredators,xlab =\"Prey\",ylab =\"Predators\",zlab =\"SuperPredators\")\nrglwidget()\n\n\n\n\n\nIn this case, our system moves towards a balance between prey and predators. This time, however, superpredators are excluded from our system.\nFor a more thorough analysis, we’d have to simulate the same model but draw our parameters randomly, but unfortunately this takes a lot of time, so we’d have to do it with very few simulations. To get a better idea of the model’s behavior, more advanced simulations should be carried out, but unfortunately the fun has to stop at some point.\nIf you eat graphics cards and want to delve deeper into the subject, the book Solving Differential Equations in R may be of some interest, even if it deals with much more complex and interesting problems than we do."
  },
  {
    "objectID": "Chap_ODE.html#generalizations-of-the-three-species-model-and-other-examples",
    "href": "Chap_ODE.html#generalizations-of-the-three-species-model-and-other-examples",
    "title": "Ordinary Differential Equations and their application in Ecology",
    "section": "3) Generalizations of the three-species model and other examples",
    "text": "3) Generalizations of the three-species model and other examples\n\na) Generalizations of the three-species model\nThe three-species model presented previously is a simplified model that models a perfectly linear trophic chain. However, nature is more complex and a species can be preyed upon by multiple predators and predators can predate multiple prey items.\nWe invite you to go see the course of Lalith Devireddy (2016) entitled “Extending the Lotka-Volterra Equations” which develops this model and its resolutions.\nMany models with 3-ODE systems exist to model ecological dynamics. Here we will present one additional ones to you but we will not develop their resolution.\n\n\nb) NPZ model\nThe NPZ model makes it possible to model the lake production of a pelagic ecosystem. This model studies variations in nutrients, phytoplankton and zooplankton that are linked to each other.\nHere are the equations of the system, but it will not be worked up here. \\[\n\\begin{align} & NPZ = \\begin{cases} \\frac{dP}{dt}= \\frac{V_m N}{k_s + N}P -mP -ZR_m(1-e^{-\\Lambda P}) & \\text{Pythoplankton}\n\\\\ \\frac{dZ}{dt}= \\gamma ZR_m(1-e^{- \\Lambda P}-dZ) & \\text{Zooplakton}\n\\\\ \\frac{dN}{dt}=-\\frac{V_m N}{k_s + N}P+mP+dZ+(1- \\gamma ) ZR_m (1-e^{- \\Lambda P}) & \\text{Nutrients} \\end{cases}\n\\\\\n\\\\ & \\text{Definitions of variable:}\n\\\\ & - V_m: \\ \\text{Maximum phytoplankton growth rate}\n\\\\ & - N: \\ \\text{Nutrient concentration}\n\\\\ & - k_s: \\ \\text{Half saturation constant for nutrients}\n\\\\ & - P: \\ \\text{Phytoplankton stock size}\n\\\\ & -m: \\ \\text{Phytoplankton mortality rate}\n\\\\ & -Z: \\ \\text{Zooplankton stock size}\n\\\\ & - \\gamma : \\ \\text{Zooplankton growth efficiency}\n\\\\ & - R_m : \\ \\text{Maximum zooplankton ration}\n\\\\ & - \\Lambda : \\ \\text{Ivlev constant}\n\\\\ & -d: \\ \\text{mortality rate} \\end{align} \\]"
  },
  {
    "objectID": "python_chapter.html",
    "href": "python_chapter.html",
    "title": "Chapter : statistical tests with dependent data",
    "section": "",
    "text": "This chapter is a simple example using python\nYou can import python libraries using the code\n\nimport pandas as pd\nimport numpy as np\n\nand then describe the purpose of your chapter as well as executing python command.\nFor example a basic summary of a dataset is given by\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\")\n\nand produce a graph\n\ndf.boxplot(by = 'species', column = 'body_mass_g')  \n\n&lt;Axes: title={'center': 'body_mass_g'}, xlabel='species'&gt;"
  },
  {
    "objectID": "simple_chapter.html",
    "href": "simple_chapter.html",
    "title": "A simple chapter with no code",
    "section": "",
    "text": "This chapter is a simple example of qmd format"
  },
  {
    "objectID": "simple_chapter.html#subsection",
    "href": "simple_chapter.html#subsection",
    "title": "A simple chapter with no code",
    "section": "subsection",
    "text": "subsection\na list\n\nblabla\nblabla 2\nblabla 3"
  },
  {
    "objectID": "AMV_chapter.html",
    "href": "AMV_chapter.html",
    "title": "Multivariate Analysis",
    "section": "",
    "text": "Biologists’ data, whether on individuals, populations or communities, are usually presented in the form of rectangular tables, with observations (n) in the rows and variables (p) in the columns.\nThe graphical representation of these n observations and p variables is easily achieved when there are only 2 or 3 variables (dimension). However, when the number of variables increases, the graphical representation becomes complicated, and Multivariate Analyses come into their own!\nThe aim of these analyses is to reduce the number of dimensions and examine the structure of the data by answering the following questions:\n\nWhich observations are similar?\nAre there observations that stand out? Subgroups?\nWhich variables are correlated?\nAre there particular links between certain observations and/or variables?\n\nMultivariate analysis methods are therefore used to describe the data and generate hypotheses that can then be tested."
  },
  {
    "objectID": "AMV_chapter.html#introduction",
    "href": "AMV_chapter.html#introduction",
    "title": "Multivariate Analysis",
    "section": "Introduction",
    "text": "Introduction\nPCA can be used to process a measurement table with :\n\nIn row: the n observations.\nIn column: the p quantitative variables.\n\nThe 2 sets (observations and variables) are totally distinct and non-interchangeable. In other words, if you interchange the rows and columns (with the variables in the rows and the observations in the columns), the table no longer has the same meaning.\nIn PCA tables, the mean of a column has a meaning, while the mean of a row does not.\nThe aim of PCA is to achieve the best geometric representation of individuals and variables. To achieve this, we seek to reduce the dimension by finding the best projection plane (subspace) for “best” visualization of the point cloud in reduced space.\nHowever, this reduction must :\n\nTwo individuals who resemble each other must be close in the representation space.\nPreserve correlations between variables:\n\nTwo variables that are correlated must be represented by vectors forming an acute angle.\nTwo independent variables are represented by orthogonal vectors."
  },
  {
    "objectID": "AMV_chapter.html#mathematics",
    "href": "AMV_chapter.html#mathematics",
    "title": "Multivariate Analysis",
    "section": "Mathematics",
    "text": "Mathematics\nPrincipal Component Analysis (PCA) is a powerful method for simplifying the complexity of highly correlated data. This approach aims to reduce dimensionality in a linear manner, thereby providing a clearer perspective on the underlying structure of the data.\nIn essence, PCA performs a linear transformation of the original variables to new variables called principal components. These components are carefully selected to capture the majority of the variance present in the dataset, thereby maximizing the separation between different observations.\nThe space of principal components can be expressed as :\n\\[\nZ_p=\\Sigma^p_{j=1}ø_j\\times X_j\n\\]Where :\n\n\\(Z_p\\) is the principal component \\(p\\)\n\\(Ø_j\\) is the weight vector comprising the \\(j\\) weights for principal component \\(p\\), i.e., the coefficients of the linear combination of the original variables from which the principal components are constructed.\n\\(X_j\\) is the standardized predictor, meaning it has a mean equal to 0 and a standard deviation of 1.\n\nThe process begins with a matrix \\(Y\\), where each row represents an observation and each column represents a continuous variable.\n\n\n\n\n\nFirst, we normalize the observations as follows :\n\\[\nY_{std} = \\frac{y_i-\\overline{y}}{\\sigma_y}\n\\] ; which is equivalent to centering as in:\n\\[\ny_c= [y_i-\\overline{y}]\n\\] ; then scaling as in:\n\\[\ny_s = \\frac{y_i}{\\sigma_y}\n\\]\nWe can then calculate the variance-covariance matrix:\n\\[\nR = cov(Y_{std})\n\\]The decomposition of this matrix provides the matrix \\(U\\) of eigenvectors, which contains the principal components.\nThe eigenvectors, defined as the principal components, have associated eigenvalues. The graphical representation of the distances of observations to these eigenvectors illustrates how each principal component captures the variation in the data.\n\n\n\n\n\n\n\n\n\n\nThe orthogonal aspect of the principal components becomes evident when observing the second principal component, which, in turn, maximizes the variance in the data. These principal components, each aligned along an orthogonal axis, form a new coordinate system in which the information is more clearly structured.\nIn addition to representing the variance of the principal components, the eigenvectors also provide a relative understanding of their influence. This influence is calculated by dividing the eigenvalues by the total sum of these values, thus providing context on the contribution of each principal component \\(v_k\\) to the overall information.\n\\[\nExplained\\ variance\\ of\\ v_k = \\frac{\\lambda_{v_k}}{\\Sigma_{i=1}^p\\lambda_v}\n\\]The conclusion of the PCA involves obtaining the coordinate matrix \\(F\\). This matrix results from the multiplication of matrix \\(U\\) by the standardized matrix \\(Y_{std}\\), enabling a rotation of the new data space. This process aligns the data along the principal components, providing a simplified yet informative view of the underlying structure of the initial data."
  },
  {
    "objectID": "AMV_chapter.html#interpretation",
    "href": "AMV_chapter.html#interpretation",
    "title": "Multivariate Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere’s an example of how PCA can be applied. dataset presentation package needed\n\na) Importing the dataset :\n\n# Loading the \"iris\" dataset available on R :\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\ndata(\"decathlon2\") \ndata=decathlon2[decathlon2$Competition==\"OlympicG\",c(1:10,12)]\nsummary(data)\n\n     X100m         Long.jump        Shot.put       High.jump    \n Min.   :10.44   Min.   :6.990   Min.   :13.07   Min.   :1.880  \n 1st Qu.:10.72   1st Qu.:7.303   1st Qu.:14.52   1st Qu.:1.940  \n Median :10.88   Median :7.475   Median :14.86   Median :2.045  \n Mean   :10.82   Mean   :7.474   Mean   :14.90   Mean   :2.017  \n 3rd Qu.:10.94   3rd Qu.:7.688   3rd Qu.:15.29   3rd Qu.:2.112  \n Max.   :11.14   Max.   :7.960   Max.   :16.36   Max.   :2.150  \n     X400m        X110m.hurdle       Discus        Pole.vault   \n Min.   :46.81   Min.   :13.97   Min.   :40.11   Min.   :4.400  \n 1st Qu.:48.56   1st Qu.:14.07   1st Qu.:43.90   1st Qu.:4.625  \n Median :49.05   Median :14.23   Median :44.73   Median :4.900  \n Mean   :49.01   Mean   :14.29   Mean   :45.43   Mean   :4.843  \n 3rd Qu.:49.41   3rd Qu.:14.36   3rd Qu.:47.66   3rd Qu.:5.000  \n Max.   :50.79   Max.   :14.95   Max.   :51.65   Max.   :5.400  \n    Javeline         X1500m          Points    \n Min.   :51.53   Min.   :264.4   Min.   :7926  \n 1st Qu.:55.43   1st Qu.:270.5   1st Qu.:8088  \n Median :58.11   Median :276.3   Median :8236  \n Mean   :59.58   Mean   :275.1   Mean   :8317  \n 3rd Qu.:62.92   3rd Qu.:278.6   3rd Qu.:8396  \n Max.   :70.52   Max.   :287.6   Max.   :8893  \n\n\nIn this data set, we have 14 individuals (athletes) on whom 10 quantitative variables (performances in different sports disciplines) have been recorded:\n\nX100m\nLong.jump\nShot.put\nX400m\nX110m.hurdle\nDiscus\nPole.vault\nJavelin\nX1500m\n\nIf NAs are present in the data. In order to carry out the PCA without any problems with NAs, they can be omitted from the analysis with the following command:\n\n# data=na.omit(object = data)\n\nWe want to find out whether particular links between certain observations and/or variables can be observed in this data set.\n\n\nb) Study of correlations :\nObtain the correlation matrix between variables using the cor() function:\n\ncor(data)\n\n                   X100m  Long.jump    Shot.put   High.jump       X400m\nX100m         1.00000000 -0.8350966 -0.21746312 -0.26333512  0.54802600\nLong.jump    -0.83509656  1.0000000  0.31407521  0.24623935 -0.51919384\nShot.put     -0.21746312  0.3140752  1.00000000  0.77045776 -0.18546890\nHigh.jump    -0.26333512  0.2462394  0.77045776  1.00000000 -0.17357206\nX400m         0.54802600 -0.5191938 -0.18546890 -0.17357206  1.00000000\nX110m.hurdle  0.38366645 -0.4914562  0.03157435  0.01126452  0.35162059\nDiscus       -0.51341585  0.5136697  0.88239852  0.71362867 -0.36823138\nPole.vault    0.07021566  0.1578300 -0.41715958 -0.65024394  0.19459315\nJaveline     -0.06085398  0.2256900  0.53582882  0.20440490  0.04744169\nX1500m       -0.47201188  0.5394643  0.10753385 -0.03189887  0.15026787\nPoints       -0.64440656  0.7809521  0.76009926  0.60096524 -0.52293108\n             X110m.hurdle     Discus  Pole.vault    Javeline      X1500m\nX100m          0.38366645 -0.5134159  0.07021566 -0.06085398 -0.47201188\nLong.jump     -0.49145618  0.5136697  0.15783003  0.22568995  0.53946433\nShot.put       0.03157435  0.8823985 -0.41715958  0.53582882  0.10753385\nHigh.jump      0.01126452  0.7136287 -0.65024394  0.20440490 -0.03189887\nX400m          0.35162059 -0.3682314  0.19459315  0.04744169  0.15026787\nX110m.hurdle   1.00000000 -0.1755752  0.05716187  0.23674145 -0.41440555\nDiscus        -0.17557519  1.0000000 -0.46586569  0.47246648  0.12824707\nPole.vault     0.05716187 -0.4658657  1.00000000  0.08916975  0.42631317\nJaveline       0.23674145  0.4724665  0.08916975  1.00000000  0.01073007\nX1500m        -0.41440555  0.1282471  0.42631317  0.01073007  1.00000000\nPoints        -0.23729481  0.8596594 -0.13740583  0.60192747  0.24700780\n                 Points\nX100m        -0.6444066\nLong.jump     0.7809521\nShot.put      0.7600993\nHigh.jump     0.6009652\nX400m        -0.5229311\nX110m.hurdle -0.2372948\nDiscus        0.8596594\nPole.vault   -0.1374058\nJaveline      0.6019275\nX1500m        0.2470078\nPoints        1.0000000\n\n\nVariables are correlated if their value is greater than 0.9 (as a general rule). Here, we can see that no variables are correlated.\nHere’s another way of visualizing correlations between variables: (useful when you have a lot of variables, as here):\n\nabs(cor(data))&gt;0.9\n\n             X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nX100m         TRUE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nLong.jump    FALSE      TRUE    FALSE     FALSE FALSE        FALSE  FALSE\nShot.put     FALSE     FALSE     TRUE     FALSE FALSE        FALSE  FALSE\nHigh.jump    FALSE     FALSE    FALSE      TRUE FALSE        FALSE  FALSE\nX400m        FALSE     FALSE    FALSE     FALSE  TRUE        FALSE  FALSE\nX110m.hurdle FALSE     FALSE    FALSE     FALSE FALSE         TRUE  FALSE\nDiscus       FALSE     FALSE    FALSE     FALSE FALSE        FALSE   TRUE\nPole.vault   FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nJaveline     FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nX1500m       FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\nPoints       FALSE     FALSE    FALSE     FALSE FALSE        FALSE  FALSE\n             Pole.vault Javeline X1500m Points\nX100m             FALSE    FALSE  FALSE  FALSE\nLong.jump         FALSE    FALSE  FALSE  FALSE\nShot.put          FALSE    FALSE  FALSE  FALSE\nHigh.jump         FALSE    FALSE  FALSE  FALSE\nX400m             FALSE    FALSE  FALSE  FALSE\nX110m.hurdle      FALSE    FALSE  FALSE  FALSE\nDiscus            FALSE    FALSE  FALSE  FALSE\nPole.vault         TRUE    FALSE  FALSE  FALSE\nJaveline          FALSE     TRUE  FALSE  FALSE\nX1500m            FALSE    FALSE   TRUE  FALSE\nPoints            FALSE    FALSE  FALSE   TRUE\n\n\nIf TRUE (outside the diagonal), then both variables are correlated. If two variables are correlated, one must be removed. The choice of deleting one of the two correct variables is arbitrary and depends on the question being asked.\nTo remove a variable from the data set, use the following function: data=data[,-(column_of_the_variable_to_remove)}]\n\n\nc) Performing PCA :\nTo run the PCA, you need to load the following two packages: factoextra and FactoMineR\nThen, to perform the PCA on R, you can use the function : PCA(). Remember to store the result of this PCA in a new variable, so that you can easily retrieve the PCA information you need for subsequent interpretation.\n\n\nd) Interpretation of outputs :\n\n1) Inertia and choice of axes :\n\n# Recover the eigenvalues of each axis and the inertia carried by the principal components\nPCA$eig\n\n          eigenvalue percentage of variance cumulative percentage of variance\ncomp 1  4.753038e+00           4.320944e+01                          43.20944\ncomp 2  2.418289e+00           2.198444e+01                          65.19388\ncomp 3  1.551340e+00           1.410310e+01                          79.29698\ncomp 4  9.863587e-01           8.966897e+00                          88.26388\ncomp 5  5.554931e-01           5.049937e+00                          93.31381\ncomp 6  3.216961e-01           2.924510e+00                          96.23832\ncomp 7  2.203570e-01           2.003246e+00                          98.24157\ncomp 8  1.149733e-01           1.045212e+00                          99.28678\ncomp 9  6.178365e-02           5.616696e-01                          99.84845\ncomp 10 1.666416e-02           1.514924e-01                          99.99994\ncomp 11 6.323601e-06           5.748728e-05                         100.00000\n\n# Graphical display of the inertia of each axis : \nfviz_eig(PCA, addlabels = TRUE)\n\n\n\n\nTo determine the number of axes used in the PCA analysis, we need to identify the jump in variance explained by the different axes. Here we can see that the first axis represents 43.2% of the variance, the second axis 22%, the third axis 14.1%, …\nWe can therefore see that the jump in variance explained by the different axes is between the first and second axes. The difference between the variance explained by the axes other than the first is negligible compared to the difference between the first axis and the others.\nFor the PCA interpretation, we therefore retain the first two axes, which together explain 65.19% of the variance.\n\n\n2) Interpretation of the biological meaning of the axes :\nIn PCA, the axes are interpreted according to the columns (i.e. variables) via the correlation circle and the absolute contributions of the variables.\nTo obtain this information in R, use the following commands:\n\n# Obtain the absolute contributions of variables :\nPCA$var$contrib\n\n                 Dim.1       Dim.2       Dim.3      Dim.4       Dim.5\nX100m        10.824776 10.03651625  1.63515357  0.3755928 23.39362182\nLong.jump    12.418510 13.24601095  0.18397545  1.0039891  1.29845908\nShot.put     13.118558  8.89903353  2.86005522  2.7322519  1.12053773\nHigh.jump    10.101229 10.94386007  1.34856164  9.6446306  5.75676065\nX400m         6.050422  1.75575069 17.68011201 32.4590377  0.09123132\nX110m.hurdle  2.265809 13.96580459 10.80010469  7.4256988 54.75065458\nDiscus       17.620323  2.99067030  0.06343601  0.5423102  0.71456580\nPole.vault    2.249144 16.38731995 23.04479888  4.8414968  0.03149013\nJaveline      4.133763  3.25169636 31.78461582  9.3858284 10.87089875\nX1500m        1.779257 18.51325372  8.53578854 28.5491707  1.51933507\nPoints       19.438208  0.01008361  2.06339817  3.0399930  0.45244507\n\n# Obtain the circle of correlations :\nfviz_pca_var(PCA)\n\n\n\n# Graphical representation of absolute variable contribution values for an axis: \n# Axis 1 case : \nfviz_contrib(PCA, choice = \"var\", axes = 1)\n\n\n\n# Axis 2 case: \nfviz_contrib(PCA, choice = \"var\", axes = 2)\n\n\n\n\nThe absolute contribution shows how the initial variable contributes to the formation of the axis. To find out from these results which variables contribute to the formation of the synthetic axis, we use the threshold of \\(\\frac{1}{nb\\_variables}\\times100\\) (in general). For a variable to contribute, its contribution value must be greater than this threshold. This threshold value corresponds to the red dotted line on the graphs showing the absolute contributions of the variables along the axes.\nFrom these results, we can see that axis 1 is mainly represented by the following variables:\n\nPoints, Discus, Shotput, Long.jump, X100m, High.jump\n\nAxis 2 is represented by the variables:\n\nX1500m, Pole.vault, X110m.hurdle, X100m\n\nThe correlation circle also shows that some variables are related and others independent. Two related variables are represented by vectors forming an acute angle. Two independent variables are represented by orthogonal vectors. Two negatively related variables are represented by two opposite vectors.\n\n\n3) Projection of individuals to create a typology based on the axes:\nFor this, we use the relative contributions of the individuals. These relative contributions can be used to create a typology of individuals based on the axes. The relative contribution of an individual corresponds to the share of information on the axis explained by the point and supported on the axis (in percentage).\nTo obtain the relative contributions of individuals on R, use the following commands:\n\n# Relative contributions of individuals :\nPCA$ind$cos2\n\n                Dim.1        Dim.2        Dim.3        Dim.4        Dim.5\nSebrle     0.68963196 6.540767e-12 0.1505401735 3.421119e-03 0.0919816558\nClay       0.68982961 7.883438e-02 0.1191186204 1.593953e-04 0.0009981983\nKarpov     0.74957187 2.476108e-02 0.1608118952 2.542731e-03 0.0008147725\nMacey      0.12167773 7.112491e-01 0.0847580597 1.109719e-04 0.0239527703\nWarners    0.03513216 7.035171e-01 0.2043199790 2.553907e-02 0.0041832424\nZsivoczky  0.01766196 7.411226e-01 0.0583141713 1.304529e-02 0.1454252042\nHernu      0.23531031 2.767503e-01 0.1923125481 1.038527e-01 0.0748120286\nBernard    0.03767052 2.359745e-02 0.4446099398 2.664423e-01 0.0658970688\nSchwarzl   0.74103094 1.190975e-01 0.0087379756 1.268006e-06 0.0098608763\nPogorelov  0.12049107 2.472670e-02 0.0687218609 7.285021e-01 0.0045896049\nSchoenbeck 0.47746580 2.724861e-02 0.2569700383 5.580082e-02 0.0190441531\nBarras     0.32981991 4.127756e-01 0.0005470956 1.535366e-02 0.1765554931\nNool       0.23451399 1.310712e-01 0.2420363184 2.449685e-01 0.1242846825\nDrews      0.53811419 2.784934e-01 0.1528997775 1.306683e-02 0.0056915740\n\n# Obtaining the projection of individuals : \nfviz_pca_ind(PCA)\n\n\n\n\nThe individuals who most explain Axis 1 are :\n\nSebrle, Clay, Karpov, Schwarzl, Drews, Schoenbeck\n\nThose who most explain axis 2 are :\n\nMacey, Warners, Zsivoczky, Barras\n\n\n\n\ne) Biological conclusion :\nThus, thanks to the results of the absolute contributions of the variables and the relative contributions of the individuals, we can represent the projection of the individuals and variables as follows:\n\nfviz_pca_biplot(PCA)\n\n\n\n\nKnowing the biological interpretation of the axes :\n\n\n\nPCA interpretation\n\n\nWe can therefore see that the individuals with the highest scores at this tournament are those who had the best results in the discus, shot put, long jump and high jump events. In addition, having good results in the 100m race doesn’t give you a good ranking.\nThanks to PCA, we can see that the final score is more linked to the score of certain disciplines than others. And therefore, that links between certain variables and individuals can be observed."
  },
  {
    "objectID": "AMV_chapter.html#introduction-1",
    "href": "AMV_chapter.html#introduction-1",
    "title": "Multivariate Analysis",
    "section": "Introduction",
    "text": "Introduction\nCA deals with tables that cross two categorical variables with several modalities, such as contingency tables, frequency tables, …\nIn these tables, rows and columns are interchangeable and the variables are all in the same units.\nThe aim of CA is to reveal structures and associations between these qualitative variables by representing them in a multidimensional space. It allows us to visualize relationships between categories of different variables, and to highlight trends, groupings or disparities. It is particularly useful for analyzing categorical data, when you want to understand the underlying structure of the data and identify associations between categories.\nUnlike PCA, for a CA, we’re not interested in the distances between points, but in the distances between profiles of different modalities. A CA is therefore simply a PCA on the profiles, by equipping the space with a suitable distance: the \\(\\chi^2\\) distance. With this distance, the weight of rows (or columns) is relativized, but not cancelled out, and the symmetry between rows and columns is preserved."
  },
  {
    "objectID": "AMV_chapter.html#mathematics-1",
    "href": "AMV_chapter.html#mathematics-1",
    "title": "Multivariate Analysis",
    "section": "Mathematics",
    "text": "Mathematics\nCorrespondence Analysis (CA) represents a statistical method aimed at exploring the relationship between two sets of categories. In contrast to PCA, which applies to continuous variables, CA is designed for categorical variables, allowing the analysis of the structure of a contingency table.\n\n\n\n\n\nIn essence, Correspondence Analysis (CA) seeks to graphically represent associations between rows and columns in a contingency table. It helps reveal trends, groupings, or oppositions within categories, providing insight into the structure of dependence between variables.\nThe process begins with a contingency table showing the observed frequencies of co-occurrence between different categories. These data are then transformed to obtain a probability table where the frequency of each co-occurrence is calculated as follows:\n\\[\nf_{ij} = \\frac{X_{ij}}{n}\n\\]With \\(f_{ij}\\) representing the probability that events \\(i\\) and \\(j\\) occur jointly, \\(X_{ij}\\) the number of times this co-occurrence has been observed, and \\(n\\) the total number of observations.The marginal probabilities of the columns \\((f_{.j})\\) and rows \\((f_{i.})\\) are calculated as follows:\n\\[\nf_{.j} = \\sum_{i=1}^{I}f_{ij}\\\\\\\\\\\\  \nf_{i.}=\\sum_{j=1}^{J}f_{ij}\n\\]\n\n- Profiles\n\nRow profiles\nThe row profile is \\(L_i = (\\frac{f_{i1}}{f_{i.}},...,\\frac{f_{ij}}{f_{i.}})\\). In matrix terms, we will set up \\(D_n=diag(f_{i.})\\) such that \\(L = D_n^{-1}F\\). We can then define the average row profile, obtained by summing the columns of our frequency table :\n\\[\n(\\sum_{i=1}^nf_{i.}\\frac{f_{i1}}{f_{i.}},...,\\sum_{i=1}^nf_{i.}\\frac{f_{ij}}{f_{i.}})=(f_{.1},...,f_{.j})\n\\]\nColumn profiles\nThe column profile is \\(C_j=(\\frac{f_{1j}}{f_{.j}},...,\\frac{f_{ij}}{f_{.j}})\\). As with row profiles, it can be expressed and calculated in matrix terms by setting: \\(D_p=diag(f_{.j})\\), such that \\(C=D_p^{-1}F^T\\). We can then define the average column profile by summing the rows of our frequency table:\n\\[\n(\\sum_{j=1}^pf_{.j}\\frac{f_{1j}}{f_{.j}},...,\\sum_{j=1}^pf_{.j}\\frac{f_{ij}}{f_{i.}})=(f_{1.},...f_{i.})\n\\]\n\n\n\n- Distances between profiles\nOne can measure the distance between two row profiles by\\[d^2(i,i')=\\sum_{j=1}^p(\\frac{f_{i j}}{f_{i.}}-\\frac{f_{i'j}}{f_{i'.}})^2\\]\nBut this distance does not take into account the importance of each column. A more sensible choice is to use the \\(\\chi^2\\) distance, which does take the importance of each column into consideration.\n\\(d^2(i,i')=\\sum_{j=1}^p\\frac{1}{f_{.j}}(\\frac{f_{i j}}{f_{i.}}-\\frac{f_{i'j}}{f_{i'}})^2\\)\nLet’s use the notation \\(x\\) to refer to the vector of distances between two rows \\(L_{11}-L_{21}\\) or between two columns \\(C_{11}-C_{12}\\). Adopting matrix notation, the square distance of the \\(\\chi^2\\) is of the form:\n\\[\nx^TD_p^{-1}x = \\sum_{j=1}^p\\frac{x^2_j}{f_{.j}}\n\\]for a row point \\(x\\ \\epsilon\\ \\mathbb{R}^n\\) .\n\n\n- Independence\nIn the theory of tests, the objective is to determine whether there is a link between the two qualitative variables. The null hypothesis \\(H_0\\) is the independence between the two variables (no effect). The alternative hypothesis \\(H_1\\) is the presence of a link between the two variables.\nThe association between the two categorical variables is assessed through the discrepancy between the observed data and the independence model.This independence model is calculated based on the following equation:\n\\[\nP(A\\ \\cap\\ B) = P(A) \\times P(B)\n\\]Therefore, the joint probability is the product of the marginal probabilities:\n\\[\nf_{ij} = f_{i.} \\times f_{.j}\n\\]One can then calculate the discrepancy between the observed data \\((f_{ij})\\) and the independence model:\n\nThe significance of the association is measured with a \\(\\chi^2\\) test\n\\[\n\\chi_{obs}^2 = \\sum_{i=1}^I\\sum_{j=1}^J\\frac{(obs.headcount - th.headcount)^2}{th.headcount}=\\sum_{i=1}^I\\sum_{j=1}^J\\frac{(n\\times f_{ij} - n\\times f_{i.}\\times f_{.j})^2}{n\\times f_{i.}\\times f_{.j}}\n\\]\nThe strength of the association \\(Ø^2\\) is measured by the discrepancy between theoretical and observed probabilities.\n\n\\[\nØ^2 = \\sum_{i=1}^I\\sum_{j=1}^J\\frac{(obs.prob - th.prob)^2}{th.prob}\n\\]\n\n\n- Binary Correspondence Analysis\nBy conducting a correspondence analysis, we aim to simultaneously represent the row profiles belonging to \\(\\mathbb{R}^p\\) and the column profiles belonging to \\(\\mathbb{R}^n\\) of a frequency table. As this representation is done in the Cartesian plane, it will be obtained through a double principal component analysis.\nThe two principal component analyses are not directly performed on the variables but rather on the row profiles (direct analysis) and the column profiles (dual analysis) presented earlier. Moreover, binary correspondence analysis incorporates the notion of column (or row) weights and the \\(\\chi^2\\) distance.\n\n- Direct Analysis (Rows)\nThe direct analysis is performed on the row profiles. The rows of \\(L = D_n^{-1}F\\) are elements of \\(\\mathbb{R}^p\\). We aim to represent them in this space using the distance function \\(x^TD_p^{-1}x\\).\nIn the direct analysis, we seek the vector \\(u\\ \\epsilon\\ \\mathbb{R}^p\\) such as :\n\\[\n(u^TD_p^{-1}F^TD_n^{-1})D_n(D_n^{-1}FD_p^{-1}u)\n\\]\nWith \\(F\\) : the observed frequency matrix\nis maximal, given that \\(u^TD_p^{-1}u=1\\).\nThe solution is given by the principal eigenvector of\n\\[\nD_pD_p^{-1}F^TD_n^{-1}FD_p^{-1}=F^TD_n^{-1}FD_p^{-1}\\equiv S\n\\]\n\n\n- Dual Analysis (Columns)\nDual analysis is performed on the column profiles. The rows of \\(C = D_p^{-1}F^T\\) are elements of \\(\\mathbb{R}^n\\). We aim to represent them in this space using the distance function \\(x^TD-n^{-1}x\\).\nIn dual analysis, we seek the vector \\(v\\ \\epsilon\\ \\mathbb{R}^n\\) such as\n\\[\n(v^TD_n^{-1}FD_p^{-1})D_p(D_p^{-1}F^TD-n^{-1}v)\n\\]is maximal, given that \\(v^TD_n^{-1}v=1\\).\nThe solution is given by the principal eigenvector of\n\\[\nD_n(D_n^{-1}FD_p^{-1}F^TD_n^{-1})=FD_p^{-1}F^TD_n^{-1}\\equiv T\n\\]\n\n\n\n- Projection of both analyses onto the same plane\nThe usual goal of correspondence analysis is to produce a 2-dimensional graph that summarizes the information contained in the frequency table and highlights different interesting associations. In other words, we want to somehow overlay the figures resulting from dual and direct analyses.\nTo present both analyses in the same plane, it is essential to ensure that both PCAs project the data into the same dimensions. The transition relationships and the concept of the center of gravity presented in the following sections allow us to ensure that we are indeed projecting the “individuals” from dual and direct analyses into the same dimensions.\n\n- Transition Relationships\nThe following relationships\n\\[\nD_n^{-1}F\\varphi_j=\\sqrt{\\lambda_j}\\Psi_j\\equiv \\hat{\\Psi}_j\n\\]\n\\[\nD_p^{-1}F^T\\Psi_j=\\sqrt{\\lambda_j}\\varphi_j\\equiv \\hat{\\varphi}_j\n\\]\nWith \\(\\varphi_j\\) : the eigenvector associated with the eigenvalue \\(\\lambda_j\\) of the matrix F; \\(\\Psi_j\\) : the modality coordinates of the row variable in the row space defined by the eigenvalues; \\(\\hat{\\Psi}_j\\) the contrinution of the component \\(j\\) to the row variable in the row space and \\(\\hat{\\varphi}_j\\) the contrinution of the component \\(j\\) to the column variable in the column space\nestablish an almost barycentric link between the two types of analysis, meaning that the coordinates of points in one space are proportional to the components of the factor from the other space corresponding to the same eigenvalue.\nVerification: The first relationship is confirmed as follows.\n\\[\nD_n^{-1}F\\varphi_j=D_n^{-1}(FD_p^{-1}u_j) = D_n^{-1}(\\sqrt{\\lambda_j}v_j)=\\sqrt{\\lambda_j}\\Psi_j\n\\]The second identity is demonstrated in a similar manner.\nThe consequences of the preceding transition relationships are that…\n\n\\(1=\\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_p \\geq 0\\)\n\\((f_1,…,f_p)^T\\) is an eigenvector associated with the eigenvalue \\(\\lambda_1=1\\) of \\(S=F^TD_n^{-1}FD_p^{-1}\\).\nNote that since the first eigenvalue is always equal to one, many software packages only mention the other \\(p-1\\) eigenvalues.\n\n\n\n- Center of Gravity\nTo project the observations onto the same plane, we need to determine the origin of the graph.\nThe center of gravity of the rows is defined by \\(G_L=(g_1,…,g_p)^T\\), where\n\\[\ng_j=\\sum_{i=1}^nf_i\\frac{f_{ij}}{f_i}=\\sum_{i=1}^nf_{ij}=f_j, 1\\leq j \\leq p\n\\]Similarly, the center of gravity of the columns is defined by\n\\[\nG_C=(f_1,...,f_n)^T\n\\]Centering of the rows is obtained by calculating\n\\[\n\\frac{f_{ij}}{f_i}-g_j=\\frac{f_{ij}}{f_i}-f_j=\\frac{f_{ij}-f_if_j}{f_i}\n\\]so that \\(\\sum_{j=1}^p\\frac{f_{ij}-f_if_j}{f_i}=0\\)\nfor all \\(i\\ \\epsilon\\) \\({1,…,n}\\).\nThe consequence of centering the rows is that the analysis is no longer conducted on \\(S=F^TD_n^{-1}FD_p^{-1}\\) but rather on \\(S*=(s^*_{jj'})\\), where\n\\[\ns^*_{jj'}=\\sum_{i=1}^n\\frac{(f_{ij}-f_if_j)(f_{ij'}-f_if_j)}{f_if_{j'}}\n\\]To detect associations between rows and columns, one must establish a connection with the statistic of the \\(\\chi^2\\) test.\nBy definition, \\(trace(S*)=\\sum_{j=1}^p\\sum_{i=1}^n\\frac{(f_{ij}-f_if_j)^2}{f_if_j}\\)"
  },
  {
    "objectID": "AMV_chapter.html#interpretation-1",
    "href": "AMV_chapter.html#interpretation-1",
    "title": "Multivariate Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere’s an example of how CA can be applied.\nYou can import R package using the code\n\n# Creating the dataset :\n## Step 1: Create eye and hair color vectors\ncolors_of_eyes &lt;- c(\"Blue\", \"Brown\", \"Green\", \"Gray\", \"Black\", \"Yellow\")\nhair_colors &lt;- c(\"Black\", \"Brown\", \"Blond\", \"Redhead\", \"Chestnut\", \"Blues\", \"White\", \"Pink\")\n\n## Step 2: Create a data array with fixed values\nfixed_values &lt;- matrix(c(\n  50, 35, 32, 24, 0, 0,\n  15, 84, 34, 18, 16, 0,\n  7, 4, 1, 20, 10, 0,\n  0, 18, 0, 21, 34, 6,\n  5, 3, 1, 0, 0, 3,\n  8, 4, 2, 5, 13, 24,\n  9, 10, 0, 12, 0, 10,\n  1, 20, 5, 46, 2, 5\n), nrow = length(hair_colors), ncol = length(colors_of_eyes))\n\n## Store the matrix in an array variable : \ndata &lt;- as.data.frame(fixed_values)\n\n## Name rows and columns\ncolnames(data) &lt;- colors_of_eyes\nrownames(data) &lt;- hair_colors\n\n\ndata\n\n         Blue Brown Green Gray Black Yellow\nBlack      50    34    10    5     2      0\nBrown      35    18     0    3     5     10\nBlond      32    16     0    1    13      1\nRedhead    24     0    18    0    24     20\nChestnut    0     7     0    0     9      5\nBlues       0     4    21    3    10     46\nWhite      15     1    34    8     0      2\nPink       84    20     6    4    12      5\n\n\nThis table shows the number of people with each eye color (in columns) and hair color (in rows), for a total of 617 people sampled.\nBy performing a CA on this dataset, we seek to show the correspondences/oppositions between the different modalities of a variable.\n\na) Performing CA :\nTo run the CA, such as the PCA, you need to load the following two packages: factoextra and FactoMineR.\nThen, to perform the CA on R, you can use the function : CA(). Remember to store the result of this PCA in a new variable, so that you can easily retrieve the PCA information you need for subsequent interpretation.\n\n\n\n\n\nIn a CA, rows are called rows and columns called col.\n\n\nb) Interpretation of outputs :\n\n1) Inertia and choice of axes :\n\n# Recover the eigenvalues of each axis and the inertia carried by the principal components\nCA$eig\n\n       eigenvalue percentage of variance cumulative percentage of variance\ndim 1 0.415178377             55.3969063                          55.39691\ndim 2 0.215052153             28.6942303                          84.09114\ndim 3 0.073125570              9.7570841                          93.84822\ndim 4 0.043902740              5.8579062                          99.70613\ndim 5 0.002202466              0.2938732                         100.00000\n\n# Graphical display of the inertia of each axis : \nfviz_eig(CA, addlabels = TRUE)\n\n\n\n\nTo determine the number of axes used in the CA analysis, we need to identify the jump in variance explained by the different axes. It’s the same as for PCA. Here we can see that the first axis represents 55.4% of the variance, the second axis 28.7%, the third axis 9.8%, …\nWe can therefore see that the jump in variance explained by the different axes is between the first and second axes. The difference between the variance explained by the axes other than the first is negligible compared to the difference between the first axis and the others.\nFor the CA interpretation, we therefore retain the first two axes, which together explain 65.19% of the variance.\n\n\n2) Interpretation of the biological meaning of the axes :\nIn CA, in contrast to PCA, axes are interpreted either column-wise or row-wise. The choice depends on the initial biological question.\nHere we’ll interpret the axes according to the lines, i.e. according to the hair colors. To find out which variables contribute to the synthetic axis, we use the threshold of : \\(\\frac{1}{nb\\_variables}\\)\nThe threshold in our case is: \\(\\frac{1}{8}\\).\nTo obtain this information in R, use the following commands:\n\n# Obtaining absolute line contribution values : \nRow_contrib=CA$row$contrib\nRow_contrib&gt;(1/8)*100\n\n         Dim 1 Dim 2 Dim 3 Dim 4 Dim 5\nBlack     TRUE FALSE  TRUE FALSE  TRUE\nBrown    FALSE FALSE FALSE FALSE  TRUE\nBlond    FALSE FALSE FALSE FALSE FALSE\nRedhead  FALSE FALSE  TRUE FALSE  TRUE\nChestnut FALSE  TRUE FALSE  TRUE FALSE\nBlues     TRUE FALSE  TRUE FALSE FALSE\nWhite    FALSE  TRUE FALSE FALSE FALSE\nPink      TRUE FALSE FALSE  TRUE FALSE\n\n\nAccording to these results, the lines explaining the first axis are :\n\nBlack, Blues, Pink\n\nThe lines explaining the second axis are :\n\nChestnut, White\n\n\n\n3) Interpretation of relative contributions :\nRelative contributions represent the quality of representation of the column/row by the axis.\nEither the rows or the columns are interpreted, depending on the choice made in the next step. In our case, we chose the columns, since we interpreted the axes with the absolute contributions of the rows.\n\n# Obtain relative column contribution values :\nCA$col$cos2\n\n            Dim 1      Dim 2      Dim 3       Dim 4        Dim 5\nBlue   0.85575259 0.01448471 0.00950636 0.120205825 5.051631e-05\nBrown  0.57556811 0.05455375 0.17954769 0.189337969 9.924835e-04\nGreen  0.45208660 0.52672933 0.01096464 0.008743165 1.476265e-03\nGray   0.01081361 0.82960368 0.08394864 0.020223487 5.541059e-02\nBlack  0.06314083 0.46740988 0.41166914 0.056819078 9.610773e-04\nYellow 0.74157647 0.18279275 0.05802235 0.017605743 2.692628e-06\n\n\nBlue and yellow eye colors are well represented by axis 1. Grey eye color is represented by axis 2.\n\n# Relative line contribution values : \nCA$row$cos2\n\n              Dim 1       Dim 2      Dim 3       Dim 4        Dim 5\nBlack    0.69781032 0.070862823 0.15315848 0.066726581 1.144180e-02\nBrown    0.52754846 0.167378322 0.26792495 0.020035304 1.711296e-02\nBlond    0.69002130 0.182751826 0.08463182 0.041935390 6.596638e-04\nRedhead  0.49239795 0.083209727 0.41251121 0.007108659 4.772456e-03\nChestnut 0.03638017 0.564361623 0.02919618 0.365627649 4.434383e-03\nBlues    0.87482351 0.049760467 0.07057401 0.004824928 1.708337e-05\nWhite    0.15364853 0.827793121 0.00721218 0.010354558 9.916087e-04\nPink     0.78192585 0.000285946 0.02156349 0.195417315 8.073975e-04\n\n\nThis verifies that the modalities that contribute most to the formation of the first two axes all have a good representation quality (not always the case).\n\n\n\nc) Biological conclusion :\nFor the question of links between modalities in rows or columns, you need to know that :\n\nIf we have a positive scalar product, i.e. less than 90 degree difference between the arrows of two different modalities. Then we have a conjunction representing an affinity between these two modalities.\nIf we have a negative scalar product, i.e. more than 90? difference between the arrows of two different modalities. Then we have an opposition of these two modalities. They reject each other.\nIf we have a zero scalar product, i.e. the arrows between two different modalities are perpendicular. Then there is no link between these two modalities.\n\n\nfviz_ca_biplot(CA)\n\n\n\n\nOn this graph, the rows are shown in blue and the columns in red. In other words, hair color in blue and eye color in red.\nFrom this analysis, we can see :\n\nA conjunction between black, pink, brown or blond hair and blue or brown eyes.\nA conjunction between yellow eyes and blue hair\nAn opposition between these two groups on axis 1.\n\nIn an analysis where the data are not created randomly, but come from different individuals from different populations. This could reveal a particular typology of one population in relation to another."
  },
  {
    "objectID": "AMV_chapter.html#introduction-2",
    "href": "AMV_chapter.html#introduction-2",
    "title": "Multivariate Analysis",
    "section": "Introduction",
    "text": "Introduction\nMCA is a statistical method adapted to table of type “individuals x quatitative variable”. Eingen Values correspond to means of squared correlation ratios. It could be used a pre-processing before a classification or a coinertia analysis on tables with quantitative."
  },
  {
    "objectID": "AMV_chapter.html#mathematics-2",
    "href": "AMV_chapter.html#mathematics-2",
    "title": "Multivariate Analysis",
    "section": "Mathematics",
    "text": "Mathematics\nThe MCA stands out from other multivariate analyzes with a single table (PCA, CA or mix between MCA and PCA, etc.) because it only takes qualitative variables as input. Its goal is to find the relationships between modalities by visualizing the possible associations and producing a quantitative indicator of their relationships.\nThe data taken by an MCA is a table comprising \\(J\\) qualitative variables to describe \\(I\\) statistical individuals. All individuals have the same weight \\(\\frac{1}{I}\\). The value \\(V_{i,j}\\) in the table corresponds to the modality taken by individual \\(i\\) for variable \\(j\\).\nTo obtain a complete disjunctive table (CDT) and carry out the statistical analysis, it is necessary to subdivide each variable \\(j\\) into \\(n_{j}\\) modalities corresponding to all of the responses \\(V_{.,j}\\) of the \\(I\\) individuals for this variable. The table will always have \\(I\\) rows but \\(\\sum_{j=1}^J {n_j}\\) columns (we will denote K columns), in which each of the \\(V_{i,k}\\) values is either 1 or 0 depending on the presence or absence of the modality.\nThe value \\(V_{i,k}\\) is transformed again : the old value of \\(V_{i,k}\\) (1 or 0) is divided by the probability \\(p_{k}\\) of having this modality (i.e. \\(p_k=\\frac{\\sum_{i=0}^I V_{i,k}}{I}\\), result from which we subtract 1 to center the result.\n\\[V'_{i,k}=\\frac{\\sum_{i=1}^I V_{i,k}}{I}-1\\]\nWe define the distance D between 2 individuals \\(i\\) and \\(i’\\) as : \\[D=\\frac{1}{J}\\sum_{k=1}^K \\frac{1}{p_{k}}(V'_{i,k}-V'_{i',k})^2\\]\nSo 2 individuals taking the all same modalities are at a distance D = 0, the more similar the responses are over a large number of modalities, the lower the distance D will be and conversely. If two individuals share a rare modality, the distance will be reduced to take into account the common specificity.\nThe inertia of the point cloud : \\(N = \\frac{K}{J} – 1\\). \\(η^2(x,y)\\) is the correlation ratio between 2 variables \\(x\\) and \\(y\\) and \\(η^2(F_s,k)\\) is the correlation ratio between the modality \\(k\\) and l’axe \\(F_s\\). The axis \\(F_1\\) is the one which maximize its correlation ratio with all the modalities \\(k\\) : \\[F_1=\\max(\\sum_{k=1}^K η^2(F_s,V_{.,k}))\\]"
  },
  {
    "objectID": "AMV_chapter.html#interpretation-2",
    "href": "AMV_chapter.html#interpretation-2",
    "title": "Multivariate Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThis part is a simple example of MCA using R. You can import R package using the code. Let’s have a look to fictive the data set we will be working on :\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhippo &lt;- read.table(file = \"https://raw.githubusercontent.com/AnsaldiL/MODE_reproduciblescience/master/hp.csv\", sep=';', header=TRUE)\nhippo=hippo[,-1]\nstr(hippo)\n\n'data.frame':   143 obs. of  4 variables:\n $ Age      : int  20 21 23 24 23 35 26 22 22 20 ...\n $ Sex      : chr  \"F\" \"M\" \"F\" \"F\" ...\n $ Frequency: chr  \"regularly\" \"rarely\" \"regularly\" \"regularly\" ...\n $ Lake     : chr  \"G1\" \"G3\" \"G1\" \"G3\" ...\n\n\nWater consumption behaviour of Hippopotamus was observed in Penjari National Park, Benin. Drinking frequency was evaluated by a technician and rated “rarely” or “regularly”. Sex is indicated with F for female and M for male. As there are 3 lakes in the park which are noted “G1”, “G2” and “G3”.\nWe are performing MCA with the package FactoMineR.\nLet’s have a look to Eigen Values :\n\nres.mca$eig\n\n      eigenvalue percentage of variance cumulative percentage of variance\ndim 1  0.4348479               32.61359                          32.61359\ndim 2  0.3391468               25.43601                          58.04960\ndim 3  0.3304681               24.78511                          82.83470\ndim 4  0.2288706               17.16530                         100.00000\n\nfviz_eig(res.mca, addlabels = TRUE)\n\n\n\n\nThe total variance of the data set is divided between four dimensions. The first dimension concentrates 32.6% of the total variance. The first plan gathers 58% of the variance. You can better visualize the repartition of variance with the plot.\nHere, you can see the results for the individuals, their coordinates (\\(coord\\)), absolute contributions (\\(contrib\\)), and the quality of their projection (\\(cos2\\)).\n\nhead(res.mca$ind$coord)\n\n       Dim 1      Dim 2        Dim 3      Dim 4\n1 -0.2750115 -0.3282379  0.953675653  0.2177425\n2  1.1910863 -0.2970654 -0.817927621  0.0463482\n3 -0.2750115 -0.3282379  0.953675653  0.2177425\n4 -0.5234147 -0.6299231 -0.447176405  0.1119000\n5 -0.2750115 -0.3282379  0.953675653  0.2177425\n6 -0.6770613  0.6031855  0.005825436 -0.2073689\n\nhead(res.mca$ind$cos2)\n\n       Dim 1      Dim 2        Dim 3        Dim 4\n1 0.06632695 0.09448565 7.976083e-01 0.0415790594\n2 0.65134489 0.04051620 3.071527e-01 0.0009862576\n3 0.06632695 0.09448565 7.976083e-01 0.0415790594\n4 0.31017449 0.44925117 2.263977e-01 0.0141766670\n5 0.06632695 0.09448565 7.976083e-01 0.0415790594\n6 0.52978429 0.42047948 3.921931e-05 0.0496970050\n\nhead(res.mca$ind$contrib)\n\n      Dim 1     Dim 2        Dim 3       Dim 4\n1 0.1216265 0.2221539 1.924579e+00 0.144863948\n2 2.2814613 0.1819619 1.415677e+00 0.006563565\n3 0.1216265 0.2221539 1.924579e+00 0.144863948\n4 0.4405736 0.8181846 4.231479e-01 0.038259024\n5 0.1216265 0.2221539 1.924579e+00 0.144863948\n6 0.7371954 0.7502017 7.181106e-05 0.131389632\n\n\nCoordinates of the individuals are their position on the first plan. \\(cos2\\) represents the quality of the representation of the individuals on the first plan. Contribution is how the point contributes to the creation of different axis.\nYou can access the same information for the variable (instead of the individuals):\n\nres.mca$var$coord\n\n                Dim 1       Dim 2       Dim 3      Dim 4\nF         -0.47746388 -0.21701783  0.15112124 -0.3434173\nM          1.14818694  0.52187620 -0.36341060  0.8258369\nrarely     1.23505546 -0.11004338 -0.08731625 -0.8834510\nregularly -0.53107385  0.04731865  0.03754599  0.3798839\nG1         0.46448522 -0.40376182  1.45603286  0.2760402\nG2        -0.33088584  1.22351803 -0.17862073 -0.3340851\nG3        -0.02692834 -0.93083264 -0.95986346  0.1241336\n\nres.mca$var$contrib\n\n                Dim 1      Dim 2       Dim 3      Dim 4\nF         12.34263168  3.2693937  1.62699252 12.1316114\nM         29.68109047  7.8621135  3.91252964 29.1736370\nrarely    35.15982729  0.3578914  0.23124469 34.1811065\nregularly 15.11872574  0.1538933  0.09943522 14.6978758\nG1         4.62603750  4.4819347 59.81574581  3.1042541\nG2         3.05186316 53.5031214  1.17025431  5.9111247\nG3         0.01982416 30.3716520 33.14379781  0.8003904\n\nres.mca$var$cos2\n\n                 Dim 1       Dim 2       Dim 3       Dim 4\nF         0.5482177869 0.113256440 0.054919059 0.283606714\nM         0.5482177869 0.113256440 0.054919059 0.283606714\nrarely    0.6559056566 0.005207104 0.003278375 0.335608865\nregularly 0.6559056566 0.005207104 0.003278375 0.335608865\nG1        0.0837850567 0.063310139 0.823313280 0.029591524\nG2        0.0625631068 0.855426501 0.018231638 0.063778755\nG3        0.0004019773 0.480314347 0.510741640 0.008542035\n\n\nLet’s have a look to the plot of this MCA analysis :\n\nplot(res.mca)\n\n\n\n\nAs this is a factice data set, some individuals are overlapping. We will not focus on this artefact of the data set.\nHow to interprete these graphs?\nGeneral description : The individuals are in black. The variables are in red. The percentage of variance of the first two axis is written on them. Individuals in the center of the cloud are individuals taking a mean value for all of their caracteristics, unlikely individuals far from the middle which are specific individuals, very different from others. Close individuals present close characteristics.\nInterpretation: Here, we can see that the first axis separates individuals drinking regularly from individuals drinking rarely. The second axis separates individuals drinking at G2 from individuals drinking at G3. Male and female seem to be separated by the first axis. As we can gather individuals sharing close properties, females seems to drink regularly and male more rarely. It is a bit less clear but male are drinking preferably in pound 1.\nNote that supplementary quantitaive data could be added to the analysis to help interpretation. As they are quantitative, they would not participate to the creation of axis but they may be ploted on the final graph."
  },
  {
    "objectID": "AMV_chapter.html#introduction-3",
    "href": "AMV_chapter.html#introduction-3",
    "title": "Multivariate Analysis",
    "section": "Introduction",
    "text": "Introduction\nTwo table analysis is used when the scientist possesses two data set with expecting the second to explain the first. The first one is called Y, the response variable and the second one is called X, the explanatory variable. For example :\nTable Y | Table X\na data set with temperature data | a data set with cities caracteristics such as heigth of buildings, concreted area, number of cars… |\na data set with the quantity of chemicals in soil | a data set with % of pesticide applied |\na data set with functional traits of an animal | a data set with the quantities of different food given to the animal |\nRDA is used when expecting a linear response from \\(X\\) to \\(Y\\). Only the variables of \\(X\\) explaining \\(Y\\) would be kept. The canonical axis are a linear combinaison of the explicative variable (\\(Y\\)). The \\(Y\\) table will be ordered with a PCA. Then, a multiple regression of \\(Y\\) on \\(X\\) is done and stored in a new table, \\(Y'\\). The residual matrix correspond to \\(Y-Y'\\), what was not explained by \\(X\\)."
  },
  {
    "objectID": "AMV_chapter.html#interpretation-3",
    "href": "AMV_chapter.html#interpretation-3",
    "title": "Multivariate Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nHere are the library you will need to perfor the RDA, make sure your import them at the begining of your working document.\nLet’s import the data, they are from @thilenius1963synecology. However, the data of abundance have been changed to facilitated the interpretation. For access to real data and metadata, please see the JDBakker Git page: appliedmultivariatestatistics/Oak_data_47x216.csv at main · jon-bakker/appliedmultivariatestatistics (github.com)\n\nOak &lt;- read.csv(\"https://raw.githubusercontent.com/AnsaldiL/MODE_reproduciblescience/master/Oak_data.csv\", sep=';', header = TRUE) \n\nTopo &lt;- Oak %&gt;%\nselect(LatAppx, LongAppx, Elev.m, Slope.deg) %&gt;%\ndecostand(\"range\")\n\nOak_sp=Oak[,28:83]\n\nWe can explore the tables\n\nhead(Oak_sp)\n\n  Abgr.s Abgr.t Acar Acgld.t Acma.s Acma.t Acmi Adbi Agha Agre Agse Agte Aica\n1      0      0    0       0      0      0    0    0    0    0    0    1    1\n2      5      0    0       0      1      0    0    0    0    0    0    0    0\n3      4      0    2       0      1      0    0    0    0    0    0    0    0\n4      0      0    0       0      0      0    0    0    0    0    0    0    1\n5      0      0    0       0      0      0    0    0    0    0    0    0    0\n6      0      0    0       1      0      0    0    0    0    0    0    0    0\n  ALL Alpr Amal.s Amal.t Apan Aqfo Arel Arme.s Arme.t Avfa Beaq.s Brco Brla\n1   0    2      0      0    0    0    0      0      0    0      0    0    1\n2   0    0      1      0    0    0    0      0      0    0      0    0    0\n3   0    0      0      0    0    0    0      0      0    0      0    0    0\n4   0    0      0      0    0    2    0      0      0    0      0    0    0\n5   0    0      0      0    0    0    1      0      0    0      0    0    0\n6   0    0      0      0    0    1    0      0      0    0      0    0    0\n  Brpu Brri Brse Brst Brvu Caqu CAR Cato Cear Ceum Ceve.s Cipa Civu Coco.s\n1    1    0    0    0    0    1   0    0    0    0      0    0    0      0\n2    0    0    0    0    0    1   0    0    0    0      0    0    0      0\n3    0    0    0    0    0    0   0    0    0    0      0    0    0      0\n4    1    0    0    0    0    3   0    0    0    1      1    0    0      0\n5    0    0    0    0    0    1   0    0    0    0      1    0    0      0\n6    0    0    0    0    0    0   0    0    0    0      1    0    0      0\n  Coco.t Cogr Conu.s Conu.t CORY.t Cost Crca Crdo.s Crdo.t Crox.s Cyec Cyfo\n1      0    0      0      0      0    0    0      0      0      0    0    0\n2      0    0      0      0      0    0    0      0      0      0    0    0\n3      0    0      0      0      0    0    0      0      0      0    0    0\n4      0    0      0      0      0    0    0      0      0      0    0    0\n5      0    0      0      0      0    0    0      0      0      0    0    0\n6      0    0      0      0      0    0    0      0      0      0    0    0\n  Cygr Daca Dacar Dagl\n1    0    0     0    0\n2    0    0     0    0\n3    0    0     0    0\n4    0    0     0    0\n5    0    0     0    0\n6    0    0     0    0\n\nhead(Topo)\n\n    LatAppx  LongAppx     Elev.m  Slope.deg\n1 0.3882609 1.0000000 0.06578947 0.03448276\n2 0.5073864 0.6239120 0.13157895 0.24137931\n3 0.5750381 0.7687530 0.33333333 0.20689655\n4 0.7392188 0.6721803 0.06578947 0.17241379\n5 0.8261238 0.9377551 0.33333333 0.34482759\n6 0.3309702 0.9046193 0.06578947 0.17241379\n\n\nSo here, the first table (“Oak_sp”) represents a table of plant abundance for 47 sites (47 rows). We have 56 differents species. The second table (“Topo”) corresponds to 4 topographics variables (Latitute, Longitute of sites, the elevation and the slope). As it is not the same unit for each variable we have centred-reduced these four variables.\nAs explained above, the CCA objective is to link the abundance of plant by the topographic variables. Here, we have a priori hypothesis on the effect of these environmental variables on the plants.\nWe have seen in introduction of the part 3 that CCA was used when the respond have a non-linear distribution but rather a gaussian relationship.\nSo we begin by performing a CA on the species table:\n\ncaoak &lt;- dudi.coa(Oak_sp,scannf=F,nf=2)\n\nThen, we can perform the PCA on the CA that corresponds to the CCA with the ADE4 package:\n\nccaoak &lt;- pcaiv(caoak,Topo,scannf=F,nf=2)\n\nYou can also realize the CCA with one code row only with the vegan package:\n\nccavegan=cca(Oak_sp,Topo,scan=F)\n\nIt is important to use the two methods because each allows us to have different information during the analyse of the result\nAnd we already have completed our analyse, it was no difficult! But now, we need to analyse the result and test the significant.\nTo test the significant, we use a bootstrapping test thank you the ADE4 package.It is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between are different tables. To produce this, we perform a permutation test with the function randtest.\n\nrandtest(ccaoak)#with ADE4\n\nMonte-Carlo test\nCall: randtest.pcaiv(xtest = ccaoak)\n\nObservation: 0.1270957 \n\nBased on 99 replicates\nSimulated p-value: 0.01 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n2.6491375067 0.0889501198 0.0002073378 \n\nplot(randtest(ccaoak))\n\n\n\n\nThe outputs above correspond to the permutation test. We see that the number of permutations of columns and rows was to 999 (default value).\nThe results of this test show that our result is significatively different to the permutation result with a threshold of 5% (p-value = 0.02)\nFinally, the results of the CCA are plot in the distribution law calculated by the permutation. So we can conclude that our environmental variables explain a part of the plant distribution.\nWe have now to look the collinearity between the environmental variables. It corresponds to the correlation between them. For that, we use the Variance Inflation Factor (VIF).\n\ntest=vif.cca(ccavegan) \ntest\n\n  LatAppx  LongAppx    Elev.m Slope.deg \n 1.072988  1.115539  1.169013  1.111402 \n\n\nIf all the variables have a number lower than 10, you can conclude that you do not not have collinearity. If you have, you need to select some variables to remove with, for example, an ordistep function.\nNow that we have tested all the conditions, we can look at the results.\n\nccaoak\n\nCanonical correspondence analysis\ncall: pcaiv(dudi = caoak, df = Topo, scannf = F, nf = 2)\nclass: caiv pcaiv dudi \n\n$rank (rank)     : 4\n$nf (axis saved) : 2\n\neigen values: 0.3446 0.179 0.1231 0.05905\n\n vector length mode    content                \n $eig   4      numeric eigen values           \n $lw    47     numeric row weigths (from dudi)\n $cw    56     numeric col weigths (from dudi)\n\n data.frame nrow ncol content                             \n $Y         47   56   Dependant variables                 \n $X         47   4    Explanatory variables               \n $tab       47   56   modified array (projected variables)\n\n data.frame nrow ncol content                               \n $c1        56   2    PPA Pseudo Principal Axes             \n $as        2    2    Principal axis of dudi$tab on PAP     \n $ls        47   2    projection of lines of dudi$tab on PPA\n $li        47   2    $ls predicted by X                    \n\n data.frame nrow ncol content                                  \n $fa        5    2    Loadings (CPC as linear combinations of X\n $l1        47   2    CPC Constraint Principal Components      \n $co        56   2    inner product CPC - Y                    \n $cor       4    2    correlation CPC - X                      \n\nprint(paste(\"The pourcentage of variance explained by topographic variables is: \",sum(ccaoak$eig)/sum(caoak$eig),\"%\"))\n\n[1] \"The pourcentage of variance explained by topographic variables is:  0.127095660396168 %\"\n\n\nThe first output (by ADE4) allows us to see each part of the analyse that could be use to describe the result. We will use especially the last part afterward. You also have, at the beginning, the number of rank that corresponds to the number of eigenvalue with their score below. We can calculate the percentage of variance explained by topographic variables.\n\nccavegan\n\nCall: cca(X = Oak_sp, Y = Topo, scan = F)\n\n              Inertia Proportion Rank\nTotal          5.5529     1.0000     \nConstrained    0.7057     0.1271    4\nUnconstrained  4.8471     0.8729   38\nInertia is scaled Chi-square \n17 species (variables) deleted due to missingness\n\nEigenvalues for constrained axes:\n  CCA1   CCA2   CCA3   CCA4 \n0.3446 0.1790 0.1231 0.0590 \n\nEigenvalues for unconstrained axes:\n   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 \n0.7256 0.5739 0.4521 0.4240 0.3563 0.2448 0.2239 0.1857 \n(Showing 8 of 38 unconstrained eigenvalues)\n\n\nHere, it is the output with the vegan package. We can see the total inertia in the CCA analyse (5.55) and the inertia explained by the topographic variables (0.71) and by the residuals (4.85). We can therefore conclude that our topographic variables explain only 13% of the plant variances (same result that before). Below that, we have the eigenvalues for the constrained axes (variance part explained by topographic variables) and for the unconstrained axes (variance part explained by the residuals)\nWe can now, observe the complet plot of the CCA result:\n\nplot(ccaoak)\n\n\n\n\nHere, we can have a complete plot with the ADE4 package.\n\nThe “Loadings” part represents the canonical coefficients. For each axis, the arrows explain the relative weight of the topographic variables in the multiple regression calculation.\nThe “Correlation” part shows the correlation between topographic variables themselves and with CCA axes\nThe “Scores and Predictions” part allows us to understand, for each site, the real plant abundance (base of arrows) and the abundance predicted by multiple regressions with the topographic variables.\nThe “Species” part corresponds to the coordinates of each plant species in the CCA analyse\n\nBut this can also be decomposed in different plots and output.We can detail each part and observe the biological results.\nWe can begin by the absolute contribution of species:\n\ncontrib=inertia.dudi(ccaoak, col = TRUE, row = FALSE)\ncontrib$col.abs\n\n               Axis1        Axis2\nAbgr.s  6.021847e+00  1.212813788\nAbgr.t  3.177359e+01 10.422451480\nAcar    2.635049e+00  0.134855688\nAcgld.t 7.493068e+00  1.910539707\nAcma.s  2.515315e-02  1.689674506\nAcma.t  8.936196e-01  1.898419377\nAcmi    3.919403e-02  0.004891136\nAdbi    9.970574e+00  5.143409094\nAgha    8.209713e-01 17.598302739\nAgre    8.571153e-01  1.890912149\nAgse    1.299772e-01  8.924213094\nAgte    4.233778e+00  1.403202784\nAica    3.007289e+00  0.000614369\nALL     1.076835e-01  6.374919825\nAlpr    1.413603e-01  0.237212759\nAmal.s  5.831519e-02  0.289243172\nAmal.t  2.843090e-02  2.844815278\nApan    6.791621e+00  1.385883395\nAqfo    6.407529e+00 11.512960665\nArel    3.725579e+00  2.142662004\nArme.s  0.000000e+00  0.000000000\nArme.t  1.002592e-05  2.416236998\nAvfa    1.389095e-01  1.919937151\nBeaq.s  1.385684e+00  4.664898207\nBrco    0.000000e+00  0.000000000\nBrla    1.570857e+00  0.757779956\nBrpu    4.750564e-01  0.266026605\nBrri    8.163005e-01  1.287295464\nBrse    1.292897e-01  0.143416542\nBrst    0.000000e+00  0.000000000\nBrvu    4.429482e+00  0.016067778\nCaqu    1.841713e-02  1.290440906\nCAR     6.964536e-01  0.036573989\nCato    1.206790e+00  0.522162462\nCear    9.936004e-01  0.303130230\nCeum    8.493802e-01  0.392276059\nCeve.s  1.019629e+00  2.122835911\nCipa    1.182233e-04  1.942686629\nCivu    0.000000e+00  0.000000000\nCoco.s  0.000000e+00  0.000000000\nCoco.t  9.551865e-01  0.003571735\nCogr    5.754591e-02  1.919957503\nConu.s  0.000000e+00  0.000000000\nConu.t  0.000000e+00  0.000000000\nCORY.t  0.000000e+00  0.000000000\nCost    0.000000e+00  0.000000000\nCrca    0.000000e+00  0.000000000\nCrdo.s  0.000000e+00  0.000000000\nCrdo.t  5.203645e-02  0.143607303\nCrox.s  0.000000e+00  0.000000000\nCyec    0.000000e+00  0.000000000\nCyfo    0.000000e+00  0.000000000\nCygr    4.350974e-02  2.829101566\nDaca    0.000000e+00  0.000000000\nDacar   0.000000e+00  0.000000000\nDagl    0.000000e+00  0.000000000\n\n\nThe output, here, is the absolute contribution of each species for the axis 1 and the axis 2. This allows us to observe what species contributes the most of the axes.\n\nccaoak$fa\n\n                      RS1           RS2\n(Intercept) -1.448845e-16  1.304775e-16\nLatAppx      5.546487e-01 -1.321836e-01\nLongAppx     8.933635e-02  1.113464e-01\nElev.m      -2.378758e-02 -2.828042e-01\nSlope.deg    2.306181e-02  3.754766e-01\n\ns.corcircle(ccaoak$fa)\n\n\n\n\nThis line gives you the canonical coefficients on each axis. The circle is explain in the “Loadings” part of the main plot of the RDA. On axis 1, the variable with the main contribution of the predictive power is \\(LatAppx\\). On axis 2, \\(Slope.deg\\) and \\(Elev.m\\) contributes the most of the predictive power.\n\nccaoak$co\n\n               Comp1        Comp2\nAbgr.s  -0.344559610 -0.111446061\nAbgr.t  -1.499805193  0.619091450\nAcar    -0.405170072 -0.066061105\nAcgld.t -0.657447384 -0.239263999\nAcma.s   0.098964466 -0.584591822\nAcma.t  -0.527599532  0.554232875\nAcmi     0.093384329 -0.023775904\nAdbi    -1.970348894 -1.019944979\nAgha    -0.376925736  1.257752946\nAgre    -0.408495885 -0.437292359\nAgse     0.318149791  1.899989382\nAgte     0.560360412  0.232504722\nAica     0.510110670  0.005254841\nALL      0.204765928  1.135503753\nAlpr     0.135452232  0.126461872\nAmal.s  -0.213102669  0.342056435\nAmal.t  -0.210430331 -1.517079157\nApan     0.566164741  0.184326417\nAqfo     0.455971525 -0.440508302\nArel     0.525654360  0.287308446\nArme.s   0.000000000  0.000000000\nArme.t   0.002281469 -0.807217082\nAvfa     0.328900132  0.881271477\nBeaq.s   0.555259619  0.734265751\nBrco     0.000000000  0.000000000\nBrla     0.451534247  0.226027947\nBrpu     0.384680895  0.207471640\nBrri     0.797302436  0.721614627\nBrse     0.259080296 -0.196661887\nBrst     0.000000000  0.000000000\nBrvu     0.656643322  0.028503562\nCaqu    -0.043729888 -0.263818130\nCAR     -0.520749860  0.086007708\nCato     0.969424379  0.459588372\nCear     0.470186239 -0.187174556\nCeum     0.575087756 -0.281674227\nCeve.s  -0.476305048 -0.495325172\nCipa    -0.009595104 -0.886477227\nCivu     0.000000000  0.000000000\nCoco.s   0.000000000  0.000000000\nCoco.t   0.862466323 -0.038010697\nCogr     0.172846131 -0.719558962\nConu.s   0.000000000  0.000000000\nConu.t   0.000000000  0.000000000\nCORY.t   0.000000000  0.000000000\nCost     0.000000000  0.000000000\nCrca     0.000000000  0.000000000\nCrdo.s   0.000000000  0.000000000\nCrdo.t   0.201303763 -0.241020771\nCrox.s   0.000000000  0.000000000\nCyec     0.000000000  0.000000000\nCyfo     0.000000000  0.000000000\nCygr     0.184073508 -1.069770155\nDaca     0.000000000  0.000000000\nDacar    0.000000000  0.000000000\nDagl     0.000000000  0.000000000\n\ns.label(ccaoak$co,boxes=F)\n\n\n\n\n\\(co\\) gives you access to the coordinates of each species on the first plan. The table explicits the coordinates and the plot help you to find close species. On the first axis, two species have high negative value : Adbi (-1.97) & Abgr.t (-1.50). They are opposed at species as Coco.t (0.86) & Cato (0.96). You can realize the same for the second axis.\nHere, we can highlight that there is a group formed by Agse (coord = 0.31 ; 1.90 respectively for the axis 1 and 2), Agha (coord = -0.38 ; 1.25) Avfa (coord = 0.33 ; 0.88) and ALL (coord = 0.20 ; 1.14). An other on with two outliers, Abgr.t and Adbi. These groups will be correlated with the environment data further in the analysis.\n\nccaoak$cor \n\n                 RS1         RS2\nLatAppx    0.9867311 -0.09977934\nLongAppx   0.3726389  0.33053940\nElev.m    -0.1505108 -0.45170572\nSlope.deg  0.1235033  0.65338800\n\ns.corcircle(ccaoak$cor)\n\n\n\n\nHere, we have the correlation between axes and the topographic variables. We can conclude that the elevation (corr = -0.15[axis1] ; -0.45[axis2]) and the slope (corr = 0.12[axis1] ; 0.65[axis2]) are strongly negatively correlated. The Longitude seems to be positively correlated with the slope and negatively correlated with the elevation.\n\ns.match(ccaoak$ls, ccaoak$li)\n\n\n\nplot(ccavegan,scaling=1) \n\n\n\nplot(ccavegan,scaling=2) \n\n\n\n\nThe first plot with the arrows represents the real abundances for each site (base of the arrow) and the predictions of abundance calculated by a multiple regression thanks to the topographic variables. We see here that there are some large differences between reality and prediction. Most of the sites are in a center group but some sites differ. It is the case with the sites n°7,40,41,42. We have another group with the sites n°43,44,45,46.\nThe second plot, with scaling = 1 preserves euclidian distances between stations.\nThe third plot, with scaling = 2 preserves correlations between species.\nTo conclude on the CCA analyse, we have seen that the topographic variables explain 13% of the plant variances. The first axis represents 34% of the constrained inertia and is explained by the Latitude (corr = 0.99). We can see that two species have a high coordinate in this axis (Adbi & Abgr.t).\nThe second axis (18% of the constrained inertia) is explained by the Elevation (corr = -0.45), the Slope (corr = 0.65) and the Longitude (corr = 0.33).\nFinally, the sites n°43,44,45,46 are sites with a small latitude value and with a high abundance of species as Abgr.t."
  },
  {
    "objectID": "AMV_chapter.html#introduction-4",
    "href": "AMV_chapter.html#introduction-4",
    "title": "Multivariate Analysis",
    "section": "Introduction",
    "text": "Introduction\nThe RLQ analysis is a generalized co-inertia with 3 tables : environmental variables for each sites (R), species abundances for each sites (L) and trait variables for each species (Q). We will have to perform a CA on the table L and weighted analysis (MCA or PCA) on tablesR and Q. Then a function named rlq() perform 3 co-inertia analysis between R and L and L and Q and then between the results of each co-inertia."
  },
  {
    "objectID": "AMV_chapter.html#interpretation-4",
    "href": "AMV_chapter.html#interpretation-4",
    "title": "Multivariate Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThe use of RLQ analysis is important in ecology to integrate the traits of species with the environmental variables. So here, we don’t have 2 tables (environment & specie) as RDA part but 3 tables:\n\nenvironmental variables by sites (R)\nabundance of species by sites (L)\ntrait values by species (Q)\n\nTo perform the RLQ, we need to decompose the analyse by three types of analyses already done in this chapter:\n\nwe will use a CA analyse on the abundance of species\nwe will use a MCA on the environmental table by taking the sites weight on the CA\nwe will use a MCA on the trait table by taking the species weight on the CA\n\nHere, we use MCA for R and Q because our variables are factors but you can perform a PCA if your variables are quantitative. Warning, for R and Q you have the obligation to weight by the L table (see below).\nYou can import R package using the code\n\nlibrary(tidyverse)\nlibrary(ade4)\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(factoextra)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(RVAideMemoire)\n\n*** Package RVAideMemoire v 0.9-83-7 ***\n\nlibrary(PerformanceAnalytics)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\n\nAttaching package: 'PerformanceAnalytics'\n\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\n\nHere, we work with a dataset of ADE4 package. The data comes from @dray2008testing, @legendre1997relating\n\n#import the dataset\ndata(aviurba)\n\n#create the three tables\nsummary(aviurba$mil)    #(R)\n\n farms    small.bui high.bui industry fields   grassland scrubby  deciduous\n yes:22   yes:13    yes: 8   yes:12   yes:22   yes:12    yes:30   yes:36   \n no :29   no :38    no :43   no :39   no :29   no :39    no :21   no :15   \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n conifer  noisy      veg.cover\n yes: 8   yes:10   R37    :9  \n no :43   no :41   R5     :8  \n                   R98    :7  \n                   R93    :7  \n                   R22    :6  \n                   R87    :5  \n                   (Other):9  \n\nR&lt;-aviurba$mil\n\nsummary(aviurba$fau)    #(L)\n\n      Sp1               Sp2              Sp3               Sp4        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.05882   Mean   :0.1373   Mean   :0.07843   Mean   :0.7647  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :4.0000  \n      Sp5              Sp6              Sp7              Sp8       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :3.000  \n Mean   :0.1176   Mean   :0.2353   Mean   :0.1569   Mean   :2.078  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:3.000  \n Max.   :2.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n      Sp9              Sp10             Sp11             Sp12        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.5882   Mean   :0.6275   Mean   :0.3922   Mean   :0.03922  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :3.0000   Max.   :4.0000   Max.   :4.0000   Max.   :1.00000  \n      Sp13              Sp14              Sp15             Sp16       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.05882   Mean   :0.03922   Mean   :0.1961   Mean   :0.3725  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :2.0000   Max.   :1.0000  \n      Sp17              Sp18             Sp19             Sp20       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.07843   Mean   :0.4118   Mean   :0.7059   Mean   :0.2157  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :2.0000   Max.   :2.0000  \n      Sp21              Sp22             Sp23              Sp24        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.00000  \n Mean   :0.03922   Mean   :0.3725   Mean   :0.07843   Mean   :0.09804  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :1.00000  \n      Sp25              Sp26             Sp27             Sp28        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.03922   Mean   :0.1961   Mean   :0.2353   Mean   :0.03922  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.0000   Max.   :2.0000   Max.   :1.00000  \n      Sp29           Sp30             Sp31             Sp32       \n Min.   :0.00   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:2.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :3.00   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :2.49   Mean   :0.6667   Mean   :0.2353   Mean   :0.8627  \n 3rd Qu.:3.00   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :4.00   Max.   :4.0000   Max.   :2.0000   Max.   :3.0000  \n      Sp33             Sp34              Sp35             Sp36      \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.00000   Median :1.0000   Median :0.000  \n Mean   :0.2941   Mean   :0.07843   Mean   :0.5686   Mean   :0.451  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :2.00000   Max.   :3.0000   Max.   :3.000  \n      Sp37              Sp38             Sp39              Sp40       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.05882   Mean   :0.1765   Mean   :0.03922   Mean   :0.4118  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :3.0000  \n\nL&lt;-aviurba$fau\n\nsummary(aviurba$traits) #(Q)\n\n   feed.hab    feed.strat     breeding     migratory \n insect:19   ground :27   ground  : 6   resident:23  \n grani :12   aerial : 3   building:14   migrant :17  \n omni  : 9   foliage:10   scrub   :12                \n                          foliage : 8                \n\nQ&lt;-aviurba$traits\n\nand explore the tables\n\nhead(R)  \n\n   farms small.bui high.bui industry fields grassland scrubby deciduous conifer\nR1   yes        no       no       no    yes        no      no       yes      no\nR2   yes        no       no       no     no       yes     yes        no      no\nR3    no        no       no       no    yes        no     yes       yes      no\nR4    no        no       no       no    yes        no     yes        no      no\nR5    no        no       no      yes     no        no     yes        no      no\nR6    no        no       no      yes     no       yes     yes        no      no\n   noisy veg.cover\nR1    no       R98\nR2    no       R87\nR3    no      R100\nR4    no      R100\nR5    no        R5\nR6   yes        R5\n\nhead(L)\n\n   Sp1 Sp2 Sp3 Sp4 Sp5 Sp6 Sp7 Sp8 Sp9 Sp10 Sp11 Sp12 Sp13 Sp14 Sp15 Sp16 Sp17\nR1   0   0   0   0   0   0   1   0   0    0    0    0    0    0    0    1    0\nR2   0   0   0   0   0   0   1   0   0    0    0    0    1    0    0    0    0\nR3   0   2   0   0   0   0   1   0   1    0    0    0    0    1    0    0    0\nR4   0   0   0   0   0   0   1   0   1    0    0    0    0    0    1    0    0\nR5   0   0   0   0   0   0   0   0   2    0    0    0    0    0    1    1    0\nR6   0   0   0   0   0   0   0   0   0    0    0    0    0    0    0    1    0\n   Sp18 Sp19 Sp20 Sp21 Sp22 Sp23 Sp24 Sp25 Sp26 Sp27 Sp28 Sp29 Sp30 Sp31 Sp32\nR1    1    1    0    0    1    0    0    0    1    0    0    4    0    0    1\nR2    2    1    0    0    1    0    1    0    1    0    0    4    4    1    1\nR3    2    0    0    0    1    0    0    0    1    1    0    1    0    1    0\nR4    2    0    0    0    1    0    1    0    0    2    0    1    0    1    1\nR5    1    0    0    0    1    0    0    0    1    1    0    1    1    1    0\nR6    0    0    0    0    0    0    0    0    0    0    0    4    1    0    1\n   Sp33 Sp34 Sp35 Sp36 Sp37 Sp38 Sp39 Sp40\nR1    1    0    1    0    0    0    0    1\nR2    1    0    0    0    0    0    0    0\nR3    1    0    0    0    0    0    0    1\nR4    1    0    0    0    0    1    0    0\nR5    0    0    0    0    0    0    0    0\nR6    1    2    1    2    0    0    0    0\n\nhead(Q)\n\n    feed.hab feed.strat breeding migratory\nSp1     omni     ground  foliage   migrant\nSp2     omni     ground building   migrant\nSp3     omni     ground   ground   migrant\nSp4    grani     ground building  resident\nSp5    grani     ground    scrub  resident\nSp6    grani     ground  foliage  resident\n\n\nThe first part is to perform our CA on specie table\n\nafcL &lt;- dudi.coa(log(L+1), scannf = FALSE) \nafcL  \n\nDuality diagramm\nclass: coa dudi\n$call: dudi.coa(df = log(L + 1), scannf = FALSE)\n\n$nf: 2 axis-components saved\n$rank: 39\neigen values: 0.3987 0.2736 0.1917 0.1624 0.1421 ...\n  vector length mode    content       \n1 $cw    40     numeric column weights\n2 $lw    51     numeric row weights   \n3 $eig   39     numeric eigen values  \n\n  data.frame nrow ncol content             \n1 $tab       51   40   modified array      \n2 $li        51   2    row coordinates     \n3 $l1        51   2    row normed scores   \n4 $co        40   2    column coordinates  \n5 $c1        40   2    column normed scores\nother elements: N \n\n\nThe first CA is done. We use log transformation because the abundance of species has a large range and we add “+1” to avoid the log(0) for some species. You must adapt the presence of transformation (or not) to your data.\nNow, we can perform the two MCA analysis on the trait table and environmental table\n\nacmR &lt;- dudi.acm(R, row.w = afcL$lw, scannf = FALSE,nf = 4)\nscatter(acmR)\n\n\n\nacmQ &lt;- dudi.acm(Q, row.w = afcL$cw, scannf = FALSE,nf = 4)\nscatter(acmQ)\n\n\n\n\nThe scatterplot allows to see the ordination of each table and the repartition of factor on the simple axe of MCA.\nBut now, we will use the RLQ analyse that creates two co-inertia (R-L, L-Q), assembles and compares the co-inertia. We use the rlq function for that.\n\nrlq &lt;- rlq(acmR, afcL, acmQ, scannf = FALSE)\nrlq\n\nRLQ analysis\ncall: rlq(dudiR = acmR, dudiL = afcL, dudiQ = acmQ, scannf = FALSE)\nclass: rlq dudi \n\n$rank (rank)     : 8\n$nf (axis saved) : 2\n\neigen values: 0.01068 0.003128 0.001525 0.0004536 0.0001298 ...\n\n  vector length mode    content                    \n1 $eig   8      numeric Eigenvalues                \n2 $lw    28     numeric Row weigths (for acmR cols)\n3 $cw    12     numeric Col weigths (for acmQ cols)\n\n   data.frame nrow ncol content                                      \n1  $tab       28   12   Crossed Table (CT): cols(acmR) x cols(acmQ)  \n2  $li        28   2    CT row scores (cols of acmR)                 \n3  $l1        28   2    Principal components (loadings for acmR cols)\n4  $co        12   2    CT col scores (cols of acmQ)                 \n5  $c1        12   2    Principal axes (loadings for acmQ cols)      \n6  $lR        51   2    Row scores (rows of acmR)                    \n7  $mR        51   2    Normed row scores (rows of acmR)             \n8  $lQ        40   2    Row scores (rows of acmQ)                    \n9  $mQ        40   2    Normed row scores (rows of acmQ)             \n10 $aR        4    2    Corr acmR axes / rlq axes                    \n11 $aQ        4    2    Corr afcL axes / coinertia axes              \n\naxe=c(1:8)\nprint(paste(\"The contribution of axe n°\",axe, \"are\", rlq$eig/(sum(rlq$eig))*100,\"%\"))\n\n[1] \"The contribution of axe n° 1 are 66.6748744763001 %\"  \n[2] \"The contribution of axe n° 2 are 19.5228230896623 %\"  \n[3] \"The contribution of axe n° 3 are 9.51702542395437 %\"  \n[4] \"The contribution of axe n° 4 are 2.83090411660832 %\"  \n[5] \"The contribution of axe n° 5 are 0.810200444830495 %\" \n[6] \"The contribution of axe n° 6 are 0.421072849575587 %\" \n[7] \"The contribution of axe n° 7 are 0.124897113199049 %\" \n[8] \"The contribution of axe n° 8 are 0.0982024858697611 %\"\n\n#randtest(rlq)\n#summary(rlq)\n#plot(rlq)\n\nHere, the output of the RLQ is complex but only few information are, at this point important. We see that we have 8 eigenvalues and we have their values. All the different compounds of the output will be used after in representations or analysis.\nNevertheless, we can calculate the contribution of each axis of the RLQ by performing the formule below: \\[\\frac{rlq$eig}{\\sum{rlq$eig}}.100\\]\nAfter that, and before to plot and analyse the result, it is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between the different tables. To produce this, we perform a permutation test with the function randtest.\n\nrandtest(rlq)\n\nclass: krandtest lightkrandtest \nMonte-Carlo tests\nCall: randtest.rlq(xtest = rlq)\n\nNumber of tests:   2 \n\nAdjustment method for multiple comparisons:   none \nPermutation number:   999 \n     Test        Obs  Std.Obs   Alter Pvalue\n1 Model 2 0.01602442 9.114605 greater  0.001\n2 Model 4 0.01602442 2.336643 greater  0.030\n\nplot(randtest(rlq))\n\n\n\n\nThe outputs above correspond to the permutation test. We see that the number of permutations of columns and rows was to 999 (default value).\nThe results of this test shows that the permutation of sites (rows) is the first result (p_value=0.1%) and the permutation of species (columns) is the second result (p_value=1.9%) So each result is significant (5% threshold) and we can conclude that our RLQ result is not linked to random effect.\nFinally, the results of the RLQ are plot in the distribution law calculated by the permutation.\nWe can know analyse the result by using different types of plot:\n\nplot(rlq)\n\n\n\n\nThe plot above is the complet plot, difficult to understand and that will be resumed step by step afterward.\nHowever, we have here, up the score (the position) of the sites and species in this analyse. Below in the centre, we have the correlation of the environmental variables between them (R Canonical weights) and the correlation of the traits between them (Q Canonical weight). The plots of the R axes and the Q axes represent the reprojection of axis in the RLQ analyse in order to the simple analyses (CA or MCA here).\nNow, we can decompose the result and start with the environmental variables:\n\n#analyse the environmental variables:\nround(rlq$l1,dig=2)\n\n                 RS1   RS2\nfarms.yes      -0.11  0.73\nfarms.no        0.10 -0.65\nsmall.bui.yes  -2.27 -0.53\nsmall.bui.no    0.71  0.16\nhigh.bui.yes   -2.17 -0.48\nhigh.bui.no     0.35  0.08\nindustry.yes    0.43  2.63\nindustry.no    -0.11 -0.67\nfields.yes      1.34  0.52\nfields.no      -1.34 -0.52\ngrassland.yes  -1.07  0.26\ngrassland.no    0.30 -0.07\nscrubby.yes    -0.10 -0.91\nscrubby.no      0.14  1.37\ndeciduous.yes  -0.56 -0.64\ndeciduous.no    1.36  1.55\nconifer.yes    -0.87  0.91\nconifer.no      0.17 -0.18\nnoisy.yes      -2.61  0.67\nnoisy.no        0.43 -0.11\nveg.cover.R100  4.35 -4.95\nveg.cover.R98   1.74 -1.64\nveg.cover.R93   0.13  3.16\nveg.cover.R87  -0.94 -0.98\nveg.cover.R62  -1.86 -0.35\nveg.cover.R37  -2.01 -0.10\nveg.cover.R22  -2.91  0.05\nveg.cover.R5    1.04  3.24\n\ns.label(rlq$l1, boxes=FALSE)\n\n\n\n#calcul of the absolute contribution\niner=inertia.dudi(rlq,col.inertia=T,row.inertia=T)\n\nabscoiE=iner$row.abs\nabscoiE\n\n                     Axis1        Axis2\nfarms.yes       0.05223514  2.304444147\nfarms.no        0.04623764  2.039853800\nsmall.bui.yes  11.21784230  0.599492102\nsmall.bui.no    3.52200412  0.188219231\nhigh.bui.yes    5.99404001  0.297237022\nhigh.bui.no     0.97600864  0.048399060\nindustry.yes    0.33291147 12.714009779\nindustry.no     0.08422136  3.216444135\nfields.yes      8.17323519  1.238389579\nfields.no       8.20233957  1.242799407\ngrassland.yes   2.29313411  0.132425116\ngrassland.no    0.64653934  0.037336694\nscrubby.yes     0.04972304  4.526463864\nscrubby.no      0.07475839  6.805519570\ndeciduous.yes   2.01878003  2.632197876\ndeciduous.no    4.90713304  6.398193444\nconifer.yes     1.13513324  1.240985532\nconifer.no      0.22330026  0.244123224\nnoisy.yes       8.82492060  0.589761352\nnoisy.no        1.47048833  0.098271387\nveg.cover.R100 16.95851693 21.972519979\nveg.cover.R98   4.57563062  4.054631466\nveg.cover.R93   0.02364930 14.452989048\nveg.cover.R87   0.82853117  0.891676376\nveg.cover.R62   3.46745570  0.119501611\nveg.cover.R37   5.23067194  0.012183942\nveg.cover.R22   7.43352189  0.002336584\nveg.cover.R5    1.23703663 11.899594674\n\n\nThe table corresponds to the position of each modality for each environmental variables on the two first axis. You can link this table to the plot. For example, “veg.cover.R100” seems to stand out from the other modalities. In the table, we can see that the position on the first axis is 4.35 and -4.95 on the second axis. Closer to the center of the cloud, we can talk about “veg.cover.R98” that have position of 1.74 on the first axis and -1.64 on the second axis. In addition, we can observe the contribution of this two variables. On the second table we have the absolute contribution of the modalities for each variable. The contribution of the “veg.cover.R100” of the first axis is 17% and 22% on the second axis. This is a high value if we take the threshold to 1/N with N the number of modalities (28 here so 3%). The contribution of “veg.cover.R98” is 4.6% for the first axis and 4.1% for the second axis.\nWe can therefore conclude that these two variables are correlated. This seems to be biologically coherent since these two variables are part of a plant cover gradient and are the two highest values.\nWe can also see that the variable negatively correlated with the first axis are modalities explaining the urbanization ()\nNow we can realise the same analyse for the traits.\n\n#analyse the traits:\nround(rlq$c1,dig=2)\n\n                     CS1   CS2\nfeed.hab.insect     0.72  0.73\nfeed.hab.grani     -0.69 -0.24\nfeed.hab.omni      -0.13 -1.64\nfeed.strat.ground  -0.11  0.01\nfeed.strat.aerial  -1.26  1.24\nfeed.strat.foliage  2.60 -1.97\nbreeding.ground     3.60  4.16\nbreeding.building  -1.16  0.35\nbreeding.scrub      1.71 -2.22\nbreeding.foliage   -0.15 -0.62\nmigratory.resident -0.24 -0.23\nmigratory.migrant   0.54  0.50\n\ns.arrow(rlq$c1, boxes=FALSE)\n\n\n\n#absolute contribution\nabscoiV=iner$col.abs\nabscoiV\n\n                         Axis1        Axis2\nfeed.hab.insect     5.69187893  5.861738276\nfeed.hab.grani      5.10226145  0.632420595\nfeed.hab.omni       0.05897841  8.814467814\nfeed.strat.ground   0.22470905  0.003581384\nfeed.strat.aerial   7.09044249  6.818080636\nfeed.strat.foliage 19.80381068 11.362659415\nbreeding.ground    28.19988784 37.628640395\nbreeding.building  17.18536604  1.552603443\nbreeding.scrub     13.29440922 22.370738798\nbreeding.foliage    0.11612276  2.116775390\nmigratory.resident  1.00414733  0.881790779\nmigratory.migrant   2.22798580  1.956503077\n\n\nAs seen before, the first table is the position of the modalities for the two first axes of the RLQ analyses. The plot is the representation of that and the second table is the absolute contributions.\nHere, we have larger differences between the different variable and modalities. We’re going to focus on a few examples, but you can take the time to go into detail about each modality. “feed.strat.foliage” (coord= 2.60; -1.97 for first and second axis respectively) and “breeding.scrub” (coord= 1.71: -2.22) break away from the centre of the cloud. In addition their contributions are all greater than 10%. We can conclude that these modalities are correlated and highly contribute of axes.\nThe last point is to observe the species coordinates.\n\n#analyse the species:\nround(rlq$lQ,dig=2)\n\n     AxcQ1 AxcQ2\nSp1   0.04 -0.44\nSp2  -0.22 -0.19\nSp3   0.97  0.76\nSp4  -0.55 -0.03\nSp5   0.17 -0.67\nSp6  -0.30 -0.27\nSp7   0.71 -0.24\nSp8  -0.29  0.70\nSp9   0.99  1.17\nSp10 -0.29  0.70\nSp11 -0.29  0.70\nSp12  1.19  1.35\nSp13 -0.41 -0.38\nSp14  1.19  1.35\nSp15  1.19  1.35\nSp16 -0.20  0.22\nSp17  0.00  0.40\nSp18  0.71 -0.24\nSp19  0.31 -1.02\nSp20  1.39 -0.74\nSp21  1.39 -0.74\nSp22  1.20 -0.92\nSp23  1.39 -0.74\nSp24  1.39 -0.74\nSp25  0.48 -0.28\nSp26  0.48 -0.28\nSp27  0.64  0.93\nSp28  0.17 -0.67\nSp29 -0.55 -0.03\nSp30 -0.55 -0.03\nSp31 -0.30 -0.27\nSp32 -0.30 -0.27\nSp33  0.38 -0.76\nSp34  0.85 -1.16\nSp35 -0.30 -0.27\nSp36 -0.20  0.22\nSp37  1.18 -1.33\nSp38 -0.16 -0.62\nSp39 -0.41 -0.38\nSp40 -0.16 -0.62\n\ns.label(rlq$lQ,boxes=FALSE)\n\n\n\ns.label(rlq$lQ, label=aviurba$species.names.fr,boxes=FALSE)\n\n\n\n\nHere, is to see that we seem to have a gradient of response along the first axis (i.e. Loriot Jaune/Sp37 [coord=1.18; -1.33] and Alouette des champs/Sp9 [coord=0.99; 1.17]) and along the second axis (i.e. Loriot Jaune/Sp37 [coord=1.18; -1.33] and Fauvette des jardins/Sp21 [coord=0.99; 1.17]). We will see that we need to link this to the other variables (trait and environment) For species and sites, we do not have contribution because these variables do not contribute of the axis creation.\nTo conclude on this analysis, the two first axes explain 86.2% of the total variation with respectively 66.7%, and 19.5% of the total inertia.\nThe correlations between the environmental variables and the RLQ axes showed that the first axis is explained by Veg.cover.R100 (contribution of 16.95%), veg.cover.R22 (7.43%), noisy.yes (8.82%), small.bui.yes (11.22%), field.yes (8.17%), field.no (8.2%) and Veg.cover.R22 (7.43%). The traits variables that contribute to the first axis of the RLQ axes are breeding.ground (28.19%), feed.strat.foliage (19.8%), breeding.scrub (13.3%) and negatively with breeding.building (17.18%) and feed.strat.aerial (7.09%). In other words, breeding.ground, feed.strat.foliage, breeding.scrub, feed.strat.aerial and breeding.building were the higher explanatory attributes for this RLQ axis (explained by the length and the angle between the axes and the vectors)."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html",
    "href": "r_chapter_boostrapping_resampling.html",
    "title": "Resampling methods",
    "section": "",
    "text": "Authors: Lucien Ricome, Youna Douchet, Pierre-Alexandre Quittet, Bastien Clémot"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#the-jackknife-resampling-method",
    "href": "r_chapter_boostrapping_resampling.html#the-jackknife-resampling-method",
    "title": "Resampling methods",
    "section": "The Jackknife resampling method",
    "text": "The Jackknife resampling method\nThis method was first proposed by Quenouille (1949). The name “Jackknife” comes from the fact that it is often referenced as a “quick and dirty” tool of statistics (Abdi and Williams 2010). It means that it is usable in many situations but it is often not the best tool. The technique allows us to estimate a confidence interval for some statistics when the dataset is too small and/or when it does not follow a known distribution.\n\nPrinciple of the leave-one-out Jackknife (Sinharay 2010 ; Petit 2022)\nThe main goal of the jackknife method is to calculate a confidence interval around a statistic when classical statistics can not apply to the data.The principle is to create subsamples of the initial sample by successively removing some of the observations. Then, on each subsample, a pseudo-value will be calculated. With all the pseudo-values, it is possible to calculate an estimator of the statistic and to estimate its confidence interval.\nThe most famous jackknife is the the leave-one-out jackknife (or order 1 jackknife) for which all the subsamples contain all the observations except one. Concretely, for a sample of \\(n\\) observations, there will be \\(n\\) subsamples of size \\((n-1)\\). The \\(i^{th}\\) subsample will be composed of observations from \\(1\\) to \\(n\\) minus the \\(i^{th}\\) observation.\nTo illustrate, here is our sample:\n\ndata_sample\n\n [1] 3.7 2.9 2.8 3.0 2.8 3.0 3.0 3.8 3.3 3.2 2.8 3.0 3.6 3.8 2.5\n\n\nAnd we can represent some subsamples:\n\ncat(\"1st subsample : \\n\",data_sample[-1], \"\\n\")\n\n1st subsample : \n 2.9 2.8 3 2.8 3 3 3.8 3.3 3.2 2.8 3 3.6 3.8 2.5 \n\ncat(\"4th subsample : \\n\",data_sample[-4], \"\\n\")\n\n4th subsample : \n 3.7 2.9 2.8 2.8 3 3 3.8 3.3 3.2 2.8 3 3.6 3.8 2.5 \n\n\n\n\nCalculation of pseudo-values\nAfter creating the subsamples, the next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:\n\\[\nv_{i} = n\\overline{X} - (n-1)\\overline{X}_{-i}\n\\]\nwith the following variables:\n\n\n\n\n\n\n\nVariable\nMeaning\n\n\n\n\n\\(v_{i}\\)\nPseudo value of the \\(i^{th}\\) subsample\n\n\n\\(n\\)\nTotal number of observations\n\n\n\\(\\overline{X}\\)\nMean of the initial sample\n\n\n\\(\\overline{X}_{-i}\\)\nMean of the \\(i^{th}\\) subsample, corresponding to all observations except the \\(i{th}\\)\n\n\n\nLet’s write a function which create the subsamples and calculate their pseudo values:\n\n# Function for pseudo-values\npseudo_val &lt;- function(data, theta){\n  # entry : data = the vector of data to which we want to apply the Jackknife\n  # entry : theta = function for the statistic of interest\n  # output : a vector of pseudo values for each subsample\n  n &lt;- length(data)\n  mean_init &lt;- theta(data)\n  pv &lt;- rep(NA,n) #to keep in memory each pseudo value\n  for (i in 1:n) {\n    pv[i] &lt;- n*mean_init - (n-1)*theta(data[-i])\n  }\n  return(pv)\n}\n\nTo try the function:\n\n# Function test\npv &lt;- pseudo_val(data =  data_sample,\n                  theta = mean)\nprint(pv)\n\n [1] 3.7 2.9 2.8 3.0 2.8 3.0 3.0 3.8 3.3 3.2 2.8 3.0 3.6 3.8 2.5\n\n\n\n\nStatistical test\nThe jackknife estimator \\(\\theta\\) of the mean will then be calculated as follow:\n\\[\n\\begin{align}\n\\hat{\\theta} & = \\frac{\\sum_{i=1}^{n}v_{i}}{n} \\\\\n\\hat{\\theta} & = \\overline{v}\n\\end{align}\n\\]\nIt corresponds to the mean of the pseudo values \\(v_i\\).\n\n# Compute mean of the pseudo-values\nmean_pv &lt;- mean(pv)\nmean_pv\n\n[1] 3.146667\n\n\nThe Jackknife estimator \\(\\theta\\) obtained is 3.15. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:\n\\[\n\\begin{align}\nSE_\\hat{X} & = \\sqrt{\\frac{\\sum_{i=1}^{n}(v_{i}-\\overline{v})}{n(n-1)}} \\\\\nSE_\\hat{X} & = \\sqrt{\\frac{\\sigma_{v}^{2}}{n}}\n\\end{align}\n\\]\nWith \\(\\overline{v}\\) being the mean of the pseudo values (and the jackknife estimator).\n\n# Compute standard error\nSE &lt;- sqrt(var(pv)/length(pv))\nSE\n\n[1] 0.1050472\n\n\n\n\nConfidence interval\nFrom these we can calculate a confidence interval:\n\\[\n[\\, \\overline{v} - 1.96 \\, SE_\\hat{X} ; \\overline{v} + 1.96 \\, SE_\\hat{X} \\,]\n\\]\n\nNote: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\n\n\n Lower bound :  2.94 \n Higher bound :  3.35\n\n\nThe estimated Jackknife mean is \\(3.15\\) with the following confidence interval : \\([2.94\\:;\\:3.35]\\). The real value \\(\\mu = 3.05\\) is captured within the bounds of the confidence interval which indicate the robustness of the estimation process.\n\n\nR package\nOn r, functions already exist to automatically execute the Jackknife. For instance, in the package bootstrap (Tibshirani 2019), there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).\n\n# Package function\nbootstrap :: jackknife(x= data_sample,\n                theta = function(x) mean(x))\n\n$jack.se\n[1] 0.1050472\n\n$jack.bias\n[1] 0\n\n$jack.values\n [1] 3.107143 3.164286 3.171429 3.157143 3.171429 3.157143 3.157143 3.100000\n [9] 3.135714 3.142857 3.171429 3.157143 3.114286 3.100000 3.192857\n\n$call\nbootstrap::jackknife(x = data_sample, theta = function(x) mean(x))\n\n\nThe output of the function include the standard error ($jack.se) as describe above. It also include the bias ($jack.bias) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output $jack.values does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In our example, it correspond to the mean of each subsample.\n\n\nStrengths of the method\nAs seen precedently, the Jackknife allows to calculate a confidence intervall when data are not following a normal distribution and to estimate the bias induced when only a sample of the statistical population is observed. These two characteristic does not only apply to univariate estimators such as the mean. It can be used on correlation coefficients and regression parameters for instance (Sinharay 2010). In that case, each subsample leave one observations out with all its associated variables. The jackknife method is peticularly interesting when it comes to make inferences about variance ratio. It can perform as good as the Fisher test if variables are normally distributed and better if they are not (miller_jackknife?–review_1974).\nThe Jackknife method was a good tool in the last century as it was easy to apply manually but it is nowaday surpassed by other methods which emerged thanks to the evolution of computers.\n\n\nWeaknesses of the method\nThis method is not always efficient, it is not recomanded for time series analysis where some time period are successively removed to create the subsample. Moreover it has little succes when it comes to the estimation of single order statistics (specific values in an ordered set of observations or data points) such as the median or the maximum (miller_jackknife?–review_1974). But for this last point, other jackknife can be used such as the deleted-d jackknife which perform better for the median. In this method, subsamples are of size \\((n-d)\\) and there are \\(\\binom{n}{d}\\) subsamples.\nFor more informations about applications of the Jackknife you can read the paper: Jackknife a Review by Rupert G. Miller (miller_jackknife?)–review_1974."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#the-bootstrap-re-sampling-method",
    "href": "r_chapter_boostrapping_resampling.html#the-bootstrap-re-sampling-method",
    "title": "Resampling methods",
    "section": "The bootstrap re-sampling method",
    "text": "The bootstrap re-sampling method\n\nIntroduction and Principle\nNow that we’ve thoroughly explored jackknife resampling in the previous section, let’s delve into another resampling technique: the bootstrap method.\nSimilar to jackknife, bootstrap aims to estimate descriptive parameters of a sample and assess the accuracy of these estimates for making inferences about the actual population. It is yet another statistical inference method that involves generating multiple datasets through resampling from the original dataset. Essentially, the concept revolves around using resampling techniques to create a probability distribution for a chosen parameter. This distribution, known as the empirical distribution function, serves as an estimation of the true probability distribution of our parameter in the population. In practice, similar to the earlier section, our objective here is to compute parameters of interest from our sample (such as mean, median, or even the \\(R^2\\) of a regression, among others) and establish confidence intervals around these estimates.\nThe fundamental difference between jackknife and bootstrap lies in their resampling methodologies. In each of its \\(i\\) iterations, bootstrap randomly resamples \\(n\\) elements from an initial sample of \\(n\\) data points with replacement. This leads to two crucial distinctions from the resampling method employed in jackknife: i) The new samples generated through bootstrap maintain the same size \\(n\\) as the original dataset. ii) Given the sampling with replacement approach, a particular element can occur multiple times in a new sample or might not appear at all during the resampling process.\nAfter generating these \\(i\\) bootstrap samples, the intended statistical parameters are computed for each of these samples. As a result, we obtain a distribution of \\(i\\) data points derived from these samples, forming what is known as our empirical distribution function. Subsequently, the analysis of this distribution allows us to estimate precision, particularly in establishing the confidence interval for our statistical parameters.\n\n\nPractice and Application\nIf the previous introduction seemed a bit perplexing, don’t worry—we’ll use an example to illustrate this concept.\nFor this demonstration, we’ll once again consider our non-normally distributed sampling of 15 individuals. Let’s proceed with the calculation of the mean.\n\n# Field sampling mean\nmean(data_sample)\n\n[1] 3.146667\n\n\nSo, we’ve obtained a mean value of 3.15, which is great. However, this mean value can’t be reliably generalized to the entire population because we lack information about the variability induced by the specific individuals we chose to sample. As highlighted in our previous code, we only sampled \\(n=15\\) individuals in the field. Clearly, this limited sampling isn’t sufficient to confidently assert that our estimated mean is truly representative of the entire population. Hence, it becomes essential to gain insights into the variability of the mean we’ve just calculated. This is precisely where the bootstrap method comes into play.\nWe’ll now systematically apply our algorithm to resample the initial dataset obtained from our field sampling. This resampling process will be utilized to construct the probability distribution of our mean.\n\nbootstrap resampling iterations\n\n\n# Fix our number of bootstrap iterations\nB = 1000\n\n# Create an empty list to stock our resamplings\nbootstrapSamples &lt;- vector(\"list\",B)\n\n# bootstrap resampling algorithm\nfor (i in 1:B) {\n  # Randomly sample n elements with replacement\n  bootstrapSamples[[i]] &lt;- sample(data_sample, replace = TRUE)\n}\n\nFor pedagogical purposes, let’s examine a few resamples while comparing them to our initial field sampling dataset.\n\n# Our original field sampling\nsort(data_sample)\n\n [1] 2.5 2.8 2.8 2.8 2.9 3.0 3.0 3.0 3.0 3.2 3.3 3.6 3.7 3.8 3.8\n\n\n\n# A few resampling\nsort(bootstrapSamples[[5]])\n\n [1] 2.8 2.8 2.8 3.0 3.0 3.0 3.0 3.2 3.3 3.3 3.6 3.6 3.6 3.8 3.8\n\nsort(bootstrapSamples[[777]])\n\n [1] 2.8 2.8 2.8 2.8 2.8 2.9 3.0 3.0 3.0 3.0 3.2 3.2 3.3 3.8 3.8\n\n\nObserve how certain values are repeated more frequently than in our initial dataset. Additionally, note that some values aren’t sampled at all. For instance, the value 2.5 appeared only once in our initial set and consequently was seldom or perhaps never resampled. Conversely, the value 2.8 was more prevalent in our initial dataset and consequently was sampled more frequently\n\nCompute mean for every resampled datasets\n\nNow, we’ll compute the parameter of interest for each of our resampled datasets. This process will yield our set of \\(i=1000\\) mean values, which we’ll utilize to construct our empirical distribution in the subsequent step.\n\n# Compute mean for each bootstrap samples\niterationMeans &lt;- sapply(bootstrapSamples, mean)\n\n\nPlot the probability distribution\n\nThe computation of the distribution for our parameter of interest is complete. We can now proceed to plot it, providing us with an understanding of the variability surrounding our estimated mean value.\n\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(\n    binwidth = 0.05,\n    fill = \"skyblue\",\n    color = \"black\",\n    aes(y = after_stat(density))\n  ) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(\n    xintercept = mean(data_sample),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic() +\n  theme(text=element_text(size=15))\n\n\n\n\n\n\n\n\nHence, this probability distribution provides us with insights into the variability around the mean value estimated from our field sampling. As anticipated, and as indicated by the red dashed line, this distribution is centered around the estimated mean value.\n\nCompute Confidence Interval around our mean estimate\n\nRecall the primary objective of the bootstrap method: to assess the precision of our parameter estimation to make inferences about the broader population. To accomplish this, we’ll utilize the bootstrap percentiles. For instance, suppose we aim to calculate the 95% Confidence Interval. In this case, we’ll allocate 5% of the error evenly on both sides of our distribution, corresponding to the values at the 2.5th and 97.5th percentiles in our empirical distribution.\n\n# Get percentile 2.5%\nquantile(iterationMeans, probs = c(0.025,0.975))\n\n    2.5%    97.5% \n2.966667 3.333500 \n\n\nAnd thus, we’ve successfully obtained our confidence interval for the mean estimation: \\(2.97 \\leq \\overline{x} \\leq 3.33\\).\nFor educational purposes, let’s proceed to plot these percentiles in green.\n\n# Probability distribution plot\nggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +\n  geom_histogram(\n    binwidth = 0.05,\n    fill = \"skyblue\",\n    color = \"black\",\n    aes(y = after_stat(density))\n  ) +\n  geom_density(alpha = 0.5, fill = \"orange\") +\n  geom_vline(\n    xintercept = mean(data_sample),\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 1\n  ) +\n  geom_vline(\n    xintercept = quantile(iterationMeans, probs = 0.025),\n    color = \"darkolivegreen4\",\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  geom_vline(\n    xintercept = quantile(iterationMeans, probs = 0.975),\n    color = \"darkolivegreen4\",\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  labs(title = \"Empirical probability distribution of mean values\",\n       x = \"Mean\",\n       y = \"Density\") +\n  theme_classic() +\n  theme(text = element_text(size = 15))\n\n\n\n\n\n\n\n\n\nCompare with population mean value\n\nThe bootstrap resampling method has facilitated the construction of a confidence interval around our mean estimate with a 5% margin of error. Now, let’s compare this interval with the actual value of our Iris population.\n\nmean(irisPopulation)\n\n[1] 3.057333\n\n\nThe actual sepal width mean of our population falls within the bounds of our confidence interval, which is highly encouraging! Comparing it to the interval estimated using the jackknife method, we notice that this interval is narrower and more precise. Consequently, we’ve effectively inferred information about the population mean in a scenario where we had limited data, made no normality assumptions, and where the standard computation of confidence intervals might not have been sufficiently robust and a\n\n\nStrengths and Weaknesses\nAs we’ve seen, bootstrap is a powerful resampling technique used in statistics for estimating the precision of parameters. One of its primary advantages lies in its versatility across a wide spectrum of statistical estimations. Indeed, this resampling technique is applicable to various parameters, such as mean, variance, regression coefficients, and more. Furthermore, this method is particularly robust in handling complex data structure without strict distributional assumptions, meaning that it can easily works with non-normally distributed datasets with little observations, which is really relevant in real-word scenarios. In those cases, bootstrap ends up being more accurate than the standard intervals obtained under normality assumption.\nHowever, one significant concern is its computational intensity, especially noticeable with larger datasets or complex models. Generating numerous bootstrap samples can demand, in certain cases, substantial computing resources. Additionally, bootstrap might display biased estimation of the accuracy of estimates, depending on whether or not the sampling dataset is representative of the actual population.\nOverall, bootstrap’s major advantages lie in its broader applicability across various statistical estimations, but which comes with higher computational demands. In contrast, Jackknife might be less versatile, but tends to offer a better computational efficiency and less biased parameters, making it a favorable choice under certain conditions.\n\n\nTo go further\nThis section on the bootstrap method offers a fundamental understanding, providing explicit R scripts for pedagogical purposes. If you find yourself needing to utilize bootstrap, we highly recommend exploring the capabilities of the precedently presented bootstrap R package, which streamlines bootstrap implementation. Moreover, the bootstrap method extends beyond basic parameter estimation, offering various derivatives and applications catering to diverse needs:\n\nParametric bootstrap: Assumes a specific parametric distribution, generating bootstrap samples from this fitted distribution and allowing inference within a particular statistical model.\nSmooth bootstrap: Incorporates smoothing techniques to reduce variability in resampling, ideal for noisy or irregular data.\nBayesian bootstrap: Generates bootstrap samples from posterior distributions, useful for uncertainty estimation in Bayesian analysis (see more in Bayesian chapter of this book).\n\nLastly, bootstrap resampling isn’t limited to parameter estimation. It can also be employed to assess statistical tests, a topic we’ll delve into in our upcoming section."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#foundations-of-permutation-tests",
    "href": "r_chapter_boostrapping_resampling.html#foundations-of-permutation-tests",
    "title": "Resampling methods",
    "section": "Foundations of Permutation Tests",
    "text": "Foundations of Permutation Tests\n\nFundamental Principle\nPermutation tests differ from classical statistical tests such as Student’s t-test or the \\(\\chi\\) -square test in that they do not rely on specific assumptions about the distribution of data. Unlike traditional tests that assume a theoretical distribution of the test statistic under the null hypothesis, often based on well-known laws like the Gaussian distribution, permutation tests empirically generate this theoretical distribution directly from the data. This categorizes them as exact tests and non-parametric tests.\nAccording to the central limit theorem, repeating a large number of observations of a random variable (in this case, the test statistic) leads to a Gaussian distribution. This distribution allows us to position the observed test statistic (calculated on our sample) and determine the significance of the effect of our explanatory variables. In other words, we generate a large number of permutations of the observations, calculate the test statistic for each permutation, and then compare the observed statistic to this null distribution. We’ll get into some reminder on what is the statistic of a test, a null distribution and a how p-values are generated to dive into the world of permutation test.\n\n\nParametric tests : test statistic, null distribution and p-value\nThe test statistic is a numerical measure used to evaluate whether observed differences between groups in a study are statistically significant. It quantifies the gap between observed and randomly attributable observations. In a very simplistic way, we could say they allow to determine the extent to which the effect we observe with our experimental design could be attributable to chance. Here are some familiar test statistics you’ve likely encountered in your fruitful scientific career:\n\\[\n\\begin{align*}\nF [\\text{ANOVA}] &: \\quad F = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}} \\quad \\sim \\mathcal{F}(df_{\\text{between}}, df_{\\text{within}}) \\\\\n\\\\\nt [\\text{Student's t-test}] &: \\quad t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\quad \\sim t(df) \\\\\n\\\\\n\\chi^2 [\\text{$\\chi^2$ Test}] &: \\quad \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} \\quad \\sim \\chi^2(df) \\\\\n\\end{align*}\n\\]\nWith \\(MS\\) as the mean square error, \\(df\\) as the degrees of freedom, \\(\\bar{X}_i\\) as the mean of group \\(i\\), \\(s^2_i\\) as the variance of group \\(i\\), \\(n_i\\) as the number of replicates in group \\(i\\), \\(O_i\\) as the observed \\(\\chi^2\\) distance, and \\(E_i\\) as the expected \\(\\chi^2\\) distance.\nYou may recall that these statistics typically follow well-established distributional laws. But what does “known distributional laws” mean? It means we know the probability of a given value of a statistical test (referred to as its probability distribution) based on certain parameter values (e.g., the degree of freedom for Student distribution). Hence, the term parametric tests! The probability distribution of a statistic under a specific parameter, such as the degrees of freedom [ddl] for a Student’s t-distribution, forms the basis of the null hypothesis in statistical testing. The null hypothesis represents the idea that any observed differences or effects are purely due to chance variation within this established distribution. By comparing our calculated statistic (the one coming from our data) to the expected distribution under the null hypothesis, we can assess whether the observed result is consistent with what we would anticipate if there were no real effect. In essence, this comparison allows us to make informed decisions about the significance of our findings in the context of the null hypothesis. Then, the p-value merely represent the probability of observe your observed statistic under this null distribution / hypothesis.\nIn simpler terms, when we talk about “known distributional laws,” it means we know how likely certain test results are under specific conditions. In the testing process, the null hypothesis proposes that any observed differences are merely random occurrences within the expected pattern. By comparing our actual result to what we would expect by chance, the p-value indicates the probability of our result occurring in this random scenario. If the likelihood of this chance scenario is highly improbable (typically below the 5% threshold), we reject the assumption that differences are random (reject the null hypothesis!). Conversely, if the chance scenario is ‘too probable’ (more than 5%), we conclude that it’s not safe to assert our effect is not due to chance and refrain from rejecting the null hypothesis.\nEnough talking, let’s illustrate with a graphical example. Let’s imagine we want to compare the sepal length between the species I. virginica and I. versicolor using a Student test. First, we plot the data:\n\n\n\n\n\n\n\n\n\nI. virginica seems to have slightly bigger sepal length than I. versicolor. Assuming all conditions are met, we perform a Student test to determine the significance of the difference observed:\n\nt.test(\n  x = iris$Sepal.Length[iris$Species == \"virginica\"],\n  y = iris$Sepal.Length[iris$Species == \"versicolor\"],\n  paired = F,\n  var.equal = T,\n  alternative = \"two.sided\"\n)\n\n\n    Two Sample t-test\n\ndata:  iris$Sepal.Length[iris$Species == \"virginica\"] and iris$Sepal.Length[iris$Species == \"versicolor\"]\nt = 5.6292, df = 98, p-value = 1.725e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.4221484 0.8818516\nsample estimates:\nmean of x mean of y \n    6.588     5.936 \n\n\nHere we observe a significance difference between the species I. virginica and I. versicolor with I. virigina having bigger sepal length than I. versicolor.\nLet’s get more precise on how do we get the p-value associated with this comparison and make the connection with what we saw earlier in this endless introduction paragraph. The Student distribution under null hypothesis (\\(H0\\)) is defined according the parameter ‘degree of freedom’, here equal to \\(df = 98\\). Let’s plot this \\(H0\\) probability distribution of the test we just performed:\n\n# Density distribution of the t statistic\ncurve(\n  dt(x = x, df = 98),\n  xlim = c(-6, 6),\n  ylab = \"Density\",\n  xlab = \"t\",\n  cex.axis = 1.5,\n  cex.lab = 1.5\n)\n\n\n\n\n\n\n\n\nThis distribution shows the probability associated with each value of the \\(t\\) statistic for a similar degree of freedom (\\(98\\)) with our data. It contains all the probabilities of the statistical tests assuming that ‘sepal length means do not differ between both species.’ Now, our task is to position the \\(t_{obs}\\) representing the observed test statistic we calculated from our data, which is \\(t_{obs} = 5.6\\), in this null distribution. The goal is to figure out how probable is the value 5.6 in the probability distribution of the Student’s t-distribution with \\(df=98\\).\nSince we conducted a two-sided test, meaning no a priori assumption about the direction of the effect, we split the 5% of the \\(\\alpha\\) threshold between both sides of the distribution. We will visually identify the values of \\(t\\) for which less than \\(2.5%\\) and more than \\(97.5%\\) of values are respectively below and above these thresholds. If our \\(t_{observed}\\) falls below the \\(2.5%\\) or above the \\(97.5%\\) threshold, we consider the observed difference statistically significant.\n\n# Density distribution of the t statistic\ncurve(\n  dt(x = x, df = 98),\n  xlim = c(-6, 6),\n  ylab = \"Density\",\n  xlab = \"t\",\n  lwd = 1.5,\n  cex.axis = 1.5,\n  cex.lab = 1.5\n)\n\n# Quantile function : values of t associated of a two sided test\nthreshold &lt;- qt(p = c(0.025, 0.975), df = 98)\nabline(v = c(threshold[1], threshold[2]), lwd = 1.5, lty = 2, col = \"red\")\n\n\n# Color the area of significance\nx &lt;- seq(-6, 6, length.out = 100000)\ny &lt;- dt(x, df = 98)\npolygon(c(x[x&lt;=threshold[1]], threshold[1]), c(y[x&lt;=threshold[1]], y[x == max(x)]), col = \"red\")\npolygon(c(x[x&gt;=threshold[2]], threshold[2]), c(y[x&gt;=threshold[2]], y[x == max(x)]), col = \"red\")\ntext(x = 3.7, y = 0.3, labels = TeX(\"$quantile(1 - \\\\frac{alpha}{2})$\"), col = \"red\", cex = .9)\ntext(x = -3.5, y = 0.3, labels = TeX(\"$quantile(\\\\frac{alpha}{2})$\"), col = \"red\", cex = .9)\n\n# Placing the t_obs value\nabline(v = 5.6, lwd = 1.5, col = \"blue\")\ntext(x = 5, y = 0.1, labels = TeX(\"$t_{obs}$\"), col = \"blue\", cex = 1.2)\n\n\n\n\n\n\n\n\nWith \\(\\alpha\\) the type-I error (here 5%). Hence \\(t_{obs}\\) is in the significance area (in red) meaning the difference is significant. Now we have all the reminders we need to master the conception of permutation test.\n\n\nPermutation test : test Statistic, null distribution and p-value\nThe method for inference in permutation test is very similar in conception in the sense they are based on a statistic test, a null hypothesis and the generation of a p-value. In permutation test, the statistic test \\(P_{obs}\\) have to be defined. It can basically be anything that link the different conditions of our experimental design. For example, if we want to compare an effect on a response variable dispatch into two conditions (e.g. levels of a preditive categorical variable), the statistic test could be the difference of the sums, the variances, means… of the values of both groups. For example, let’s consider a difference in means as statistic test:\n\\[\nP = \\bar{X}_1 - \\bar{X}_2\n\\]\nwhere (\\(\\bar{X}_1\\)) and (\\(\\bar{X}_2\\)) are the means of the two groups compared. In the absence of a predefined theoretical distribution, like the Student distribution mentioned earlier, we need to determine where to place the observed test statistic \\(P_{obs}\\). To do this, we construct a null distribution. This involves randomly shuffling each value between the two groups and calculating the test statistic \\(P_{H0}\\) for this newly arranged dataset. Essentially, each data point is randomly assigned to either group 1 or group 2, reflecting the idea that belonging to one group or the other has no effect on the response variable since the data has been randomly shuffled.\nWe repeat this process many times, extracting the \\(P_{H0}\\) each time to build a distribution of the test statistic under the null hypothesis (\\(H_0\\)). According to the central limit theorem, which suggests that the distribution of random variables repeated a large number of times tends to follow a bell curve, regardless of the original distribution shape of those variables. Once we have this null distribution, we follow a similar procedure as described earlier for parametric tests. We compare our \\(P_{obs}\\) to this null distribution and determine its probability. Just like in parametric tests, in permutation tests, the p-value corresponds to the number of iterations where \\(P_{H0}\\) is above or below (depending on the direction of our hypothesis) the \\(P_{obs}\\) value."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#implementation-of-permutation-tests",
    "href": "r_chapter_boostrapping_resampling.html#implementation-of-permutation-tests",
    "title": "Resampling methods",
    "section": "Implementation of Permutation Tests",
    "text": "Implementation of Permutation Tests\n\nTest Steps\nTypical steps to implement a permutation test include:\n\nFormulate null and alternative hypothesis\nDetermine and calculate the test statistic from the observed data\nGenerate a large amount of permutations on independent unit of the data and calculate the test statistic for each permutation\nCompare the observed statistic to the null distribution of test statistics\nConclude on accepting or rejecting the null hypothesis\n\n\n\nPractical examples\nHere we’ll present an example of permutation test on independent data and paired data.\n\nIndependent data (e.g. comparison of means)\nLet’s take an example to test if the difference in means between two groups is statistically significant. We’ll use the mean statistic as described earlier and pose the same question asked in the Student test example: “Is the sepal length of the genera I. virginica and I. versicolor different?” However, in this case, we’ll only use 20 randomly chosen observations to illustrate how these tests can maintain high statistical power even with a low number of replicates compared to parametric tests and how we can reach to the same conclusion as before with the whole data set with minimal sample size.\n\nset.seed(123)\n\n# SELECT THE VARIABLES OF INTEREST\nfilter_data &lt;- iris %&gt;% \n  filter(Species == \"virginica\" | Species == \"versicolor\") %&gt;% \n  droplevels()\n\n# TAKE A SUBSET\nsub_data &lt;- filter_data[sample(1:nrow(filter_data), size = 20),]\n\nLet’s plot the sub-dataset.\n\n\n\n\n\n\n\n\n\nIn this example, the data doesn’t follow a Gaussian distribution, as one might expect with such a small data set. It’s important to note that we employed a Student test here for illustration and comparison with the previous tests using the full data set.\n\n# T TEST\nt.test(\n  x = sub_data$Sepal.Length[sub_data$Species == \"virginica\"],\n  y = sub_data$Sepal.Length[sub_data$Species == \"versicolor\"],\n  paired = F,\n  var.equal = T,\n  alternative = \"two.sided\"\n)\n\n\n    Two Sample t-test\n\ndata:  sub_data$Sepal.Length[sub_data$Species == \"virginica\"] and sub_data$Sepal.Length[sub_data$Species == \"versicolor\"]\nt = 0.95304, df = 18, p-value = 0.3532\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3011112  0.8011112\nsample estimates:\nmean of x mean of y \n     6.36      6.11 \n\n\n\n# MANN-WHITNEY TEST\nwilcox.test(sub_data$Sepal.Length[sub_data$Species == \"virginica\"],\n  sub_data$Sepal.Length[sub_data$Species == \"versicolor\"],\n  paired = F)\n\nFALSE \nFALSE   Wilcoxon rank sum test with continuity correction\nFALSE \nFALSE data:  sub_data$Sepal.Length[sub_data$Species == \"virginica\"] and sub_data$Sepal.Length[sub_data$Species == \"versicolor\"]\nFALSE W = 66, p-value = 0.239\nFALSE alternative hypothesis: true location shift is not equal to 0\n\n\nHence, neither of the tests for comparing means indicates a significant difference in sepal length between the two species whether we use a Student test, which assumes normality (although not met in this case), or the Mann-Whitney non-parametric test. This is where permutation test becomes interesting. They have a great power even when the number of replicates is low. Let’s see how we proceed.\nPermuted data can be created, among various methods, by randomly assigning levels of the variable ‘species’ to sepal length values using the sample function. Subsequently, the difference of means is calculated to determine p_h0, representing the statistic for the permuted data (\\(P_{H0}\\)—the difference of means of sepal length in the permuted data). This process is repeated for a specified number of iterations (\\(i\\)), generating \\(i\\) values of \\(P_{H0}\\) and resulting in the null_distribution.\n\n# INITIAL PARAMETERS\niteration &lt;- 10000  # number of iteration\nnull_distribution &lt;- c()  # storage of the statistic for permuted data\n\n\n# ALGORITHM\nfor (i in 1:iteration)\n{\n  # --- Randomly assign a specie level to each observation \n  permuted_species &lt;- with(sub_data, sample(Species, replace = F))\n\n  # --- Difference of means of both species under h0\n  p_h0 &lt;- tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)[\"virginica\"] - \n  tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)[\"versicolor\"]\n  \n  # --- Add to null distribution\n  null_distribution[i] &lt;- p_h0\n}\n\n# CALCULATION OF THE OBSERVED STATISTIC\np_obs &lt;- with(iris, mean(Sepal.Length[Species == \"virginica\"]) - mean(Sepal.Length[Species == \"versicolor\"]))\npaste0(\"P_obs = \", p_obs)\n\n[1] \"P_obs = 0.652\"\n\n\n\nNote : here we use replace = F in the sample function to keep the same number of replicates in each levels of species.\n\nSo here we have a \\(P_{obs}\\) value of 0.652. We can now plot the null distribution we generated with the permuted data and compare our observed statistic \\(P_{obs}\\) with the null distribution. Let’s plot it to have a visual support on how permutation test works.\n\n# CALCULATE SIGNIFICANCE THRESHOLD\nbornes &lt;- quantile(null_distribution,c(.025,.975))\n\n# VISUALISATION\nhist(null_distribution,\n     xlim = c(min(null_distribution) - 0.15, 1),\n     breaks = 20,\n     main = \"\",\n     xlab = TeX(\"$P_{H0}$\"),\n     ylab = \"Frequency\",\n     col = \"skyblue\",\n     cex.axis = 1.5,\n     cex.lab = 1.5\n     )\n\n    # POSITION THRESHOLD \n    abline(v = bornes, col = \"red\", lwd = 2)\n    text(x = 0.3, y = 1350, labels = TeX(\"$quantile(1 - \\\\frac{alpha}{2})$\"), col = \"red\", cex = 0.8)\n    text(x = -0.65, y = 1350, labels = TeX(\"$quantile(\\\\frac{alpha}{2})$\"), col = \"red\", cex = 0.8)\n\n    # POSITION P_OBS\n    abline(v = p_obs, lwd = 2, col = \"darkolivegreen4\", lty = 2)\n    text(x = .75, y = 1400, labels = TeX(\"$P_{obs}$\"), col = \"darkolivegreen4\", cex = 1)\n\n\n\n\n\n\n\n\nAs anticipated by the central limit theorem, the distribution of the statistic we use, repeated 10,000 times, follows a normal distribution. It’s evident that our observed statistic calculated on the data (\\(P_{obs}\\)) stands out from the null distribution. Since we didn’t assume any specific direction for the difference (i.e. two sided test), we can determine the p-value as the number of absolut permuted observations (\\(|P_{H0}|\\)) greater than the absolute value of \\(P_{obs}\\), divided by the number of iterations:\n\\[\n\\text{p-value} = \\frac{\\text{Number of } \\big|P_{H0}\\big| &gt; \\big|P_{obs}\\big|}{\\text{Number of iterations}}\n\\]\n\nsum(abs(null_distribution) &gt; abs(p_obs)) / iteration\n\n[1] 0.0037\n\n\nHere, the p-value of the test is 0.0037, meaning the test statistic observed \\(P_{obs}\\) falls into the non-significant area only 3.7% of the time. Thus, we conclude the difference between both means is significant at a threshold \\(5\\%%\\) and that I. virginica has bigger sepal length than I. versicolor.\n\n\n\n\nPaired data (e.g. comparison of correlation)\nIn this section, we’ll demonstrate how to conduct a permutation test on paired data using the iris data set. For this purpose, we’ll imagine a scenario where a fertilizer has been applied to each individual plant. The question will still focus on sepal length, exploring whether the fertilizer induces sepal length growth in a before-and-after experimental paradigm. Consequently, the data are paired by individuals. To ensure the independence of each unit of permutation, we must constrain how values are permuted within individuals. Unlike previous permutation procedures where all data were shuffled together, here we need to permute the data within each individual to account for the non-independence of the measures within each individual.\nTo simplify the procedure, we will only consider the species I. versicolor. Let’s create the sub-data and generate the new variable.\n\nset.seed(123)\n# FILTERING DATASET FOR VERSICOLOR\ndata_versicolor &lt;-\n  data %&gt;%\n  filter(Species == \"versicolor\") %&gt;%\n  droplevels()\n\n# GENERATION OF THE NEW VARIABLE \n# --- Generation of the new variable Sepal.Length.T2 drawn drom normal distribution\nSepal.Length.T2 &lt;- round(\n  runif(nrow(data_versicolor), min = .8, max = 1.2) * data_versicolor$Sepal.Length,\n  digits = 1\n)\n  \n# --- Adding the new variable to the dataset\ndata_versicolor &lt;- \n  data_versicolor %&gt;% \n  mutate(Sepal.Length.T1 = Sepal.Length,\n         Sepal.Length.T2 = Sepal.Length.T2)\n\nLet’s plot this relation:\n\nboxplot(\n  data_versicolor$Sepal.Length.T1,\n  data_versicolor$Sepal.Length.T2,\n  col = c(\"skyblue\", \"orange\"),\n  ylab = \"Sepal Length\",\n  xlab = \"\",\n  main = \"\",\n  names = c(\"T1\", \"T2\"),\n  ylim = c(3, 9),\n  cex.axis = 1.5,\n  cex.lab = 1.5\n)\n\n\n\n\n\n\n\n\nAccording to the visualization, the effect of the fertilizer does not seem to have a clear effect on the sepal length.\nLet’s follow step by step the permutation procedure :\n\nFormulate null and alternative hypotheses\n\nH1 : the fertilizer has a positive effect on the sepal length, sepal measure at T2 is expected to be greater than T1\nH0 : the fertilizer has no effect, no differences between T1 and T2\n\nDetermine and calculate the test statistic from the observed data\nTo illustrate another example of statistic, we’ll use the difference in sum of each column to assess significance :\n\\[\nP = \\sum_{i}^n \\text{Sepal length T1}_i - \\sum_{i}^n \\text{Sepal length T2}_i\n\\]\nAs long as the statistic enable comparing both conditions, it will work ! That is part of the magic of permutation test. We calculate the test statistic for \\(P_{obs}\\):\n\np_obs &lt;- sum(data_versicolor$Sepal.Length.T1) - sum(data_versicolor$Sepal.Length.T2) \np_obs\n\n[1] -2.6\n\n\nGenerate a large amount of permutations on independent unit of the data and calculate the test statistic for each permutation\nGiven the paired nature of the data, we will randomly shuffle the values of the variable Sepal Length within each individual to construct the null hypothesis. Instead of permuting data in the entire column, as done before in the case of independent data, we will permute by row to ensure that the permutation is constrained by individual.\n\n# INITIAL PARAMETERS\niteration &lt;- 10000  # number of iteration\nnull_distribution &lt;- c()  # storage of the statistic for permuted data\n\n# ALGORITHM\nfor (i in 1:iteration)\n{\n  # --- Randomly assign age data witihin individual modality\n  permuted_data &lt;-\n    t(apply(\n      X = data_versicolor[c(\"Sepal.Length.T1\", \"Sepal.Length.T2\")],\n      MARGIN = 1,\n      FUN = sample,  # randomly shuffle the two values of each rows\n      simplify = T   # to have a matrix output (and not a list)\n    ))\n\n  # --- Difference of means of both species under h0\n  p_h0 &lt;- sum(permuted_data[,1]) - sum(permuted_data[,2])\n\n  # --- Add to null distribution\n  null_distribution &lt;- c(null_distribution, p_h0)\n}\n\nCompare the observed statistic to the null distribution of test statistics\n\n# CALCULATE SIGNIFICANCE THRESHOLD\nbornes &lt;- quantile(null_distribution,c(.95))\n# VISUALISATION\nhist(null_distribution,\n     breaks = 30,\n     main = \"\",\n     xlab = TeX(\"$P_{H0}$\"),\n     ylab = \"Frequency\",\n     col = \"skyblue\",\n     cex.axis = 1.5,\n     cex.lab = 1.5\n     )\n\n# POSITION THRESHOLD \nabline(v = bornes, col = \"red\", lwd = 2)\ntext(x = 12, y = 750, labels = TeX(\"$quantile(1 - \\\\alpha)$\"), col = \"red\", cex = .8)\n\n# POSITION P_OBS\nabline(v = p_obs, lwd = 2.5, col = \"darkolivegreen4\", lty = 2)\ntext(x = -5, y = 750, labels = TeX(\"$P_{obs}$\"), col = \"darkolivegreen4\")\n\n\n\n\n\n\n\n\nIn this section, we hypothesized a direction of the effect by stating that the sepal length would be greater after the use of the fertilizer. Hence, we’ll calculate the p-value as the number of null observations greater than the observed test statistic (not in absolute value!):\n\nsum(null_distribution &gt; p_obs) / iteration\n\n[1] 0.7029\n\n\nConclude on accepting or rejecting the null hypothesis\nAs \\(p &gt; 0.05\\), we don’t reject the null hypothesis, suggesting that a difference is not observed before and after fertilizer usage.\n\nIn the next part, we delve into the Mantel test, a statistical method used to assess the correlation between two distance matrices. This test is particularly valuable in fields such as ecology and genetics, where understanding the spatial or temporal relationships between entities is crucial."
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#mantel-tests",
    "href": "r_chapter_boostrapping_resampling.html#mantel-tests",
    "title": "Resampling methods",
    "section": "Mantel tests",
    "text": "Mantel tests\n\nUses\nThe Mantel test (Mantel 1967) is used to evaluate the relationship between two matrices of correspondences measured on the same individuals. The two correspondence (distances) matrices are denoted by \\(X\\) and \\(Y\\). Here, the null hypothesis describes the independence of the mechanisms governing the distances between individuals. The Mantel test is mainly used for testing correlations between genetics or geographical distances. The H0 distribution is obtained by permuting the individuals between them. We will use the data set “ozone” from the website UCLA (https://stats.idre.ucla.edu/stat/r/faq/ozone.csv). In this case, Mantel’s test is used to know more about correlations between ozone differences and geographical distances between stations. In other terms: Is the ozone variability independent of the geographical variability?\n\n\nCorrespondence (distances) matrices\nThe first step is to extract the distances matrix of geographical distances and ozone distances between each station. Here, we select the five first individuals of the distances matrix.\n\nset.seed(123)\n\n# Download a dataset \nozone &lt;- read.table(\"https://stats.idre.ucla.edu/stat/r/faq/ozone.csv\", sep=\",\", header=T)\n\n# We create a sub sample of the data set\n\nozone &lt;- ozone[sample(1:nrow(ozone), size = 10, replace = FALSE),]\n\n\n# Create distance matrices of geographical and ozone distances\nstation.dists &lt;- dist(cbind(ozone$Lon, ozone$Lat))\nozone.dists &lt;- dist(ozone$Av8top)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n0.00\n0.29\n4.35\n0.35\n0.40\n3.09\n3.79\n3.69\n0.44\n1.63\n\n\n2\n0.29\n0.00\n4.06\n0.64\n0.69\n2.80\n3.50\n3.40\n0.15\n1.34\n\n\n3\n4.35\n4.06\n0.00\n4.69\n4.74\n1.25\n0.55\n0.66\n3.90\n2.72\n\n\n4\n0.35\n0.64\n4.69\n0.00\n0.05\n3.44\n4.14\n4.03\n0.79\n1.98\n\n\n5\n0.40\n0.69\n4.74\n0.05\n0.00\n3.49\n4.19\n4.08\n0.84\n2.02\n\n\n6\n3.09\n2.80\n1.25\n3.44\n3.49\n0.00\n0.70\n0.59\n2.65\n1.46\n\n\n7\n3.79\n3.50\n0.55\n4.14\n4.19\n0.70\n0.00\n0.11\n3.35\n2.17\n\n\n8\n3.69\n3.40\n0.66\n4.03\n4.08\n0.59\n0.11\n0.00\n3.24\n2.06\n\n\n9\n0.44\n0.15\n3.90\n0.79\n0.84\n2.65\n3.35\n3.24\n0.00\n1.19\n\n\n10\n1.63\n1.34\n2.72\n1.98\n2.02\n1.46\n2.17\n2.06\n1.19\n0.00\n\n\n\n\n\n\n\nThe distances matrix must be a squared matrix. Now we can apply the Mantel test our to matrix.\n\n\n\n\nMantel’s test application\nThere are some packages available to perform the Mantel test. For example, the ade4 package with the mantel. rtest() function as follows:\n\nset.seed(seed = 123) \nmod_ade &lt;- ade4::mantel.rtest(as.dist(station.dists), as.dist(ozone.dists), nrepet = 1000)\nmod_ade\n\nMonte-Carlo test\nCall: ade4::mantel.rtest(m1 = as.dist(station.dists), m2 = as.dist(ozone.dists), \n    nrepet = 1000)\n\nObservation: 0.2169789 \n\nBased on 1000 replicates\nSimulated p-value: 0.08691309 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n 1.587656484 -0.001886359  0.019003800 \n\n\nThe coefficient of correlation observed is -0.2 and the p-value is 0.9. If we put the threshold of significance at 0.05 the correlation is not significant here!\nAnother example of package is the vegan package which allows to use of different correlation statistics. In our case, we chose the Pearson correlation.\n\n#Using the vegan package, pearson correlation\n\nmantel(xdis = station.dists, ydis = ozone.dists, permutations = 1000)\n\n\nMantel statistic based on Pearson's product-moment correlation \n\nCall:\nmantel(xdis = station.dists, ydis = ozone.dists, permutations = 1000) \n\nMantel statistic r: 0.217 \n      Significance: 0.082917 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.188 0.279 0.366 0.468 \nPermutation: free\nNumber of permutations: 1000\n\n\nUsing this package, the results are similar. well! The p-value is indicated after the term “Significance:” and the correlation coefficient follows the term “Mantel statistic r”.\n\n\nThe mantel test evaluates the significance of the regression line between the two distance matrix. We saw that the mantel test is not significant. We detected no correlations between distances. Let’s plot the results:"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#permutation-test-on-bootstrapped-data",
    "href": "r_chapter_boostrapping_resampling.html#permutation-test-on-bootstrapped-data",
    "title": "Resampling methods",
    "section": "Permutation test on bootstrapped data",
    "text": "Permutation test on bootstrapped data\nExploring the precision of our calculated statistic becomes particularly insightful. To achieve a robust assessment, we could opt for a combined approach, integrating both bootstrap and permutation methodologies. In this process, we would initiate a permutation test on each pseudo-data set generated through bootstrapping. The objective is to collect the statistic for each iteration, resulting in a distribution that effectively communicates the variance of estimation linked to the difference in means. This combined technique not only provides a confidence interval around our statistic but also enhances the overall reliability of our findings by accounting for the complexities and uncertainties inherent in the data. Note that combining both re-sampling techniques can be computationally intensive.\n\n\nTo illustrate, we will consider construct the following model :\n\\[\n\\text{Sepal length} = \\beta + \\alpha(\\text{Sepal Width}) + \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nWith \\(\\beta\\) representing the intercept, \\(\\alpha\\) as the regression coefficient, and \\(\\epsilon\\) denoting the error. The objective is to ascertain (i) the confidence interval of the regression estimate \\(\\alpha\\) and (ii) the associated p-value. To achieve this, we will utilize a subset of the iris dataset comprising 25 randomly selected samples of the I. versicolor species:\n\nset.seed(123)\n\n# SUBSET OF IRIS (n=25)\nsub_data &lt;- iris[iris$Species == \"versicolor\",] \nsub_data &lt;- sub_data[sample(1:nrow(sub_data), size = 25, replace = F),]\n\nNow, the plan is to conduct \\(j\\) bootstrap datasets to evaluate the confidence intervals for both estimates \\(\\alpha_{obs}\\) and \\(\\beta_{obs}\\). For each of these \\(j\\) bootstrapped datasets, we will execute \\(i\\) permutation tests and calculate the mean value of the estimates from these \\(i\\) permutation tests to construct our null distribution. Subsequently, we will determine the confidence interval of the bootstrapped distribution of \\(\\alpha\\) and assess its significance by comparing it to the null distribution. Let’s proceed. First, we calculate the observed \\(\\alpha\\):\n\n# OBSERVED ALPHA\nalpha_observed &lt;- round(lm(Sepal.Length ~ Sepal.Width, data = sub_data)$coefficients[[2]], 2)\nalpha_observed\n\n[1] 0.98\n\n\nOur observed \\(\\alpha = 0.98\\). We’ll keep it for later. Now, we’ll perform the combined bootstrapped / permutation procedure :\n\nset.seed(123)\n\n# NUMBER OF ITERATION\nbootstrap_iteration &lt;- 10000\npermu_iteration &lt;- 10\n\n# STORAGE\n# --- Null distribution\nnull_distribution &lt;- matrix(NA, nrow = bootstrap_iteration, ncol = 1)\ncolnames(null_distribution) &lt;- \"alpha\"\n\n# --- Storage of mean of permuted values\npermu_a &lt;- c()\n\n# --- Storage of the bootstrap value of a\nboot_a &lt;- c()\n\n\n# ALGORITHM\n# BOOTSTRAP\nfor(j in 1:bootstrap_iteration){\n  \n  # --- Random sampling with replacement of rows tp generate bootstrap data \n  row_alea &lt;- sample(1:nrow(sub_data), replace = T)\n  boot_data &lt;- sub_data[row_alea,]\n  \n  # --- Model with bootstraped data\n  mod &lt;- lm(Sepal.Length ~ Sepal.Width,\n            data = boot_data  # bootstraped data\n            )\n  \n  # --- Store the boostrapped estimates\n  boot_a[j] &lt;- mod$coefficients[[2]]\n  \n  \n  # PERMUTATION OF boostrapped DATA\n   for (i in 1:permu_iteration){\n  \n     \n  # --- random sampling WITHOUT replacement\n  y_permuted &lt;- sample(boot_data$Sepal.Length, replace = F)\n  \n  # --- Model with permuted data\n  mod_permu &lt;- lm(y_permuted ~ Sepal.Width,\n                  data = boot_data)\n  \n  # --- Store of the estimates of permuted data\n  permu_a[i] &lt;- mod_permu$coefficients[[2]]\n  \n   }\n  \n # --- Mean values of the i permuted values of estimates for the j bootstrap iteration\n null_distribution[j] &lt;- mean(permu_a)\n \n}\n\nLet’s plot our two distributions :\n\n\n\n\n\n\n\n\n\nWe observe a notable distinction between the bootstrapped distribution and the null distribution. To formalize this, we calculate the p-value, defined as the overlap between both distributions:\n\nsum(null_distribution &gt; boot_a) / bootstrap_iteration\n\n[1] 0.0012\n\n\nWe obtain a p-value of \\(p = 0.0012\\), indicating that the coefficient is significantly greater than chance! Now, let’s leverage our combined techniques to calculate the confidence interval of this newly established significant \\(\\alpha\\) along with the associated p-value:\n\nround(quantile(boot_a, probs = c(0.025,0.975)), 2)\n\n 2.5% 97.5% \n 0.40  1.84 \n\n\nWe have an estimate of \\(\\alpha = 0.98 \\ [0.40, 1.84]\\) with a \\(95%\\) confidence interval. To further demonstrate the reliability of permutation tests, let’s compare these results with the \\(\\alpha\\) estimate of the entire dataset. Remember that we only used half of the original dataset !\n\noriginal_data_alpha &lt;- round(lm(Sepal.Length ~ Sepal.Width, data = iris[iris$Species == \"versicolor\",])$coefficients[[2]], 2)\n\noriginal_data_alpha\n\n[1] 0.87\n\n\nSo \\(0.87 \\in [0.40, 1.84]\\) ! With only half the original sample size, we achieved the exact same conclusion with a bootstrap and permutation procedure. Although it takes a bit more computation time and resources, this procedure is really efficient even with low sample size !"
  },
  {
    "objectID": "r_chapter_boostrapping_resampling.html#conclusion-on-permutation-tests",
    "href": "r_chapter_boostrapping_resampling.html#conclusion-on-permutation-tests",
    "title": "Resampling methods",
    "section": "Conclusion on permutation tests",
    "text": "Conclusion on permutation tests\nIn conclusion, permutation tests offer a powerful and flexible approach to statistical inference, especially when traditional parametric assumptions cannot be met. By iteratively permuting the observed data, these tests generate a null distribution under the assumption of no effect, enabling the assessment of the observed statistic’s significance. This makes permutation tests applicable in various scenarios, including comparisons of means, medians, correlations, or any other measure of interest.\nWhile permutation tests can be computationally intensive (especially when combined with bootstrap procedures), their key advantage lies in providing reliable results with small sample sizes and non-normally distributed data. Unlike parametric tests, permutation tests do not rely on assumptions about the underlying distribution of the data, making them robust and applicable in a wide range of situations."
  },
  {
    "objectID": "r_chapter.html",
    "href": "r_chapter.html",
    "title": "The Companion book a M1 MODE student should have!",
    "section": "",
    "text": "title: “Modèles matriciels et applications” group: “Quentin, Alexie, Jeanne, Laurane”\nbibliography: references.bib execute: freeze: auto output: html_document: toc: true toc_float: true —\nThis chapter is a simple example using R\nYou can import R package using the code\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nand then describe the purpose of your chapter as well as executing R command.\nFor example a basic summary of a dataset is given by\n\ndf &lt;- read.table(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\", sep = \",\" , header = TRUE)\n\nand produce a graph\n\ndf %&gt;% ggplot() +\n    aes(x=species, y = body_mass_g) +\n    geom_boxplot()  \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nA citation @bauer2023writing"
  },
  {
    "objectID": "prior_course.html",
    "href": "prior_course.html",
    "title": "Be ready for the course",
    "section": "",
    "text": "The Online Collaborative Resources (OCR) class will make intensive use of the Reproducible Science tools. For this class and as an investment for the future, you will need to:"
  },
  {
    "objectID": "prior_course.html#git-on-your-computer",
    "href": "prior_course.html#git-on-your-computer",
    "title": "Be ready for the course",
    "section": "Git on your computer",
    "text": "Git on your computer\nInstall Git on your personal computer: Go to the Git website and click on the computer screen on the right, which should offer you the version suitable for your operating system."
  },
  {
    "objectID": "prior_course.html#github-account",
    "href": "prior_course.html#github-account",
    "title": "Be ready for the course",
    "section": "GitHub account",
    "text": "GitHub account\nIf you don’t have one, you have to create a GitHub account and set up the link between your computer and the GitHub. Be careful with the login you choose, this account will be used professionally and you may want to avoid pseudo like toto2024, or ladybug288.\nGitHub is just one solution to share git repositories online, you may work later with gitlab with is very similar to GitHub."
  },
  {
    "objectID": "prior_course.html#a-ssh-connection",
    "href": "prior_course.html#a-ssh-connection",
    "title": "Be ready for the course",
    "section": "A SSH connection",
    "text": "A SSH connection\nTo have smooth interactions with Github, you have to use a SSH and to do so, you will to generate SSH key on your computer and copy paste the public SSH key id_rsa.pub on your GitHub account.\n\nOpen a terminal (any terminal on Mac and Linux, the Git bash program you have installed with git if you are using Windows)\ntype the command\nssh-keygen\nDO NOT ENTER ANY PASSPHRASE while asked, simply press enter (twice generally)\nYou should have a directory named .ssh in your main personnal folder. Open the file id_rsa.pub with any text basic editor (like notepad, gedit …) and copy the key.\nGo to the settings of your GitHub account, choose the SSH and GPG keys, then press New SSH key and paste the previously copied key."
  },
  {
    "objectID": "prior_course.html#let-me-know-who-you-are",
    "href": "prior_course.html#let-me-know-who-you-are",
    "title": "Be ready for the course",
    "section": "Let me know who you are",
    "text": "Let me know who you are\nPlease enter your name and GitHub login on this spreadsheet and indicate in the last column whether you are already familiar with some markup languages (markdown, HTML) and if you have prior experience with Git or any version control system.\nPlease try to install all of this as soon as possible, and if you encounter any difficulties, don’t hesitate to contact me by email or come to me at the beginning of the class.\nIt will be easier if you can follow this procedure on your laptop and bring it to the class, but if you don’t have any laptop we will able to use the computers in the classroom (however this tedious installation process will have to be repeated on your personal computer)"
  },
  {
    "objectID": "bayesian_modeles.html",
    "href": "bayesian_modeles.html",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "",
    "text": "History and statement of the theorem: Biostatistics is a mathematical discipline based on statistics applied to living organisms (Vergnault, 2013). In ecology, the mathematical formulation of ecological and biological variables enables in-depth analysis of ecosystem functioning (e.g., population dynamics, quantitative ecology) (Zuur et al., 2007). Since its inception in the 17th century, many theorists have studied the subject, and today biostatistics is at the heart of our understanding of the living world (Williams, 2017). One of them, Thomas Bayes, is at the origin of one of the most widely used branches of biostatistics: Bayesian statistics. This 18th-century English pastor, who wore many hats (mathematics, geometry, theology, etc.), developed a central probability theorem known as Bayes’ Theorem (Droesbeke et al., 2002). The need to create new statistics is linked to Thomas Bayes’ astonishing life experience, as he navigated between belief and science. What’s more, in the eighteenth century, religion was still at the heart of everyday life throughout Europe, regardless of denomination (in this case, the Anglican Church) (Bellhouse, 2004). The mathematician was therefore faced with the question of how to formulate probable scientific hypotheses, including random parameters and admitting the unknown, a priori, of an experiment. Implicitly, this means that it is necessary to consider the results of the test even before exploiting the data (Cornfield, 1967). It is formulated in such a way that for (An) a complete system of events, all of non-zero probability, for any event Y, we have (Traonmilin and Richou, 2018): \\[ P(\\theta \\mid Y) = \\dfrac{P(Y \\mid \\theta) \\cdot P(\\theta)}{P(Y)} \\] It therefore states the relationship between the probability of one event (A) occurring and the probability of another event (B) occurring. Bayesian theory is applied upstream of the drafting of statistical hypotheses, hence the a priori terminology often used to describe this approach (Puga et al., 2015). The benefits of this technique for ecology: Bayesian statistics were shelved for almost two centuries, as they were too complex to perform and required a great deal of computing power. However, it was in the 1970s and 1980s that Bayesian statistics enjoyed a new lease of life (Traonmilin and Richou, 2018). This new impetus came at a time when the scientific world was becoming increasingly aware of ecological issues (Zimmermann, 2020). Recent discoveries in computing and computing power also made it possible to carry out more advanced analyses, and thus to integrate the use of Bayesian statistics. The democratization of the use of predictive models also contributes to the spread of these probability calculations (Manabe and Smagorinsky, 1967). In the field of ecology, one of the main difficulties in analysing processes operating in ecosystems is the complexity of interactions and the abundance of variables. These same variables are not always observable, and their influence can escape the observer’s gaze (Zuur et al., 2007) . This form of statistics thus makes it possible to create a link between non-visible variables and their observable repercussions on the environment. The main objective of the Bayesian approach is therefore to assess the probability that there is an effect of these unperceived variables (Wikle, 2003). In this study, we attempt to take stock of the use of Bayesian methods in ecology and what they can offer in comparison with so-called “frequency” statistics, through concrete examples using modeling tools."
  },
  {
    "objectID": "bayesian_modeles.html#explanation-in-the-special-case-of-conjugation-between-prior-and-posterior",
    "href": "bayesian_modeles.html#explanation-in-the-special-case-of-conjugation-between-prior-and-posterior",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Explanation in the special case of conjugation between prior and posterior",
    "text": "Explanation in the special case of conjugation between prior and posterior\nFor context first, let’s take a simple example to explain the idea behind the Bayesian method. We want to estimate the mean abundance per \\(m^2\\) of one fungus in a forest. To do that, we set up some sampling areas in which we count the number of mushrooms. We can simulate the data by taking random observations in a Poisson distribution. Let’s suppose that our study has 200 sampling area. Because it is count data, the number of mushrooms that we will count should follow a Poisson distribution :\n\nn_sample = 200 # sampling area\nlbda = 5 # mean of poisson distribution\npois_distr = dpois(1:20, lambda = lbda)\nplot(pois_distr,type =\"h\",\n     lwd = 2, col = 'blue',\n     xlab = \"Mushroom Count\",\n     ylab = expression(paste( 'Density or ','[y]')) )\n\n\n\n\nTo simulate the sampling campaign we are taking values in this Poisson distribution. In our case, those will be used to estimate the mean number of mushrooms in the forest. (\\(\\lambda = 5\\) is already known because we simulate the data, but in reality, this is an unknown).\n\nset.seed(1000)\nY = rpois(n = n_sample, lambda = lbda)\nhist(Y)\n\n\n\n\nSo let’s pretend that we don’t know the \\(\\lambda\\). We want to find the the value of the mean number of mushrooms we will call \\(\\hat{\\lambda}\\) and we also want to know the probability of this \\(\\hat{\\lambda}\\). The Bayesian method will give us a range of estimated mean \\(\\hat{\\lambda}\\) and the probability of those values to be true knowing the observation \\(Y\\). This is what we call the posterior distribution. We can simply write this as follows \\([\\hat{\\lambda} \\mid Y]\\) which is the probability of \\(\\hat{\\lambda}\\) knowing our observations. This probability can be found with the equation : \\[ [\\hat{\\lambda} \\mid Y] = [Y \\mid \\hat{\\lambda}  ]\\cdot[\\hat{\\lambda}] \\] The two components of the right-hand side of the equation are \\([Y \\mid \\hat{\\lambda} ]\\) the likelihood of our data and \\([\\hat{\\lambda}]\\) the prior distribution. First, let’s start with the likelihood. Our data follow a Poisson distribution so our likelihood will follow a Poisson distribution : \\[ L(\\hat{\\lambda} ; y)=[y \\mid \\hat{\\lambda}] = \\frac{e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y}}{y!} \\] This is a first good step, but there is a small issue here, this formula isn’t completely usable in this form. This is because it can only take one observation. In English words, it is like asking what is the probability of one observation (one count of mushroom) given a model with a mean \\(\\hat{\\lambda}\\). This form isn’t powerful enough because it uses only one observation. What we want is to use all the data that we have, we want to know the probability of all the observations given a model with a mean \\(\\hat{\\lambda}\\). To do so we can write the likelihood of all our data \\(Y\\) as the product of the likelihood of each observation \\(y_i\\). (We are allowed to do this only because observations are independent) \\[ \\begin{align} [Y \\mid \\hat{\\lambda}] &= \\prod^{n}_{i=1}[y_i \\mid\n\\hat{\\lambda}] \\\\ &=\\prod^{n}_{i=1}\\frac{e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y_i}}{y_i!} \\\\ &\\propto \\prod^{n}_{i=1}e^{-\\hat{\\lambda}}\n\\cdot\\hat{\\lambda}^{y_i}\\\\ \\end{align} \\] As you can see we don’t keep the \\(\\frac{1}{y_i!}\\), it is because we are only interested in the terms that are impacted by \\(\\hat{\\lambda}\\). The last form which is proportional to the likelihood function has a more convenient form for the next step. Let’s rearrange the function in a more convenient form \\[ \\begin{align} \\prod^{n}_{i=1}e^{-\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{y_i}&=\ne^{-n \\cdot\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{\\sum^{n}_{i=1} y_i}\\\\ &= e^{-n\n\\cdot\\hat{\\lambda}} \\cdot\\hat{\\lambda}^{n \\cdot \\bar{y}}\\\\ \\end{align} \\] The first line just uses the power/exponential multiplication properties. The second line is just a writing simplification that is common in other resources, \\(mean(y)=\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n \\Rightarrow n\\cdot \\bar{y} = \\hat{\\lambda}^{\\sum^{n}_{i=1} y_i}\\). Now we want to find the prior distribution of \\(\\hat{\\lambda}\\). First, we have to choose the distribution family of our prior. We will use a Gamma distribution. We are using this one because it lets the prior and the posterior have the same distribution family, it is called conjugate distributions. The prior is called a conjugate prior for the likelihood function. This means that the prior function and the likelihood function have the same form! And this means that we can simplify! let’s try it : \\[ \\begin{align} [\\hat{\\lambda}]&\\sim Gamma(\\lambda,\\alpha_p,\\beta_p) \\\\ &=\n\\lambda^{\\alpha_p -1}\\frac{\\beta_p^\\alpha\\cdot e^{-\\beta_p\n\\lambda}}{\\Gamma(\\alpha_p)} \\propto \\lambda^{\\alpha_p -1} \\cdot e^{-\\beta_p\n\\lambda} \\end{align} \\] Same as the likelihood, we are only interested in the term that varies with \\(\\hat{\\lambda}\\) so we remove \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\) and keep the proportional formula of the prior. We can now find the real formula of our posterior distribution. \\[ \\begin{align} [\\lambda \\mid y] &= [y \\mid \\lambda]\\cdot[\\lambda] \\\\ [\\lambda\n\\mid y] &\\propto e^{-n \\cdot\\lambda} \\cdot\\lambda^{n \\cdot \\bar{y}} \\cdot\n\\lambda^{\\alpha_p -1} \\cdot e^{-\\beta_p \\lambda}\\\\ [\\lambda \\mid y] &\\propto\ne^{-\\beta_p \\lambda-n\\lambda} \\cdot\\lambda^{n\\bar{y}+\\alpha_p -1} \\\\ [\\lambda\n\\mid y] &\\propto e^{-\\lambda (\\beta_p +n)} \\cdot\\lambda^{n\\bar{y}+\\alpha_p -1}\\\\\n\\end{align} \\] Does the last formula remind you of something familiar? That’s right it is a Gamma distribution! This is the magic of the conjugate distributions. We can now write : \\[ \\begin{align} [\\lambda \\mid y] &\\propto e^{-\\lambda \\beta}\n\\cdot\\lambda^{\\alpha -1}\\\\ [\\lambda \\mid y] &\\sim Gamma(\\alpha_p +n\\bar{y},\n\\beta_p +n) \\end{align} \\] We can do some simulations to show the results. We are looking for the probability of \\(\\hat{\\lambda}\\), so for the computation we create a vector of all \\(\\hat{\\lambda}\\) for which we want to know the probability:\n\nlambda_hat &lt;- seq(0,10, by = 0.01)\n\nAnd now, in order to compare them, we compute the distribution of the prior and the posterior (which are both following a gamma distribution) with an increasing amount of sample:\n\nalph = 1\nbet= 1\npar(mfrow = c(2,3))\nn_obs = 0\nfor(n_obs in list(0,1:5,1:10,1:50,1:100,1:200)){\n  # prior distribution\n  l_prior = dgamma(lambda_hat, shape = alph, rate = bet)\n  # posterior distribution\n  l_post = dgamma(lambda_hat, shape =  alph + sum(Y[n_obs]), rate = bet + max(n_obs))\n  plot(lambda_hat,l_prior, ylim = c(0,max(c(l_post,l_prior))),\n       type = 'l', lwd = 2, col = 'orange',\n       xlab = expression(lambda),\n       ylab = expression(paste('[', lambda, '|y]')) )\n  lines(lambda_hat,l_post, type = 'l',lty = 3, lwd = 2, col = 'purple')\n  abline(v = 5, lty = 2, lwd = 2)\n  title(paste(\"n = \",max(n_obs)))\n}\n\n\n\n\nWhen we add no data in the computation of the posterior, it is normal that we don’t see any modifications from the prior. Adding 5 observation already bring some good information, the prior and the posterior have no longer the same shape and we have a better estimation of the true \\(\\lambda\\). Increasing the number of data gives us a better approximation of the true mean. The density of probability is also higher with a lot of data because we have more confidence in the approximation."
  },
  {
    "objectID": "bayesian_modeles.html#the-role-of-prior-knowledge",
    "href": "bayesian_modeles.html#the-role-of-prior-knowledge",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "The role of prior knowledge",
    "text": "The role of prior knowledge\nIn Bayesian statistics, prior knowledge and beliefs play a central role in the formulation and interpretation of Bayesian models. As explained before, the fundamentals of Bayesian inference lies in combining prior information with observed data to obtain updated or posterior probabilities.\nBayesian analyses differ from frequentist analyses by incorporating prior information via conditional probabilities, known as Bayes’ rule (Nathan P. Lemoine, 2019): \\[\\frac{[\\theta]\\cdot[Y\\mid\\theta]}{[Y]}=[\\theta\\mid Y]\\] With Pr(θ) = the probability of the parameter or hypothesis θ based on prior information Pr(Y|θ) = the likelihood of the data conditioned on the hypothesis Pr(Y) = the normalization constant Pr(θ|Y) = the posterior probability of the of the hypothesis conditioned on the observed data.\nIncorporating existing information to a data set can be based on previous studies, expert opinions, historical data or simply known subjective beliefs. It will allow the future model to avoid over-fitting and favor a more plausible and simple estimation."
  },
  {
    "objectID": "bayesian_modeles.html#informative-and-non-informative-priors",
    "href": "bayesian_modeles.html#informative-and-non-informative-priors",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Informative and non-informative priors",
    "text": "Informative and non-informative priors\nUse of informative priors: In practice, we want to base our decisions on all available information. Therefore, it is considered responsible to include informative priors in applied research whenever possible. Priors allow you to combine information from the literature, from the data, or to combine information from different datasets.\nUse of non-informative priors: In basic research where the results should only reflect the information in the current dataset. Results from a case study can be used in a meta-analysis which assumes independence between the different included studies.\nAn illustration of how two different priors affect the posterior distribution in a Beta-Binomial model:\n\n# Setup the aesthetic of future plot\nmain_theme = theme_bw()+\n  theme(line = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        panel.border = element_blank(),\n        axis.ticks =  element_line(colour = \"black\"),\n        axis.text.x = element_text(colour = \"black\", size=14,\n                                   angle = 45, hjust = 1),\n        axis.text.y = element_text(colour = \"black\", size=14),\n        legend.title = element_text(colour = \"black\", size=14),\n        legend.title.align=0.5,\n        legend.text = element_text(colour = \"black\", size=14),\n        axis.title=element_text(size=22),\n        strip.background = element_rect(fill=\"white\"))\n\n\n# for non-informative uniform prior\n# Define the range of theta values\ntheta &lt;- seq(0, 1, by = 0.01)\n# Define the number of trials and successes\nn &lt;- 10\nx &lt;- 5\n# Calculate likelihood, prior, and posterior for non-informative uniform prior\n\nlikelihood &lt;- dbinom(x = x, size = n, prob =theta)\nprior_uniform &lt;- dunif(min = 0, max = 1,x = theta)\nposterior_uniform &lt;- likelihood * prior_uniform \n# Create data frame for plotting\ndf_uniform &lt;- data.frame(theta, likelihood, prior_uniform, posterior_uniform)\n# Plotting the graph \ngraph1 &lt;- ggplot(data = df_uniform, aes(x = theta)) +\n  geom_line(aes(y = posterior_uniform, color = \"Posterior\"), linewidth = 1) +\n  geom_line(aes(y = likelihood, color = \"Likelihood\"), linetype = \"dashed\", linewidth = 1) +\n  geom_line(aes(y = prior_uniform, color = \"Prior\"), linewidth = 1) +\n\n  labs(title = \"With Non-informative Uniform Prior\",\n       x = \"Theta (Probability of Success)\",\n       y = \"Density\",\n       color = \"Legend\") +\n  scale_color_manual(values = c(\"black\",'purple', \"orange\"),\n                     labels = c(\"Likelihood\",\"Posterior\", \"Prior\"))+\n  main_theme \n\n## for highly informative uniform prior\n\n# Calculate new prior, and posterior for highly informative prior\n\nprior_informative &lt;- dbeta(theta, 2, 2)\nposterior_informative &lt;- likelihood * prior_informative\n# posterior_rescaled &lt;- posterior_informative * (max_likelihood / max_posterior) * scale_factor_posterior\n# # Create data frame for plotting\ndf_informative &lt;- data.frame(theta, likelihood, prior_informative, posterior_informative)\n# Plotting the graph \ngraph2 &lt;- ggplot(data = df_informative, aes(x = theta)) +\n  geom_line(aes(y = posterior_informative, color = \"Posterior\"), linewidth = 1) +\n  geom_line(aes(y = likelihood, color = \"Likelihood\"), linetype = \"dashed\", linewidth = 1) +\n\n  geom_line(aes(y = prior_informative, color = \"Prior\"), linewidth = 1) +\n\n  labs(title = \"With Highly Informative Prior\",\n       x = \"Theta (Probability of Success)\",\n       y = \"Density\",\n       color = \"Legend\") +\n\n  scale_color_manual(values = c(\"black\", 'purple', \"orange\"),\n                     labels = c(\"Likelihood\", \"Posterior\", \"Prior\"))+\n  main_theme\n\ngraph1\n\n\n\ngraph2\n\n\n\n\nHere we illustrate the posterior distribution with a non informative prior is equal to the likelihood. But with knowledge, we can take an informative prior,it is shown on the second figure where we have a strong prior center on a value of success of 0.5, thus the posterior isn’t equal to the likelihood anymore. This emphasizes that the impact of the chosen prior influences the posterior distribution with both its shape and the quantity of information it holds compared to the information within the data."
  },
  {
    "objectID": "bayesian_modeles.html#metropolis-hastings-algorithm-implementation",
    "href": "bayesian_modeles.html#metropolis-hastings-algorithm-implementation",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Metropolis-Hastings algorithm : implementation",
    "text": "Metropolis-Hastings algorithm : implementation\nLet’s implement the algorithm to try to understand how it works. In the following lines, we will describe the various steps of the Metropolis-Hastings algorithm along with the corresponding R code lines.\n\nStep 1 : Definition of the likelihood function and the prior law\nAs with the beginning of any Bayesian statistical analysis, we first pose the problem and define the likelihood probability distribution and the prior distributions of the parameters to be estimated. We define the likelihood as a binomial distribution with parameters \\(N\\) as the total population size and \\(p\\) as the probability of capturing individuals. It is this parameter \\(p\\) that we aim to estimate. We make the assumption that we have no prior information about this parameter, so we choose a non-informative prior: \\[ p \\sim \\beta(1,1) \\]\n\n# [y | p]\nlikelihood&lt;- function(p, y, N){\n  if(p &lt; 0 | p &gt; 1) {\n    return(0)\n  } else {\n    return(dbinom(x = y, size = N, prob = p))\n  }\n}\n\n\n# [p]\nprior.dist = function(p, a.prior = 1, b.prior = 1){\n  dbeta(x = p, shape1 = a.prior, shape2 = b.prior)\n}\n\n\n\nStep 2 : Definition of a candidate position\nWe will now aim to move, so we need to define a function to determine a candidate position. Here, we randomly draw this position from a normal distribution with a mean of \\(p_c\\), which is the current position, and we arbitrarily choose a standard deviation value.\n\nmove  &lt;- function(p, sd.explore = 0.1){\n  candidate &lt;- rnorm(1, mean = p, sd = sd.explore)\n  return (candidate)\n}\n\n\n\nStep 3 : Compute of the ratio\nOnce we have this candidate position, we need to decide whether to keep it or not. The decision criterion to calculate is the Metropolis-Hastings ratio \\(r\\), which we define as: \\[ r = \\dfrac{[p_{t+1} \\mid Y]\\space \\cdot \\space [p_{t+1}] \\space \\cdot \\space\ng(p_{t+1} \\mid p_{t})}{[\\lambda_{t} \\mid Y]\\space \\cdot \\space [p_{t}] \\space\n\\cdot \\space g(p_{t} \\mid p_{t+1})} \\] where \\(g(p_{t+1} \\mid p_{t})\\) is the probability of transitioning from the candidate position to the current position.\n\nproba_move &lt;- function(p1, p2, sd.explore = 0.1){\n  dnorm(p1, mean = p2, sd = sd.explore)\n}\n\n\nMH.ratio &lt;- function(p_c,p, y, N){\n  ratio = (likelihood(p_c, y, N) * prior.dist(p) * proba_move(p, p_c))/\n    (likelihood(p, y, N) * prior.dist(p) * proba_move(p_c, p))\n  return(ratio)\n}\n\n\n\nStep 4 : Decide if we go to the candidate position or not\nTo choose if we keep the candidate position \\(\\lambda_{t+1}\\) we define \\(u\\) : \\[ u \\sim unif(0,100) \\] If \\(u\\) is greater than the ratio \\(r\\), we remain at the current position; conversely, if \\(u\\) is less than \\(r\\), we transition to the candidate position.\n\n# parameter algorithm\nn_iter = 10000\nthin = 10\n#data\ny = 3\nN = 10\n#initialization\np_init = 0.5\np_sample = rep(NA, n_iter)\np_save = rep(NA, n_iter)\np_sample[1] = p_init\ni=2\nfor(i in 2:n_iter){\n  p = p_sample[(i-1)]\n  p_c = move(p)\n  ratio = MH.ratio(p_c,p, y, N)\n  if(runif(1)&lt;ratio){\n    p_sample[i] = p_c\n  }else{\n    p_sample[i] = p\n  }\n  }\np_save = p_sample[seq(1,n_iter, by= thin)]\ndata = data.frame(iteration = 1:length(p_save),\n                    step = p_save)\n\nThere is a parameter we haven’t discussed in this function, and that’s \\(thin\\). It means that we will only save the samples within the chain at intervals of \\(thin\\). This is due to the fact that the samples are correlated with each other as they depend on the previous position and therefore do not accurately reflect the distribution. Consequently, if we want independent samples, we must discard the majority of samples and keep only one sample every \\(thin\\) steps, with \\(thin\\) being “sufficiently large.”\n\n\nStep 5 : Visualization\n\nplot(step~iteration, data, \"l\")\n\n\n\np1 &lt;- ggplot(data = data)+\n  geom_histogram(aes(x = step))+\n  theme_bw()\np1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nalph = 1\nbet= 1\n\nBoth graphs represent the sampling of \\(p\\) in its marginal posterior distribution. This is possible when a sufficient number of iterations is used, allowing the Markov chain to reach a stationary state."
  },
  {
    "objectID": "bayesian_modeles.html#the-case-of-multiple-parameters",
    "href": "bayesian_modeles.html#the-case-of-multiple-parameters",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "The case of multiple parameters",
    "text": "The case of multiple parameters\nAnd how does it work in multiple dimensions? Obviously, above two parameters to estimate, visualization becomes impossible. So, to keep it visual, we will only perform an example with one additional parameter to estimate. We will use a simple example: estimating a population mean and standard deviation. We’ll define some population-level parameters, collect some data, then use the Metropolis-Hastings algorithm to simulate the joint posterior of the mean and standard deviation. Let’s start by simulating some data.\n\n# population level parameters\nmu &lt;- 7\nsigma &lt;- 3\n# collect some data (e.g. a sample of heights)\nn &lt;- 50\nx &lt;- rnorm(n, mu, sigma)\n\nThen, as previously, we define the likelihood and the prior distributions for the parameters to be estimated \\(\\mu\\) and \\(\\sigma\\): \\[ Y \\sim N(\\mu,\\sigma) \\]\n\n# likelihood function\nll &lt;- function(x, muhat, sigmahat){\n  sum(dnorm(x, muhat, sigmahat, log = T))\n}\n\n\\[ \\mu \\sim N(0,100) \\] \\[ \\sigma \\sim unif(0,10) \\]\n\n# prior\npmu &lt;- function(mu){\n  dnorm(mu, 0, 100, log = T)\n}\npsigma &lt;- function(sigma){\n  dunif(sigma, 0, 10, log = T)\n}\n\nIn the context of the Metropolis-Hastings algorithm, the focus is typically on probability ratios rather than the probabilities themselves. Logarithms simplify calculations and improve numerical stability in this context, especially for the ratio calculation. Indeed, as mentioned earlier, we performed: \\[ r_t = \\dfrac{[\\theta_{t+1} \\mid Y]\\space \\cdot \\space [\\theta_{t+1}] \\space\n\\cdot \\space g(\\theta_{t+1} \\mid \\theta_{t})}{[\\theta_{t} \\mid Y]\\space \\cdot\n\\space [\\theta_{t}] \\space \\cdot \\space g(\\theta_{t} \\mid \\theta_{t+1})} \\] Which is equal to : \\[ log(r_t) = [\\theta_{t+1} \\mid Y]\\space \\cdot \\space [\\theta_{t+1}] \\space +\n\\space g(\\theta_{t+1} \\mid \\theta_{t}) - [\\theta_{t} \\mid Y]\\space \\cdot \\space\n[\\theta_{t}] \\space + \\space g(\\theta_{t} \\mid \\theta_{t+1}) \\]\n\n# for the compute of the ratio\n# poseterior\npost &lt;- function(x, mu, sigma){\n  ll(x, mu, sigma) + pmu(mu) + psigma(sigma)\n}\n# Probabilité de transition multivariée (loi normale)\ntransition_prob &lt;- function(theta_candidate, theta_current, proposal_sd = 0.1) {\n  dnorm(theta_candidate, mean = theta_current, sd = proposal_sd, log = TRUE)\n}\n# to compute a new candidate\njump &lt;- function(x, dist = .2){ # must be symmetric\n  x + rnorm(1, 0, dist)\n}\n\n\nAlgorithm implementation\n\niter = 10000\ntheta.post &lt;- data.frame(mu = rep(NA,iter), sigma = rep(NA,iter))\ntheta.post[1, 1] &lt;- 9\ntheta.post[1, 2] &lt;- 5\nfor (t in 2:iter){\n    # theta_star = proposed next values for parameters\n    theta_star &lt;- c(jump(theta.post[t-1, 1],0.1),jump(theta.post[t-1, 2],0.1))\n    #ratio\n    pstar &lt;- post(x, mu = theta_star[1], sigma = theta_star[2])\n    pprev &lt;- post(x, mu = theta.post[t-1, 1], sigma = theta.post[t-1, 2])\n    r &lt;- (pstar+transition_prob(c(theta.post[t-1,1],theta.post[t-1,2]), theta_star, 0.1) )-(pprev+transition_prob(theta_star, c(theta.post[t-1,1],theta.post[t-1,2]), 0.1))\n    ratio = exp(r[1])\n    # Acceptation or not\n    if(runif(1)&lt;ratio){\n      theta.post[t, ] &lt;- theta_star\n    } else {\n      theta.post[t, ] &lt;- theta.post[t-1, ]\n    }\n}\n\n\n\nVisualization\n\nxlims &lt;- c(4, 10)\nylims &lt;- c(1, 6)\n  par(mfrow=c(1, 2))\n  plot(theta.post[ 1:1000,1], theta.post[1:1000,2],\n       type=\"l\", xlim=xlims, ylim=ylims, col=\"blue\",\n       xlab=\"mu\", ylab=\"sigma\", main=\"Markov chains\")\n  text(x=7, y=1.2, paste(\"Iteration \", 1000), cex=1.5)\n  sm.density(x=cbind(c(theta.post[1:1000,1]), c(theta.post[1:1000,2])),\n             xlab=\"mu\", ylab=\"sigma\",\n             zlab=\"\", zlim=c(0, .7),\n             xlim=xlims, ylim=ylims, col=\"white\",\n             verbose=0)\n\nWarning in rgl.init(initValue, onlyNULL): RGL: unable to open X11 display\n\n\nWarning: 'rgl.init' failed, running with 'rgl.useNULL = TRUE'.\n\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nWarning in persp.default(xgrid, ygrid, dgrid, xlab = opt$xlab, ylab = opt$ylab,\n: surface extends beyond the box\n\n  title(\"Posterior density\")\n\n\n\n\nThe figure 1 represents the joint sampling of the two parameters. At each iteration, a new pair of parameters is proposed, and the ratio calculation assigns a weight to the proximity of the candidate parameter pair to the prior. Figure 2 represents the joint distribution of the two parameters formed by the sampling. It is from this joint distribution that the marginal posterior distributions of the two parameters arise. Therefore, we can also calculate the statistics (mean, standard deviation, etc.) of interest on these marginal posterior distributions."
  },
  {
    "objectID": "bayesian_modeles.html#using-jags",
    "href": "bayesian_modeles.html#using-jags",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Using JAGS",
    "text": "Using JAGS\n\ntwo parameter example\nFor this example, we are going to use data taken from a normal distribution. We will estimate the mean and the standard deviation like we did before. But for this example, we will use the software JAGS with the library rjgas. JAGS is a software that lets us build our model to estimate the posterior distribution of parameters that we want. We start by creating the data:\n\nset.seed(1000)\nY2 = rnorm(n = 200, mean = 10, sd = 1)\nhist(Y2)\n\n\n\n\nWe can suppose here that we want to estimate the mean length and length variation of a snakes species in one dessert. So we sample 200 snakes and measure them, Y2 contain all our samples.\nrjags need 3 main component. Fist, the model:\n\nmod1 = \"\n  model{\n  ## model\n  for(i in 1:n_obs){# loop when there is multiple data. (will compute the product of the likelihood)\n    Y2[i] ~ dnorm(m,preci) # likelihood of our data given parameters\n  }\n  ## Prior distribution\n  # All parameters that we want to estimat need a prior distibution\n  m ~ dunif(0, 10**3)\n  sd ~ dunif(0, 10**3)\n  preci &lt;- 1/(sd*sd) # In JAGS dnorm take the precision as input not the standard deviation\n  }\n\"\n\nIt looks like R syntax but it is JAGS syntax, which is slightly different for some function. In the model, we setup the prior for each parameter that we want to estimate with m ~ dunif(0, 10**3) and sd ~ dunif(0, 10**3).The uniform probability law is used because we suppose that we don’t have any prior knowledge on the mean and the standard deviation. We give to the model a range of possible value between 0 and 1000 for the mean and the standard deviation. In general for uniform prior we select large upper and lower bound to be sure that the value is included in it. The likelihood is then compute with Y2[i] ~ dnorm(m,preci)). JAGS is not like R because it takes the precision as parameter for the dnorm. Then we loop over all observation, RJAG will automatically make the product of the likelihood of all the data. Our model is complete, we have the priors and the likelihood formula setup to compute the posterior distribution. Second component needed for rjags, the list of data:\n\ndata_list1 = list(\n  Y2 = Y2,\n  n_obs= length(Y2)\n)\n\nThis list contains all information needed to make the model run, nothing really interesting to say here. Third, the list of initialization:\n\ninit_list1 &lt;- list(\n  m = 5,\n  sd = 2\n)\n\nJAGS will probably run MCMC algorithm to find the best parameters, those algorithm need values to start. Try to give values that make sense. If you don’t, the algorithm might take more time to converge or JAGS could also give error messages. Now that everything is setup, we can run the model.\n\nmjags1 &lt;- jags.model(file=textConnection(mod1),\n                     data=data_list1,\n                     inits=init_list1,\n                     n.chains = 3) # Number of MCMC\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 2\n   Total graph size: 210\n\nInitializing model\n\n\nWe choose to compute 3 Markov chains.\n\n# function to extract mcmc\npostSamples1 &lt;- coda.samples(mjags1,\n                             variable.names = c(\"m\",\"sd\"),\n                             n.iter = 10000,\n                             thin = 10)\npostSamples_df1 &lt;-  postSamples1 %&gt;% ggs()\n\nAlways verify if the markov chain are stationary. (otherwise, you will have a bad estimation of the posterior distribution)\n\npostSamples_df1%&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\", nrow = 2)+\n  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+\n  scale_color_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  main_theme+\n  labs(col = \"Chains\", x= \"Iterations\")\n\n\n\n\nAll MCMC seem to be stationary, we can go on.\n\n### Dataframe of estimated and real value of the mean and standard error\nref   &lt;- data.frame(val=c(mean(Y2),sd(Y2)), Parameter = c(\"m\", \"sd\"))\nesti   &lt;- data.frame(val=c(mean(postSamples_df1$value[postSamples_df1$Parameter ==\"m\"]),mean(postSamples_df1$value[postSamples_df1$Parameter ==\"sd\"])), Parameter = c(\"m\", \"sd\"))\n\n\npostSamples_df1 %&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\") +\n  geom_histogram(data = postSamples_df1, aes(x= value, y =..density..),\n                 bins=60, position = \"identity\", alpha = 0.4, col = 'purple4', fill = \"white\")+\n  scale_fill_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  geom_vline( aes(xintercept=val), esti, col = 'purple1',\n              linewidth=1.2)+\n  geom_vline( aes(xintercept=val), ref,\n              linetype = 2, linewidth=1.2)+\n  main_theme\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHere we look at the posterior distribution estimated by the 3 chain for the mean and the standard error. Purple full lines represent the means of posterior distributions( \\(\\hat{m} =\\) 10.1 , \\(\\hat{\\sigma} =\\) 1) and dark dotted lines are the real value of standard error ( \\(\\sigma =\\) {r} round(ref$val[2],1)) and mean( \\(m =\\) 10.1) of the data inY2. The estimations are really good, which is normal with 200 data.\n\n\nMultiple class exemple\nThe next example is a little bit more elaborated. It is almost the same because we are going to estimate means and standard error like the previous one, but this time we have multiple classes in our data. We can suppose that we want to study the length of snakes in five different desserts (that we are going to call A,B,C,D,E for simplicity).\n\nset.seed(1000)\ny_means = round(runif(n= 5, min = 5, max = 15))\nn_obs = 40\nY3 = c()\nclass = c()\nIDclass = c()\nID = c(\"A\",\"B\",\"C\",\"D\",\"E\")\nfor(i in 1:length(y_means)){\n Y3= c(Y3, rnorm(n = n_obs, mean = y_means[i], sd = 2))\n class = c(class,rep(ID[i],n_obs))\n IDclass = c(IDclass,rep(i,n_obs))\n}\ndata = data.frame(Y3, class, IDclass)\nggplot(data)+\n  geom_point(aes(class,Y3))+\n  geom_violin(aes(class,Y3), alpha =0.2)+\n    main_theme\n\n\n\n\n\nmod2 = \"\n  model{\n  ## model\n  for(i in 1:n_obs){\n    Y3[i] ~ dnorm(m[IDclass[i]],preci)\n  }\n  ## Prior distribution\n  for(n in 1:n_class){\n    m[n] ~ dunif(0, 10**3)\n  }\n  sd ~ dunif(0, 10**3)\n  preci &lt;- 1/(sd*sd)\n  }\n\"\n\nFor this time we want to estimate 5 different means and we will suppose that standard deviation is the same between each class (but we could also estimate it with respect to the class). To estimate multiple parameters we initiate as much prior as there is class with the loop for(n in 1:n_class){ m[n] ~ dunif(0, 10**3)}. m[IDclass[i]] is here to select the correct estimated mean that is the same class as the data.\n\ndata_list2 = list(\n  Y3 = data$Y3, # data vector\n  n_obs= length(data$Y3), # number of observations\n  IDclass = data$IDclass, # vector to specify class of each observation\n  n_class = max(data$IDclass) # Number of class\n)\n\nFor this example we add IDclass = data$IDclass that will be used to identify which data belongs to which class and n_class = max(data$IDclass) to specify the number of class.\n\ninit_list2 &lt;- list(\n  m = rep(5,5),\n  sd = 2\n)\n\nThis is where we initiate the prior. The only change is that we initialize 5 means instead of one with m = rep(5,5). So we have one prior of mean per class. Now we can run the model:\n\nmjags2 &lt;- jags.model(file=textConnection(mod2),\n                     data=data_list2,\n                     inits=init_list2,\n                     n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 6\n   Total graph size: 415\n\nInitializing model\n\npostSamples2 &lt;- coda.samples(mjags2,\n                             variable.names = c(\"m\",\"sd\"),\n                             n.iter = 10000,\n                             thin = 10)\n\nSome data manipulation for the visualization\n\npostSamples_df2 &lt;-  postSamples2 %&gt;% ggs()\nparams_names = unique(postSamples_df2$Parameter)\npostSamples_df2$true_values = NA\ntemp = c()\n\ndata3 = data.frame(Y3 =Y3, class = IDclass)\nref2 = postSamples_df2%&gt;%\n  group_by(Parameter)%&gt;%\n  summarise(val = mean(value))\n\nesti2 = data3%&gt;%\n  group_by(class)%&gt;%\n  summarise(val = mean(Y3))\n\nesti2 = rbind(esti2,data.frame(class=\"sd\",val= mean(data3%&gt;%\n  group_by(class)%&gt;%\n  summarise(val =sd(Y3))%&gt;%pull(val))))\n\nesti2$class = ref2$Parameter\nnames(esti2)[1] = c(\"Parameter\")\nesti2$Parameter = as.character(esti2$Parameter) \nref2$Parameter = as.character(ref2$Parameter) \n\nCheck if the chains are stationary.\n\npostSamples_df2%&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\")+\n  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+\n  scale_color_manual(values = wesanderson::wes_palette(\"FantasticFox1\", n = 5))+\n  main_theme\n\n\n\n\nFinally, we look at the results of posterior distribution of the estimated means and the estimated standard deviation.\n\npostSamples_df2 %&gt;%\n  ggplot() +\n  facet_wrap(~Parameter, scales = \"free\") +\n  geom_histogram(aes(x= value, y =after_stat(density)), col = 'purple4',fill = \"white\", bins = 60)+\n  geom_vline( aes(xintercept=val), esti2, col = 'purple1',\n              linewidth=1.2)+\n  geom_vline( aes(xintercept=val), ref2,\n              linetype = 2, linewidth=1.2)+\n  main_theme\n\n\n\n\nPurple full lines represent the means of posterior distributions and doted line are the true values of the data. The utility to have posterior distribution and not only compute the mean or the standard error is that we can know the probability of each mean or sd values, therefore we have a good estimation of the confidence that we should have when making conclusion on values."
  },
  {
    "objectID": "bayesian_modeles.html#predator-prey-body-size",
    "href": "bayesian_modeles.html#predator-prey-body-size",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Predator-prey body size:",
    "text": "Predator-prey body size:\nFor example, predators are often gape-limited, meaning that larger predators should be able to eat larger prey and smaller predators, smaller preys, depending on the size of their mouth. It is known from the existing literature that predators are 2 or 3 times larger than their prey (Trebilco et al. 2013). So, in this example we could use a prior mean of the intercept to a value below zero, like an average predator/prey mass comparison (Wesner and Pomeranz 2021)."
  },
  {
    "objectID": "bayesian_modeles.html#spider-abundance-according-to-the-presence-of-fish",
    "href": "bayesian_modeles.html#spider-abundance-according-to-the-presence-of-fish",
    "title": "Introduction to bayesian modeles for ecology",
    "section": "Spider abundance according to the presence of fish:",
    "text": "Spider abundance according to the presence of fish:\nIn this example from a study done by Warmbold and Wesner in 2018, they hypothesized that fish would reduce the emergence of adult aquatic insects by eating their larvae, causing a reduction in terrestrial spiders that feed on the adult forms of those insects. The authors used a Bayesian inference approach to estimate the posterior probabilities of each parameter in their models. They preferred Bayesian inference to frequentist inference, because their interest lay in estimating the probabilities of the hypotheses or parameters θ, given the data y, i.e. p(θ|y), rather than the probability of the data given a null hypothesis.The authors based their priors for intercepts on prior knowledge from a pilot experiment. The priors for the intercepts and slopes were assigned as normal distributions, the standard deviations as half-cauchy distributions, and the priors for the scale in the gamma distribution were assigned as exponential distributions.\n\nAnt species diversity\nAs said before, bayesian inference not only uses the sample data but also any available prior information. In this example from Gotelli and Ellison, 2002, we want to see how many species of ants we can find in sampling grids according to different environment variables. They did simple additive models of richness and models that included all possible interactions. Here, they used the Bayes’ theorem to calculate the posterior probability of the model conditional on the data with explicit explication on the prior from the literature. In this example, they precisely used WinBUGS which implements the MCMC methods with presented before using a Gibbs sampler.\n\n\nModeling the life cycle of a fish\nThe paper shows an integrated life cycle model using Hierarchical Bayesian Models (HBMs) for marine fish species, highlighting larval drift processes, the contribution of several nurseries to recruitment, as well as natural and marine mortality. the Peach. These MBHs, associated with Markov chain Monte Carlo methods, make it possible to incorporate complex demographic models into statistical frameworks, while processing varied data. This approach aims to provide inferences while robustly assessing the uncertainty surrounding parameter estimates and predictions. The widespread use of MBHs in fish population dynamics demonstrates their potential for increasing biological realism."
  },
  {
    "objectID": "linear_modelling.html",
    "href": "linear_modelling.html",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "The purpose of statistics is to test a hypothesis. You would need to follow these steps:\n\n\nThey define different proposed relationships between dependent and independent variable(s).\nNull hypothesis H0 = no relationship between dependent and independent variables.\nAlternative hypothesis H1 = some expected relationship between the variables.\n\n\n\nThe observed value Tobs of the test statistic T is calculated from data given that the T variable has a probability distribution known under H0.\n\n\n\nTo get this value, you need to calculate the probability of observing Tobs value in this T distribution knowing the distribution of the test statistic T under H0.\n\n\n\nIf the P-value is less than (or equal to) \\(\\alpha\\), then the null hypothesis is rejected in favor of the alternative hypothesis and if the P-value is greater than \\(\\alpha\\) then the null hypothesis is not rejected. WARNING : never say that the null hypothesis is validated. You just know that you cannot reject it with the information you have. There could be relationships that have not been detected.\n\n\n\n\nLinear modelling is widely used in statistics to model observed data, by considering their random nature. It explains one dependent variable, noted Y (random variable, also called the response variable) in function to independent variables (also called predictors or explanatory variables), also observed/measured on statistical units of the sample. In this chapter, you will see different models corresponding to different statistical tests. It happens that several tests can be conducted for the same dataset and hypothesis ; in this case, you will select the most powerful test. It is the one with the lowest \\(\\beta\\)-error (given at the end)."
  },
  {
    "objectID": "linear_modelling.html#the-statistical-test",
    "href": "linear_modelling.html#the-statistical-test",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "The purpose of statistics is to test a hypothesis. You would need to follow these steps:\n\n\nThey define different proposed relationships between dependent and independent variable(s).\nNull hypothesis H0 = no relationship between dependent and independent variables.\nAlternative hypothesis H1 = some expected relationship between the variables.\n\n\n\nThe observed value Tobs of the test statistic T is calculated from data given that the T variable has a probability distribution known under H0.\n\n\n\nTo get this value, you need to calculate the probability of observing Tobs value in this T distribution knowing the distribution of the test statistic T under H0.\n\n\n\nIf the P-value is less than (or equal to) \\(\\alpha\\), then the null hypothesis is rejected in favor of the alternative hypothesis and if the P-value is greater than \\(\\alpha\\) then the null hypothesis is not rejected. WARNING : never say that the null hypothesis is validated. You just know that you cannot reject it with the information you have. There could be relationships that have not been detected."
  },
  {
    "objectID": "linear_modelling.html#statistical-modelling",
    "href": "linear_modelling.html#statistical-modelling",
    "title": "Linear modelling in ecology",
    "section": "",
    "text": "Linear modelling is widely used in statistics to model observed data, by considering their random nature. It explains one dependent variable, noted Y (random variable, also called the response variable) in function to independent variables (also called predictors or explanatory variables), also observed/measured on statistical units of the sample. In this chapter, you will see different models corresponding to different statistical tests. It happens that several tests can be conducted for the same dataset and hypothesis ; in this case, you will select the most powerful test. It is the one with the lowest \\(\\beta\\)-error (given at the end)."
  },
  {
    "objectID": "linear_modelling.html#anova",
    "href": "linear_modelling.html#anova",
    "title": "Linear modelling in ecology",
    "section": "1) ANOVA",
    "text": "1) ANOVA\n\nINTRODUCTION\nANOVA (Analysis of variance) is one of the most widespread techniques in data analysis. We use it to test the effect of one or more independent quantitative variables (Xs) on a dependent qualitative variable (Y). The categorical qualitative variables are named ‘factors’, et each factor has different levels that are chosen and fixed.\nWe consider 2 types of ANOVA: in the presence of a single variable X in the analysis, we follow a simple factor ANOVA; in the presence of several variables X, we follow a multiple factor ANOVA.\n\n\nSimple factor ANOVA\nThe model takes the following form: \\[ Y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith level of the single factor and \\(\\epsilon\\) is the error term (i.e. residuals).\n\n\nMultiple factor ANOVA\nThe ANOVA model depends on the experimental design: factorial or nested.\nFull factorial design This design studies the influence of multiple factors and of their interactions on the variable of interest. We frequently want to test for differences in the response variable due to the multiple factors, called ‘main effects’. What we do is test the effects of each main effect separately, then whether or not these effects interact with each other (‘factor interactions’). Considering a factorial design with two factors, the model takes the form: \\[ Y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + \\gamma_{ij} +\\epsilon_{ijk}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith group of the first factor, and \\(\\beta_{j}\\) is the effect of the jth group of the second factor, \\(\\gamma_{ij}\\) is the interaction between both factors and \\(\\epsilon\\) is the error term (i.e. residuals).\nNested ANOVA In this design, the levels of a factor are hierarchically nested within the levels of another factor. Considering a nested design with two factors in which B factor is nested in A factor, the model takes the form: \\[ Y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j/i} +\\epsilon_{ijk}  \\] where \\(\\mu\\) is the overall mean, \\(\\alpha_{i}\\) is the effect of the ith group of the first factor, and \\(\\beta_{j/i}\\) is the effect of the jth group of the second factor nested in the ith group of the first factor and \\(\\epsilon\\) is the error term (i.e. residuals).\n\n\nANOVA EXAMPLE\nLet’s consider an experimental data originated to an ANOVA example developed by James Lavender & Alistair Poore - 2016 (https://environmentalcomputing.net/statistics/linear-models/anova/anova-factorial/).\n\n\nDataset presentation and objectives of the analysis\nIn this data analysis, we will focus on experimental data with two factors that are both applied to all statistical individuals. An ecologist wants to test the effects of metal contamination on the number of species found in sessile marine invertebrates (i.e. sponges). This ecologist would precisely like to know whether copper enrichment reduces species richness, but also know that the richness of invertebrates can depend on whether the substrate is vertical or horizontal. In order to do this, they made an experiment where species richness was recorded in replicate samples in each of the six combinations of copper enrichment (\\(“None”\\),\\(“Low”\\),\\(“High”\\)) and orientation (\\(“Vertical”\\),\\(“Horizontal”\\)). The experimental design is factorial because all levels of one treatment are represented in all levels of the other treatment (i.e. crossed factors).\nIn consequence, the factorial ANOVA will test whether there are:\n\nany differences in species richness among the three levels of copper enrichment\nany differences in species richness among the two levels of substrate orientation\nany interactions between copper and orientation (i.e. the effect of the copper enrichment depends on the substrate orientation and reciprocally).\n\nWe have three null hypotheses:\n\nthere is no difference between the means for each level of copper enrichment, H0: \\(\\mu_{None}\\)=\\(\\mu_{Low}\\)=\\(\\mu_{High}\\)\nthere is no difference between the means for each level of orientation, H0: \\(\\mu_{Vertical}\\)=\\(\\mu_{Horizontal}\\)\nthere is no interaction between both factors (i.e. if factor effects exist, the factors do not interact)\n\nLet’s perform a two-factor ANOVA, something far better than running two separate single factor ANOVAs that contrast copper effects for each level of the substrate orientation, for 3 reasons:\n\n\nwe have more statistical power (higher degrees of freedom)\n\n\nwe can test whether the main effects interact or not\n\n\nwe reduce the risk of statistical error (i.e. we can’t forget that each time we perform a separate statistical analysis, we get \\(\\alpha\\) and \\(\\beta\\) risks)\n\n\n\n# Dataset import\ndatasessile &lt;- read.table(\"sessile.txt\", dec=\".\", header = TRUE)\ndatasessile$Copper&lt;-as.factor(datasessile$Copper)\ndatasessile$Orientation&lt;-as.factor(datasessile$Orientation)\nstr(datasessile)\n\n'data.frame':   60 obs. of  3 variables:\n $ Copper     : Factor w/ 3 levels \"High\",\"Low\",\"None\": 3 3 3 3 3 3 3 3 3 3 ...\n $ Orientation: Factor w/ 2 levels \"Horizontal\",\"vertical\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Richness   : int  68 64 64 63 69 63 70 68 68 62 ...\n\n# Check for presence of missing values\ncolSums(is.na(datasessile))\n\n     Copper Orientation    Richness \n          0           0           0 \n\n# There is no missing value.\n\n\n\nData exploration\nBefore any statistical analysis, we MUST explore the data in order to prevent any error. Here is the list of explorations to perform before modelling:\n\nCheck presence of outliers in \\(Y\\) and distribution of \\(Y\\) values\nIf \\(X\\) is a quantitative independent variable, check presence of outliers in X and distribution of X values\n\n2b. If \\(X\\) is a qualitative independent variable, analyse the number of levels and the number of individuals per level\n\nAnalyse the potential relationships between \\(Y\\) and the \\(X_{s}\\)\nCheck presence of interactions between \\(X_{s}\\)\nCheck presence of collinearity between \\(X_{s}\\)\n\n\n\nStatistical analysis\nModel building\nFor the statistical modelling, we first analyse the full model (model containing all independent variables and interactions to test).\n\n# Model formulation\nmod1&lt;-lm(Richness~Copper+Orientation+Copper:Orientation,data=datasessile)\n# Comment : a simplest way to write this\nmod1&lt;-lm(Richness~Copper*Orientation,data=datasessile)\n# Then we check for significance\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nRichness ~ Copper * Orientation\n                   Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                           467.0 135.12                      \nCopper:Orientation  2     570.7 1037.7 179.03  32.995 4.341e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe here have a significant interaction between COPPER and ORIENTATION, shown by the test statistic, F value and its associated p-value (Pr(&gt;F)). This means that the effect of one factor (COPPER) depends upon the other (ORIENTATION). In this example, it would mean that the effect of copper enrichment is not consistent between the vertical and horizontal habitats. This complexifies the interpretation of the main effects as a consequence. As the interaction is significant, the full model is the candidate model (i.e. the model containing only significant terms). To understand how factors and their interaction influence the species richness, we must analyse the coefficients of the model.\nModel’s coefficients analysis\n\n# Candidate model formulation\nmod1&lt;-lm(Richness~Copper*Orientation,data=datasessile)\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Richness ~ Copper * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      55.400      0.930  59.573  &lt; 2e-16 ***\nCopperLow                         0.400      1.315   0.304    0.762    \nCopperNone                       14.200      1.315  10.797 4.22e-15 ***\nOrientationvertical             -11.700      1.315  -8.896 3.63e-12 ***\nCopperLow:Orientationvertical    15.100      1.860   8.119 6.35e-11 ***\nCopperNone:Orientationvertical    8.000      1.860   4.301 7.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nThis table detailed the coefficients of the model with coefficients associated with each level of the significant fixed factor. For each factor, one level is called ‘the baseline’, meaning that its coefficient is 0 (also called the reference level).\nFrom this table, coefficients are : COPPER FACTOR\n\n\\(Copper_{High}\\) = 0 (the baseline of the factor COPPER)\n\\(Copper_{Low}\\) = \\(0.4^{NS}\\)\n\\(Copper_{None}\\) = \\(14.2^{***}\\)\n\nORIENTATION FACTOR\n\n\\(Orientation_{Horizontal}\\) = 0 (the baseline of the factor ORIENTATION)\n\\(Orientation_{Vertical}\\)= \\(-11.7^{***}\\)\n\nORIENTATION:COPPER INTERACTION\n\n\\(Copper_{Low}\\) : \\(Orientation_{Vertical}\\) = \\(15.1^{***}\\)\n\\(Copper_{None}\\) : \\(Orientation_{Vertical}\\) = \\(8^{***}\\)\n\nSo, the candidate model is: \\[ Species\\:Richness = 55.4  \\] \\[+ [\\:Copper_{High}=0;\\:Copper_{Low}=0.4^{NS}\\:,\\:Copper_{None}=14.2^{***} ]  \\] \\[ +[Orientation_{Horizontal}=0.0; \\:Orientation_{Vertical}=-11.7^{***}]  \\] \\[ +[Copper_{Low} : Orientation_{Vertical} = 15.1^{***};\\:Copper_{None} : Orientation_{Vertical} = 8^{***}]\\]\nA quick way to help understand an interaction, if we get one, is to examine the interaction plot.\n\n# Interactions graphic\nboxplot(datasessile$Richness~datasessile$Copper*datasessile$Orientation, varwidth = TRUE, ylab = \"Species Richness\", col='blue', main = \"\",cex.axis=0.7)\n\n\n\n\nMultiple comparisons\nIf we’re able to detect any significant differences in the ANOVA, we are then interested in knowing exactly which levels of a given factor differ from one another, and which do not. Remember that a significant p value in the F-test we just ran would reject the null hypothesis where the means were the same across all factor levels, but not identify which were different from each other. Here, we have two factors with their own coefficients:\nORIENTATION FACTOR\n\n\\(Orientation_{Horizontal}\\) = 0 (the baseline of the factor ORIENTATION)\n\\(Orientation_{Vertical}\\)= \\(-11.7^{***}\\)\n\nThose coefficients suggest that the species richness is lowest in vertical habitats.\nCOPPER FACTOR\n\n\\(Copper_{High}\\) = 0 (the baseline of the factor COPPER)\n\\(Copper_{Low}\\) = \\(0.4^{NS}\\)\n\n\\(Copper_{None}\\) = \\(14.2^{***}\\)\n\nThose coefficients suggest that the species richness is highest in absence of Copper enrichment (level \\(None\\) &gt; \\(High\\)). But as the level \\(High\\) is the baseline, we can’t detect whether the levels \\(None\\) and \\(Low\\) are different or not. So, we must change the level baseline and re-analyse the coefficients of the model to detect difference or not between those two factor levels.\n\n# Change the COPPER factor baseline: put 'Low' level as the baseline\ndatasessile$Copper2&lt;-relevel(datasessile$Copper,ref=\"Low\")\n# New model formulation\nmod2&lt;-lm(Richness~Copper2*Orientation,data=datasessile)\n# Coefficients of the model\nsummary(mod2)\n\n\nCall:\nlm(formula = Richness ~ Copper2 * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       55.800      0.930  60.003  &lt; 2e-16 ***\nCopper2High                       -0.400      1.315  -0.304  0.76218    \nCopper2None                       13.800      1.315  10.493 1.21e-14 ***\nOrientationvertical                3.400      1.315   2.585  0.01246 *  \nCopper2High:Orientationvertical  -15.100      1.860  -8.119 6.35e-11 ***\nCopper2None:Orientationvertical   -7.100      1.860  -3.817  0.00035 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nNow, the coefficients of the COPPER factor are: COPPER FACTOR\n\n\\(Copper_{Low}\\) = 0 (the new baseline of the factor COPPER)\n\\(Copper_{High}\\) = \\(-0.4^{NS}\\)\n\\(Copper_{None}\\) = \\(13.8^{***}\\)\n\nThose coefficients suggest that the species richness is highest in absence of Copper enrichment (\\(None\\) &gt; \\(Low\\)). In conclusion, \\(Richness_{Copper_{High}}\\) = \\(Richness_{Copper_{Low}}\\)&lt; \\(Richness_{Copper_{None}}\\)\nModel explanation: R²\nLet’s determine the part of the \\(Y\\) variation explained by the model.\n\n# R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Richness ~ Copper * Orientation, data = datasessile)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.700 -1.825  0.350  1.400 11.400 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      55.400      0.930  59.573  &lt; 2e-16 ***\nCopperLow                         0.400      1.315   0.304    0.762    \nCopperNone                       14.200      1.315  10.797 4.22e-15 ***\nOrientationvertical             -11.700      1.315  -8.896 3.63e-12 ***\nCopperLow:Orientationvertical    15.100      1.860   8.119 6.35e-11 ***\nCopperNone:Orientationvertical    8.000      1.860   4.301 7.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 54 degrees of freedom\nMultiple R-squared:  0.8986,    Adjusted R-squared:  0.8893 \nF-statistic: 95.76 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nIn this output, the adjusted R² is equal to 0.8893, which means that about 89% of the variance of species richness is explained by the model.\n\n\nModel validation: Check the assumptions\nSee part III/."
  },
  {
    "objectID": "linear_modelling.html#ancova",
    "href": "linear_modelling.html#ancova",
    "title": "Linear modelling in ecology",
    "section": "2) ANCOVA",
    "text": "2) ANCOVA\n\nINTRODUCTION\nANCOVA (i.e. Analysis of Covariance) is a statistical technique widely used in various disciplines like the previous analysis presented, ANOVA and the linear regression . It is used to model the relationship between a quantitative dependent variable Y, called the response variable, and several independant variable X1, X2… called explanatory variables or predictors). The difference between ANOVA and ANCOVA, is that the predictors can be quantitative and qualitative. For example, we could use an ANCOVA to test whether the size (continuous), sex (categorical), reproductive status (categorical), habitat (categorical), age (continuous) of a sperm whale are good predictors of its weight (the response variable). As the ANCOVA includes quantitative and categorical independent variables, this technique represents between linear regression and ANOVA.\nWe can give an example and write the model with one continuous and one categorical independent variables, the model takes the form: \\[ Y_{ij} = \\mu + \\alpha_{i}+ \\beta.X_{ij}+\\gamma_{i}.X_{ij}+\\epsilon_{ij}\\]\n\n\\(\\mu\\) is the overall mean, the intercept\n\\(\\alpha_{i}\\) is the effect of the modality i of the categorical variable)\n\\(\\beta\\) is the slope (amount of change in Y for each unit of the quantitative covariate), it is the effect of the quantitative variable\\(X_{j}\\)\n\\(\\gamma_{i}\\) is the interactive coefficient between the modality i of the factor and the quantitative covariate\n\\(\\epsilon_{ij}\\) is the error term (residuals). The inclusion of the error term \\(\\epsilon_{ij}\\), also called the stochastic part of the model, that makes the model statistical rather than mathematical. The error term is drawn from a statistical distribution that integers the random variability in the response. In standard linear model, this is assumed to be a normal (Gaussian) distribution with parameter 0 et \\(\\sigma^2\\), avec \\(\\sigma\\) l’écart type.\n\n\n\nANCOVA EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nFor this example, we will use data from a study performed in 1994 that concerns the fly, Anatalanta aptera, a wingless fly living in subantarctic islands, particularly in sea bird colonies. 320 individuals has been collected in the Crozet Island, living either on the coast or inland (i.e. mountain landscape). The goal of that study is to determine which variables may explain the dry weight of Anatalanta aptera. Dry weight represents total organic and inorganic matter in the tissue and is more accurate than measuring wet weight (using dry weight as a measure of animal growth tends to be more reliable). Then, the following life-history traits have been measured to try and explain the response variable Y= Dry weight. Those are the predictors.\n\nSex = the sex of a given individual, categorical variable\n\nHabitat = the habitat of a given individual(‘Coast’ or ‘Inland’), categorical variable\n\nLength = the length of a given individual, continuous variable\n\nWidth = the width of a given individual, continuous variable\n\nWaterContent = the water content of a given individual, continuous variable\n\nFatContent = the fat content of a given individual, continuous variable\n\nDryWeight = the dry weight of a given individual, continuous variable\n\nThe underlying question for this research is simple; do variables drive the dry weight in Anatalanta aptera? Predictors being continuous and categorical, we are gonna perform an ANCOVA.\n\n# Dataset import\ndataFly &lt;- read.table(\"FlyWeight.txt\", dec=\".\", header = TRUE)\n\nWe must check whether the variables are correctly imported, i.e. they are of the right type. Sex and Habitat are ‘chr’, so we must transform them into factors.\n\ndataFly$Sex&lt;-as.factor(dataFly$Sex)\ndataFly$Habitat&lt;-as.factor(dataFly$Habitat)\nstr(dataFly)\n\n'data.frame':   320 obs. of  8 variables:\n $ FlyNumber   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Habitat     : Factor w/ 2 levels \"Coast\",\"Inland\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Length      : num  1.4 1.55 1.52 1.44 1.77 1.69 1.79 1.65 1.58 1.62 ...\n $ Width       : num  1.24 1.39 1.26 1.18 1.46 1.41 1.46 1.41 1.32 1.4 ...\n $ WaterContent: num  5.9 7.1 6.8 5.3 8.2 7.6 8.2 7.2 6.5 8.8 ...\n $ FatContent  : num  0.3 0.2 0 0.2 0.1 0 0 0.3 0.1 0.2 ...\n $ DryWeight   : num  2.2 3 2.2 2 3.1 2.6 2.8 3.1 2.2 2.6 ...\n\n\n\n\nData exploration\nSee part 1).\n\n\nStatistical analysis\nModel building\nFor the statistical modelling, we first analyse the full model (model containing all independent variables to test). We will test the effects of the main effects (3 continuous and 2 categorical independent variables) and their interactions (by excluding interactions between quantitative independent variables). To get the candidate model we will perform a backward selection. It consist in testing the full model first, then we drop the least significant interaction, we perform this step until all the remaining interaction effect are significant. We do the qsame approach for the main effect. By following these two steps, candidate model is found. We use the function drop (F test) to test the significant at each step and choose the lest significant effect. Keep in mind that if a variable is involved in an interaction effect you must keep it as a main effect in the model.\nBackward selection\n\n# Model formulation\n#Full model\nmod1&lt;-lm(DryWeight~ Sex + Habitat + Width + WaterContent + FatContent + Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + Habitat:WaterContent + Sex:FatContent + Habitat:FatContent \n        ,data=dataFly)\n# Then we check for significance\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + \n    Habitat:WaterContent + Sex:FatContent + Habitat:FatContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            83.710 -403.11                      \nSex:Habitat           1    0.1683 83.878 -404.47  0.6172 0.4326813    \nSex:Width             1    0.0076 83.717 -405.08  0.0277 0.8678741    \nHabitat:Width         1    3.0401 86.750 -393.69 11.1492 0.0009443 ***\nSex:WaterContent      1    0.0105 83.720 -405.07  0.0385 0.8445863    \nHabitat:WaterContent  1    3.3319 87.042 -392.62 12.2195 0.0005426 ***\nSex:FatContent        1    0.0033 83.713 -405.10  0.0122 0.9122382    \nHabitat:FatContent    1    0.9549 84.665 -401.48  3.5022 0.0622392 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this significance output, we will exclude the interaction that is ‘the less significant’ = ‘Sex:Fat Content’\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Sex:Habitat + Sex:Width + Habitat:Width + Sex:WaterContent + \n    Habitat:WaterContent + Habitat:FatContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            83.713 -405.10                      \nSex:Habitat           1    0.1673 83.880 -406.46  0.6156 0.4332877    \nSex:Width             1    0.0061 83.719 -407.07  0.0226 0.8805490    \nHabitat:Width         1    3.0926 86.806 -395.49 11.3783 0.0008380 ***\nSex:WaterContent      1    0.0095 83.722 -407.06  0.0349 0.8518861    \nHabitat:WaterContent  1    3.4666 87.180 -394.11 12.7546 0.0004119 ***\nHabitat:FatContent    1    0.9516 84.665 -403.48  3.5013 0.0622693 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this new significance list, we will exclude the interaction that is ‘the less significant’… and so on.\nAfter the non significant interaction deletions, the model contains only 2 significant interactions = ‘Habitat:Width’ and ‘Habitat:WaterContent’. That means that the main effects ‘Habitat’, ‘Width’ and ‘WaterContent’ are maintained in the model as included in significant interactions.\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Sex + Habitat + Width + WaterContent + FatContent + \n    Habitat:Width + Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value   Pr(&gt;F)    \n&lt;none&gt;                            85.017 -408.15                     \nSex                   1    0.5309 85.547 -408.16  1.9482 0.163775    \nFatContent            1    0.7347 85.751 -407.40  2.6961 0.101602    \nHabitat:Width         1    2.5775 87.594 -400.59  9.4592 0.002287 ** \nHabitat:WaterContent  1    3.5343 88.551 -397.12 12.9705 0.000368 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the previous listing, we decide to first exclude the ‘Sex’ factor.\n\n\nSingle term deletions\n\nModel:\nDryWeight ~ Habitat + Width + WaterContent + FatContent + Habitat:Width + \n    Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            85.547 -408.16                      \nFatContent            1    0.6398 86.187 -407.78  2.3409 0.1270272    \nHabitat:Width         1    2.6522 88.200 -400.39  9.7040 0.0020089 ** \nHabitat:WaterContent  1    4.0859 89.633 -395.23 14.9495 0.0001343 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs the ‘FatContent’ is still non significant, we exclude this main effect and we obtain the ‘Candidate Model’. This candidate model contains three main effects (we are keeping the main effect because the variables have interaction effects) and two interactions.\nCandidate model:\n\n# Model formulation\nmod1&lt;-lm(DryWeight~ Habitat+ Width + WaterContent + Habitat:Width + Habitat:WaterContent,data=dataFly)\n\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nDryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent\n                     Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            86.187 -407.78                      \nHabitat:Width         1    2.7963 88.983 -399.56  10.188 0.0015570 ** \nHabitat:WaterContent  1    4.2441 90.431 -394.39  15.462 0.0001036 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo understand how main effects and interactions influence the dry weight of the flies, we will analyse the coefficients of the model.\nModel’s coefficients analysis\n\nsummary(mod1)\n\n\nCall:\nlm(formula = DryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent, data = dataFly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.86592 -0.25639 -0.01119  0.23329  1.91099 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.05228    0.66459   0.079 0.937354    \nHabitatInland              -2.43270    0.78331  -3.106 0.002072 ** \nWidth                       0.21761    0.64964   0.335 0.737865    \nWaterContent                0.31332    0.04925   6.362 7.02e-10 ***\nHabitatInland:Width         2.34365    0.73427   3.192 0.001557 ** \nHabitatInland:WaterContent -0.21378    0.05437  -3.932 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 314 degrees of freedom\nMultiple R-squared:  0.6129,    Adjusted R-squared:  0.6067 \nF-statistic: 99.42 on 5 and 314 DF,  p-value: &lt; 2.2e-16\n\n\nThis output presents a table detailing the coefficients of the model with coefficients associated with each significant main effect and interaction. Remind that for a factor, one level is called ‘the baseline’ meaning that its coefficient is 0 (also called the reference level). From this table, coefficients are:\nHABITAT FACTOR\n\n\\(Habitat_{Coast}\\) = 0 (the baseline of the factor Habitat)\n\\(Habitat_{Inland}\\) = \\(-2.43^{**}\\)\n\nWIDTH COVARIATE\n\n\\(\\beta_{Width}\\) = \\(0.21^{NS}\\)\n\nWATER CONTENT COVARIATE\n\n\\(\\beta_{WaterContent}\\) = \\(0.31^{***}\\)\n\nHABITAT:WIDTH INTERACTION\n\n\\(\\beta_{Width_{Inland}}\\) = \\(2.34^{***}\\)\n\nHABITAT:WATER CONTENT INTERACTION\n\n\\(\\beta_{WaterContent_{Inland}}\\) = \\(- 0.21^{***}\\)\n\nModel explanation: R²\nYou can determine the part of the \\(Y\\) variation explained by your model. See the output of the model summary.\n\n#to get R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = DryWeight ~ Habitat + Width + WaterContent + Habitat:Width + \n    Habitat:WaterContent, data = dataFly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.86592 -0.25639 -0.01119  0.23329  1.91099 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.05228    0.66459   0.079 0.937354    \nHabitatInland              -2.43270    0.78331  -3.106 0.002072 ** \nWidth                       0.21761    0.64964   0.335 0.737865    \nWaterContent                0.31332    0.04925   6.362 7.02e-10 ***\nHabitatInland:Width         2.34365    0.73427   3.192 0.001557 ** \nHabitatInland:WaterContent -0.21378    0.05437  -3.932 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 314 degrees of freedom\nMultiple R-squared:  0.6129,    Adjusted R-squared:  0.6067 \nF-statistic: 99.42 on 5 and 314 DF,  p-value: &lt; 2.2e-16\n\n\nIn this output, you get that the adjusted R² = 0.6067. That means that about 61% of the variance of the dry weight of the flies is explained by its relationship with their habitat, water content and width and the interactions between these predictors.\nModel expression\nYou can write the model with the coefficient. Be careful: you have a different expression for each modality. \\[ Dry\\:Weight = 0.05\\:+\\:(Habitat_{Coast} = 0 \\:;\\:Habitat_{Inland} = -2.43^{**}) \\] \\[ +\\:0.21^{NS}. Width\\:+\\:0.31^{***}. Water\\:Content \\] \\[ + (if\\:Habitat=Inland:\\:+ 2.34^{***}. Width\\: -\\: 0.21^{***}.Water\\:Content)\\]\nBiological interpretation\nBefore drawing any conclusions from your result, you have to validate the model (part model validation). The assumptions of ANCOVA are the same as for all general linear models (i.e. regressions, ANOVAs), you have to check independence of residuals, normality of residuals and homogeneity of variances."
  },
  {
    "objectID": "linear_modelling.html#regression",
    "href": "linear_modelling.html#regression",
    "title": "Linear modelling in ecology",
    "section": "3) REGRESSION",
    "text": "3) REGRESSION\n\nINTRODUCTION\nYou have certainly used the technique of regression many times in various exercises. It is used to model the relationship between a quantitative dependent variable \\(Y\\) (the response, that is continuous, and one or several explanatory quantitative variable(s) (the predictors, that are independant). \\(X_{1}\\),\\(X_{2}\\)…\\(X_{p}\\). For example, we could use a linear regression to test whether the weight of a dog (i.e. the explanatory variable) is a good predictor of its lifespan (the response variable).\n\n\nMODEL WRITING\nThe general model is written: \\[ Y_{i} = \\alpha + \\beta_{j}.X{ij}+ \\epsilon_{i}\\] \\[ i=1,...,n\\] \\[ j=1,...,p\\] The inclusion of the error term \\(\\epsilon\\), also called the stochastic part of the model, makes the model statistical rather than mathematical. The error term is drawn from a statistical distribution that integers the random variability in the response. In standard linear regression, this is assumed to be a normal (Gaussian) distribution.\nThere are two different regression types:\n• the simple linear regression includes a single \\(X\\) in your analysis. The model takes the form: \\[ Y = \\alpha + \\beta.X + \\epsilon \\] where \\(\\alpha\\) is the intercept (value of \\(Y\\) when \\(X\\) = 0), \\(\\beta\\) is the regression slope (amount of change in \\(Y\\) for each unit of \\(X\\)), and \\(\\epsilon\\) is the error term (i.e. residuals).\n• the multiple linear regression includes several \\(X_{s}\\) in your analysis. The model takes the form: \\[ Y_{i} = \\alpha + \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}+ \\epsilon_{i}\\]\n\n\nREGRESSION EXAMPLE:\n\n\nDataset presentation and objectives of the analysis\nThis data was published in ‘Latitudinal variation in light levels drives human visual system size’. Eiluned Pearce and Robin Dunbar. Biol. Lett. published online 27 July 2011 (http://doi:10.1098/rsbl.2011.0570). Ambient light levels influence visual system size in birds and primates and Pearce and Dunbar (2011) argue that the same is true for humans. Using linear regression techniques, they want to test the relationship between (absolute) latitude and human orbital volume, an index of eyeball size. Pearce and Dunbar (2011) measured cranial capacity (CC), orbital volume and foramen magnum (FM) dimensions for 73 healthy adult crania from the Oxford University Museum of Natural History and Duckworth Collection, University of Cambridge.\nList of the variables: - MeanOrbitalVolume = Index for eyeball size, continuous variable (the response)\n\nCranialCapacity = measure of the volume of the interior of the cranium, continuous variable\nMinimum_Illuminance = log scale of the minimum of illuminance of the sample site, continuous variable\nMinimum_Temperature = Minimum temperature of the sample site, continuous variable\nAbsoluteLatitude = Absolute values of latitude of the sample, continuous variable\nPopulation = country/region of the sample, Categorical variable\n\nThe response variable is mean orbital volume and the rest are assumed covariates. The Population variable is a descriptor of the samples, so not included in the modelling. Each value for orbital volume represents the mean of 3 replicate measurements from the same skull.\nQuestion: which covariates drive the mean orbital volume?\n\n# Dataset import\ndataHVS &lt;- read.table(\"HumanVisualSystem.txt\", dec=\".\", header = TRUE)\ndataHVS$Population&lt;-as.factor(dataHVS$Population)\nstr(dataHVS)\n\n'data.frame':   55 obs. of  6 variables:\n $ Population         : Factor w/ 12 levels \"Australia\",\"CanaryIslands\",..: 8 8 8 2 2 2 2 11 11 11 ...\n $ AbsoluteLatitude   : num  1.33 5.42 5.42 28.51 28.51 ...\n $ CranialCapacity    : int  1200 1300 1100 1300 1420 1400 1620 1340 1480 1280 ...\n $ Minimum_Illuminance: num  109648 104713 104713 64565 64565 ...\n $ Minimum_Temperature: num  26.7 24.4 24.4 14.4 14.4 ...\n $ MeanOrbitalVolume  : num  22.5 21 22 20.5 25 25.5 27.6 24.5 24 25.3 ...\n\n# Check for presence of missing values\ncolSums(is.na(dataHVS))\n\n         Population    AbsoluteLatitude     CranialCapacity Minimum_Illuminance \n                  0                   0                   0                   0 \nMinimum_Temperature   MeanOrbitalVolume \n                  0                   0 \n\n#There is no missing value.\n\n# Simplify the names of the variables\ndataHVS$Pop&lt;-dataHVS$Population\ndataHVS$Lat&lt;-dataHVS$AbsoluteLatitude\ndataHVS$Capacity&lt;-dataHVS$CranialCapacity\ndataHVS$Illumi&lt;-dataHVS$Minimum_Illuminance\ndataHVS$Temp&lt;-dataHVS$Minimum_Temperature\ndataHVS$Response&lt;-dataHVS$MeanOrbitalVolume      \n\n\n\nData exploration\nSee part 1).\n\n\nStatistical analysis\nModel building\nYou need to first analyse the full model containing all the independent variables in order to test their significance (i.e. see if they are relevant in the modelling) and decide whether you keep them or not.\n\n# Model formulation\nmod1&lt;-lm(Response~Lat+Capacity,data=dataHVS)\n# Then we check for significance, here with an F-Test\ndrop1(mod1,test=\"F\")\n\nSingle term deletions\n\nModel:\nResponse ~ Lat + Capacity\n         Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   \n&lt;none&gt;                236.18 86.150                    \nLat       1    48.830 285.01 94.486  10.751 0.001863 **\nCapacity  1    28.079 264.26 90.328   6.182 0.016155 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLook at the test statistic, the F value and its associated p-value (Pr). They show that both covariates are significant. So here, no need to go further, because the full model is the candidate model. To understand how covariates influence the response (the orbital volume), we analyse coefficients of the model (i.e. the \\(\\beta_{j}\\)).\nModel’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Response ~ Lat + Capacity, data = dataHVS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3661 -1.5412  0.0534  1.6091  4.4284 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.406367   3.063999   5.028 6.24e-06 ***\nLat          0.048910   0.014917   3.279  0.00186 ** \nCapacity     0.005836   0.002347   2.486  0.01616 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.131 on 52 degrees of freedom\nMultiple R-squared:  0.3701,    Adjusted R-squared:  0.3459 \nF-statistic: 15.28 on 2 and 52 DF,  p-value: 6.034e-06\n\n\nThis table detailed the coefficients of the model with coefficients associated with each covariate. You can deduce the candidate model: \\[ Orbital\\:Volume = 15.4 \\:+\\: 0.04.Latitude\\: +\\: 0.005.Cranial\\:Capacity  \\] Those coefficients suggest that the orbital volume increases with the latitude (in relation with Temperature and Illuminance) and the cranial capacity.\nModel explanation: R²\nLet’s determine the part of the \\(Y\\) variation explained by your model.\n\n# R² of the model\nsummary(mod1)\n\n\nCall:\nlm(formula = Response ~ Lat + Capacity, data = dataHVS)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3661 -1.5412  0.0534  1.6091  4.4284 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.406367   3.063999   5.028 6.24e-06 ***\nLat          0.048910   0.014917   3.279  0.00186 ** \nCapacity     0.005836   0.002347   2.486  0.01616 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.131 on 52 degrees of freedom\nMultiple R-squared:  0.3701,    Adjusted R-squared:  0.3459 \nF-statistic: 15.28 on 2 and 52 DF,  p-value: 6.034e-06\n\n\nYou can read the adjusted R² = 0.345. That means that about 35% of the variance of the Orbital Volume is explained by its relationship with the Latitude (Temperature & Illuminance) and the Cranial Capacity.\nModel validation: check the assumptions\nSee III/.\nYou must want to have the best visual representation of your model !\n\n# set your variables on each axis\nx &lt;- dataHVS$Capacity\ny &lt;- dataHVS$Lat\nz &lt;- dataHVS$Response\n# Remind the candidate model with these\nmod1&lt;-lm(z~x+y)\n\n# Create a grid from the x and y values (min to max) and predict values for every point: this is the base of the regression plane\ngrid.lines = 40\nx.pred &lt;- seq(min(x), max(x), length.out = grid.lines)\ny.pred &lt;- seq(min(y), max(y), length.out = grid.lines)\nxy &lt;- expand.grid( x = x.pred, y = y.pred)\nz.pred &lt;- matrix(predict(mod1, newdata = xy),nrow = grid.lines, ncol = grid.lines)\n# Create the fitted points for droplines to the surface\nfitpoints &lt;- predict(mod1)\n\n# Scatter plot with regression plane, in the format that you like\nscatter3D(x, y, z, pch = 19, cex = 1,colvar = NULL, col=\"red\", \n          theta = 30, phi = 10, bty=\"b\",\n          xlab = \"Cranial Capacity\", ylab = \"Latitude\", zlab = \"Response\",  \n          surf = list(x = x.pred, y = y.pred, z = z.pred,  \n          facets = TRUE, fit = fitpoints, col=ramp.col (col = c(\"dodgerblue3\",\"seagreen2\"), n = 300, alpha=0.9),           border=\"black\"), main = \" \")\n\n\n\n\nSo ! Does it look cool to you ?"
  },
  {
    "objectID": "linear_modelling.html#binomial-law",
    "href": "linear_modelling.html#binomial-law",
    "title": "Linear modelling in ecology",
    "section": "1) Binomial Law",
    "text": "1) Binomial Law\n\nINTRODUCTION\nX is a binary variable with two modalities: 1 (success) and 0 (failure). The probability of success is: \\[ P(X = 1) = \\pi \\] Then, Y, a variable corresponding to N randoms and independent draws of X, follows a binomial law whose parameters are N and \\(\\pi\\).\nThe density function of Y is: \\[ f(y;\\pi)=\\binom{N}{y}.\\pi^{y}.(1-\\pi)^{(N-y)}\\] The expectancy and the variance of Y are: \\[E(Y)=N.\\pi\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:var(Y)=N.\\pi.(1-\\pi)\\]\nIn the generalized model, the link function for Y is the function “logit” such that: \\[logit(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\] Thus, the predicted value is: \\[\\mu_{y}= \\frac {e^{\\eta}}{1+e^{\\eta}} \\]\n\n\nBINOMIAL LAW EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nLet’s apply this on the example of the dataset “badger.txt” from the book: Zuur et al. 2009 “Mixed effects models and extensions in ecology with R” - Springer. This dataset comes from a survey carried out on 36 farms in South-West England over 8 consecutive seasons running from autumn 2003 to summer 2005. It contains 277 rows and the columns:\n\nyear: Calendar year\nseason: spring, summer, autumn, winter\nfarm_code: farm identifier\nsurvey: which of the 8 survey occasions (i.e. a time indicator)\nbadger_activity: presence-absence of signs of badgers activity\nN_setts_in_fields: number of badger ‘homes’ observed\nN_buildings: number of buildings on farm\nN_cattle_in_buildings: number of cattle housed in the building yard\naccessible_feed_store_present: presence-absence of a feed’s store in farm\naccessible_cattle_house_present:presence-absence of a direct access to cattle house\naccessible_feed_present: presence-absence of accessible feed on farm\ngrass_silage: presence-absence of grass_silage\ncereal_silage: presence-absence of cereal_silage\nhay_straw: presence-absence of hay_straw\ncereal_grains: presence-absence of cereal_grains\nconcentrates: presence-absence of concentrates\nsugar_beet: presence-absence of sugar beet\nmolasses: presence-absence of molasses\n\nFor the example, the binary variable that will be explained is the badger activity and any other variables are taken as explanatory variables. The objective is to find a model that predict the occurrence of signs of badger activity on farms in order to find a way to reduce the rates of badgers’ visits to farms. This objective is motivated by the numerous transmissions of bovine tuberculosis from badgers to cattle.\nLet’s import the dataset and perform a binomial generalized linear model.\n\n\nData exploration\nSee part I/1).\n\n\nStatistical analysis\nModel building\nWe will use a backward selection with the most complete model, considered here, for a question of simplicity, as the model with all the explanatory variables but no interactions.\n\n# Let's define the model with the function \"glm\" with the family \"binomial\" and the link function \"logit\" \nmod1&lt;-glm(Activity~N_setts + N_buildings + N_cattle + season + feed_store + cattle_house + feed + grass + cereal + straw + grains + concen + sugar,data=dataBadger, family=binomial(link=logit))\n\n# We can use then the function drop1 to check the significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nActivity ~ N_setts + N_buildings + N_cattle + season + feed_store + \n    cattle_house + feed + grass + cereal + straw + grains + concen + \n    sugar\n             Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;            180.19 212.19                     \nN_setts       1   214.98 244.98 34.782 3.687e-09 ***\nN_buildings   1   180.32 210.32  0.127   0.72116    \nN_cattle      1   184.41 214.41  4.217   0.04003 *  \nseason        3   181.15 207.15  0.955   0.81222    \nfeed_store    1   183.73 213.73  3.532   0.06019 .  \ncattle_house  1   181.23 211.23  1.033   0.30942    \nfeed          1   182.53 212.53  2.338   0.12629    \ngrass         1   180.22 210.22  0.021   0.88467    \ncereal        1   180.76 210.76  0.565   0.45242    \nstraw         1   181.42 211.42  1.223   0.26869    \ngrains        1   180.33 210.33  0.132   0.71641    \nconcen        1   180.21 210.21  0.019   0.89129    \nsugar         1   181.57 211.57  1.380   0.24011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSome of the coefficient are not significant because their p values are under 0.05 so we will suppress them and test the new model.\n\n\nSingle term deletions\n\nModel:\nActivity ~ N_setts + N_cattle\n         Df Deviance    AIC    LRT Pr(&gt;Chi)    \n&lt;none&gt;        191.84 197.84                    \nN_setts   1   240.81 244.81 48.969  2.6e-12 ***\nN_cattle  1   195.62 199.62  3.779   0.0519 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis time, all of the variables are significant. Let’s check the coefficient :\n\n# Coefficients of the model\nsummary(mod2)\n\n\nCall:\nglm(formula = Activity ~ N_setts + N_cattle, family = binomial(link = logit), \n    data = dataBadger)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.638947   0.408344  -8.911  &lt; 2e-16 ***\nN_setts      0.268288   0.042921   6.251 4.09e-10 ***\nN_cattle     0.003486   0.001766   1.974   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 246.17  on 277  degrees of freedom\nResidual deviance: 191.84  on 275  degrees of freedom\nAIC: 197.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nAll the coefficients are significant.\nThe candidate model is: \\[  logit(Presence\\:of\\:badger\\:activity) = - 3.64 0.27*Number\\:of\\:Setts\\: +\\: 0.004* Number\\:of\\:Cattles \\]\nModel explanation\nHowever there is no \\(R^2\\) in Generalized Linear Models, we can still calculate a $pseudo:R^2* to estimate how far the candidate model is from the null model by determining the distance between deviance of the null model and the residual deviance of the candidate model.\n\n# Estimate of deviance explained\n(mod2$null.deviance-mod2$deviance)/mod2$null.deviance\n\n[1] 0.2207131\n\n# Some others estimates of deviance explained - package 'rcompanion' =&gt; code lines : nagelkerke(mod2)\n\nFrom these code lines, we deduce that the estimate of deviance explained is 22 %. We found about the same thing with the \\(Pseudo\\:R^2\\) estimate (package ‘rcompanion’).\nModel validation: check the assumptions\nSee III/.\nNota Bene : For this example, the validation will show a dependency that could be resolved by using a mixed model with “Farm_code” as random factor."
  },
  {
    "objectID": "linear_modelling.html#poisson-law",
    "href": "linear_modelling.html#poisson-law",
    "title": "Linear modelling in ecology",
    "section": "2) POISSON LAW",
    "text": "2) POISSON LAW\n\nINTRODUCTION\nHere we will focus on count data. This count data is a positive discrete variable. There are two types of distribution for this type of data. It can follow a Poisson or a Negative Binomial law. We will first focus on a Poisson law.\nFor the Poisson law, we have: \\[ E(y)= Var(y)=\\lambda \\]\nWe can write the distribution under a Poisson distribution as follows: \\[Pr(Y=y)=\\frac{e^{-\\lambda}.\\lambda^y}{y!}\\] \\(y\\) permits to count the number of occurrences and \\(\\lambda\\) is the mean and the variance of the Poisson distribution.\nThe link function of the Poisson law in the Generalized Linear model is log and can be written like that: \\[log(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\]\nBy applying the inverse link function to \\(\\eta\\), we obtain the predicted values of Y: \\[\\mu_{y}= e^{\\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}} = e^{\\eta} \\] Let’s take an example to illustrate the use of the Poisson law:\n\n\nPOISSON LAW EXAMPLE\n###Dataset representation and objectives of the analysis\nThe study of Gotelli and Ellison in 2002 is a good example to apply the Poisson law. It is named “Biogeography at a regional scale: determinants of ants species density in New England bogs and forests”. At each of 22 sites, 25 pitfall traps were set in two 8x8m arrays, one in the center of the bog and one in adjacent upland forest 50 to 500m from the corresponding bog. Traps are treated as 50 independent replicate observations. The data are in the file BogAnts.txt.\n\n# Dataset importation \nants&lt;-read.table(\"BogAnts.txt\", dec = \".\", header = TRUE) \nants$Location&lt;-as.factor(ants$Location)\nstr(ants)\n\n'data.frame':   44 obs. of  6 variables:\n $ Site     : chr  \"ARC\" \"ARC\" \"BH\" \"BH\" ...\n $ Latitude : num  42.3 42.3 42.6 42.6 45 ...\n $ Elevation: int  95 95 274 274 133 133 210 210 362 362 ...\n $ Area     : int  1190 1190 105369 105369 38023 38023 73120 73120 38081 38081 ...\n $ Location : Factor w/ 2 levels \"Bog\",\"Forest\": 2 1 2 1 2 1 2 1 2 1 ...\n $ Nsp      : int  9 8 10 8 6 5 9 4 6 2 ...\n\n# Check for presence of missing values\ncolSums(is.na(ants))\n\n     Site  Latitude Elevation      Area  Location       Nsp \n        0         0         0         0         0         0 \n\n# There is no missing value.\n\nWe have 6 variables. The first one gives the site name. Latitude, Area (of the bog) and Elevation are covariates for each site. Location is a qualitative variable (‘Bog’ or ‘Forest’). The response variable is the last one variable (Nsp) which give the number of ant species found in the traps, in other words the ant species richness.\nFor this research we can wonder: which continuous or categorical variables drive the species richness of ants in bogs from New England ?\n\n\nData exploration\nSee I/1).\nHowever, before continuing, we need to transform the variable ‘Area’, as in the exploration we can see a presence of outliers due to presence of very extensive bogs. We will therefore perform a log-transformation of this covariate.\n\npar(mfrow=c(1,3))\n# Bog Area\n# Cleveland plot\ndotchart(ants$Area,pch=16,col='blue',xlab='Bog Area')\n# Histogram\nhist(ants$Area,col='blue',xlab=\"Bog Area\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(ants$Area,pch=16,col='blue',xlab='')\nqqline(ants$Area,col='red')\n\n\n\n\n\npar(mfrow=c(1,3))\n\n# Log-transformation for the 'Area' variable : \nants$LogArea&lt;-log(ants$Area)\n\n# Log Bog Area\n# Cleveland plot\ndotchart(ants$LogArea,pch=16,col='blue',xlab='LogBog Area')\n# Histogram\nhist(ants$LogArea,col='blue',xlab=\"LogBog Area\",main=\"\")\n# Quantile-Quantile plot\nqqnorm(ants$LogArea,pch=16,col='blue',xlab='')\nqqline(ants$LogArea,col='red')\n\n\n\n\nNow we can use the ‘Area’ variable.\n\n\nStatistical analysis\nModel building\nAs every model building, we will search for the candidate model by first analysing the full model with all the independent variables and their interactions. Then a backward selection will be used to select the best model based on term significance. Successively, the non-significant interactions are deleted, and then the non-significant main effects. However, a non-significant main effect is deleted only if it is non-significant AND not contained in a significant interaction.\nHere we perform a Poisson generalized linear model with these code lines:\n\n# The model is : \nmod1&lt;-glm(Nsp~ Location + Latitude + Elevation + LogArea + Location:Latitude + Location:Elevation+ Location:LogArea, data=ants,family=poisson(link=\"log\"))\n# To check the significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Latitude + \n    Location:Elevation + Location:LogArea\n                   Df Deviance    AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;                  39.973 216.33                 \nLocation:Latitude   1   39.981 214.34 0.00801   0.9287\nLocation:Elevation  1   40.370 214.72 0.39675   0.5288\nLocation:LogArea    1   40.075 214.43 0.10247   0.7489\n\n\nHere any interaction is significant. We delete the less significant interaction : Location:Latitude and do this code lines until there are only significant effects.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Elevation + \n    Location:LogArea\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                  39.981 214.34                      \nLatitude            1   55.094 227.45 15.1134 0.0001012 ***\nLocation:Elevation  1   40.426 212.78  0.4446 0.5049344    \nLocation:LogArea    1   40.078 212.43  0.0974 0.7550075    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant interaction is Location:LogArea. So we delete this interaction and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea + Location:Elevation\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                  40.078 212.43                      \nLatitude            1   55.176 225.53 15.0976 0.0001021 ***\nLogArea             1   40.273 210.63  0.1943 0.6593240    \nLocation:Elevation  1   40.499 210.85  0.4208 0.5165445    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant interaction is Location:Elevation. So, we delete this interaction and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation + LogArea\n          Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;         40.499 210.85                      \nLocation   1   70.185 238.54 29.6856 5.081e-08 ***\nLatitude   1   55.621 223.97 15.1221 0.0001008 ***\nElevation  1   49.909 218.26  9.4100 0.0021580 ** \nLogArea    1   40.690 209.04  0.1913 0.6618646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe less significant effect is LogArea. So, we delete this effect and continue.\n\n\nSingle term deletions\n\nModel:\nNsp ~ Location + Latitude + Elevation\n          Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;         40.690 209.04                      \nLocation   1   70.376 236.73 29.6856 5.081e-08 ***\nLatitude   1   56.649 223.00 15.9590 6.473e-05 ***\nElevation  1   50.284 216.64  9.5938  0.001952 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere all the effects are significant. So we can stop the backward selection and conclude with this the selected model: Nsp~Location + Elevation + Latitude\nIt is necessary to analyse the coefficients of the model in order to understand how the main effects influence the ant species richness in bogs.\nModel’s coefficients analysis\n\n# Coefficients of the model\nsummary(mod1)\n\n\nCall:\nglm(formula = Nsp ~ Location + Latitude + Elevation, family = poisson(link = \"log\"), \n    data = ants)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    11.9368121  2.6214970   4.553 5.28e-06 ***\nLocationForest  0.6354389  0.1195664   5.315 1.07e-07 ***\nLatitude       -0.2357930  0.0616638  -3.824 0.000131 ***\nElevation      -0.0011411  0.0003749  -3.044 0.002337 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 102.76  on 43  degrees of freedom\nResidual deviance:  40.69  on 40  degrees of freedom\nAIC: 209.04\n\nNumber of Fisher Scoring iterations: 4\n\n\nSo we can write the model like that: \\[ log(Species\\:Richness) = 11.93 + (Location_{Bog} = 0 ;\\:Location_{Forest} = +0.63^{***})\\:- 0.23^{***}.Latitude\\: -0.001^{***}. Elevation  \\] Be careful here, for a factor, there is a level called “the baseline” which mean that its coefficient is 0. It is the reference level.\nModel explanation\nIn generalized linear models, there is no R2, so we need to calculate a pseudo R2 with the distance between the null model deviance and the residual deviance of the model with this formula: \\[Pseudo\\:R^2=100\\:.\\:\\frac{Null\\:Deviance- Residual\\:Deviance}{Null\\:Deviance}\\]\nLet’s do it with R:\n\n# Pseudo R2 calculation \n(mod1$null.deviance-mod1$deviance)/mod1$null.deviance\n\n[1] 0.6040372\n\n# We can have other pseudo R2 with the package 'rcompanion'\nnagelkerke(mod1)\n\n$Models\n                                                                                  \nModel: \"glm, Nsp ~ Location + Latitude + Elevation, poisson(link = \\\"log\\\"), ants\"\nNull:  \"glm, Nsp ~ 1, poisson(link = \\\"log\\\"), ants\"                              \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                             0.235913\nCox and Snell (ML)                   0.756039\nNagelkerke (Cragg and Uhler)         0.757956\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n      -3     -31.036 62.073 2.1197e-13\n\n$Number.of.observations\n         \nModel: 44\nNull:  44\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\nSo here, the model explains 60.4% of the deviance, but with the others estimate, we found the model explain about 75% of the deviance. Now, it is necessary to check the assumptions of the model to validate. See the part dedicated to this at the end."
  },
  {
    "objectID": "linear_modelling.html#negative-binomial-law",
    "href": "linear_modelling.html#negative-binomial-law",
    "title": "Linear modelling in ecology",
    "section": "3) NEGATIVE BINOMIAL LAW",
    "text": "3) NEGATIVE BINOMIAL LAW\n\nINTRODUCTION\nThe Negative Binomial distribution has been configured in a number of different ways in the statistical literature. Perhaps the most common way to parameter is to see the Negative Binomial distribution arising as a distribution of the number of failures (X) before the r^th success in independent trials, with success probability p in each trial (consequently, r &gt; 0 and 0 &lt; p &gt; 1). In such a case the probability mass function can be expressed as: \\[Pr(X=x|r,p)=\\frac {\\Gamma(x+r)}{x!.\\Gamma(r)}.p^r.(1-p)^x\\]\nand the random variable X has the expectation (theoretical mean): \\[\\mu =\\frac{r.(1 - p)}{p}\\]\nand variance: \\[\\sigma^2 = \\frac{r.(1 – p)}{p^2}\\]\nIn a NB Generalized Linear model, the link function is log so that: \\[log(\\mu_{y})= \\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip} = \\eta \\]\nThe predicted values of Y is obtained by applying the inverse link function to \\(\\eta\\). \\[\\mu_{y}= e^{\\alpha+ \\beta_{1}.X{i1}+ \\beta_{2}.X{i2}+\\beta_{3}.X{i3}+...\\beta_{p}.X{ip}} = e^{\\eta} \\] The negative binomial model has a NB error structure. This error structure allows, among other things, to correctly specify the relationship between the mean and the variance. This relationship is used by the maximum likelihood approach to estimate the coefficients and standard errors of the generalized linear model parameters.\nLet’s take an example to illustrate the use of the Negative binomial law:\n\n\nNEGATIVE BINOMIAL LAW EXAMPLE\n\n\nDataset presentation and objectives of the analysis\nThe study of Timi and Poulin in 2003 is a good example to apply the Negative binomial law. We will use a subset of the original data. At each of 4 stations, fish sample were collected, with a total of 522 individual of anchovy. These fish were examined for parasites in order to understand the parasite community structure across the host population of anchovy. The data are in the file FishParasite.txt The response variable is “Number”, which represents the total number of parasites found in the fish. It is a count variable. There are 3 explicative variables: “Sex” (of the fish), “Length” (of the fish) and “Area”. Sex and Length are continuous, and Area is a categorical variable with 4 categories (A, B, C or D). Here we only consider the interaction between Length and Area.\n\n# Dataset import\npara &lt;- read.table(\"FishParasites.txt\", dec=\".\", header = TRUE)\npara$Area&lt;-as.factor(para$Area)\npara$Sex&lt;-as.factor(para$Sex)\nstr(para)\n\n'data.frame':   521 obs. of  4 variables:\n $ Area  : Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Length: int  88 93 94 94 94 94 94 94 94 96 ...\n $ Sex   : Factor w/ 2 levels \"Female\",\"Male\": 1 2 1 1 1 2 2 2 2 1 ...\n $ Number: int  3 2 8 14 12 4 10 0 2 17 ...\n\n# Missing values ? \ncolSums(is.na(para))\n\n  Area Length    Sex Number \n     0      0      0      0 \n\n# There is no missing value.\n\nLet’s begin quickly with a Poisson model in order to see the problems and then apply a Negative Binomial model.\n\n\nData exploration\nSee part I/1).\n\n\nStatistical analysis\nModel building\nWe perform a backward selection as explained previously, with a Poisson model:\n\n# Model formulation\nmod1&lt;-glm(Number~ Sex + Area + Length + Area:Length ,data=para ,family=poisson(link=\"log\"))\n# Then we check for significance\ndrop1(mod1,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNumber ~ Sex + Area + Length + Area:Length\n            Df Deviance   AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;            10030 12300                     \nSex          1    10066 12334  35.91 2.071e-09 ***\nArea:Length  3    10953 13217 922.60 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod1)\n\n\nCall:\nglm(formula = Number ~ Sex + Area + Length + Area:Length, family = poisson(link = \"log\"), \n    data = para)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.2067538  0.1056462   1.957   0.0503 .  \nSexMale      -0.1041194  0.0173649  -5.996 2.02e-09 ***\nAreaB         1.2976071  0.1447951   8.962  &lt; 2e-16 ***\nAreaC        -2.6680762  0.1814043 -14.708  &lt; 2e-16 ***\nAreaD        -5.4800806  0.2431050 -22.542  &lt; 2e-16 ***\nLength        0.0229176  0.0008601  26.645  &lt; 2e-16 ***\nAreaB:Length -0.0070126  0.0011600  -6.045 1.49e-09 ***\nAreaC:Length  0.0162007  0.0012135  13.350  &lt; 2e-16 ***\nAreaD:Length  0.0316032  0.0015890  19.889  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 19911  on 520  degrees of freedom\nResidual deviance: 10030  on 512  degrees of freedom\nAIC: 12300\n\nNumber of Fisher Scoring iterations: 5\n\n\nHere the interaction between Length and Area is significant, so we keep both variables. And Sex is also significant. So the full model is the candidate model. To see if we can apply the Poisson model, we test the overdispersion. Sometimes, the variance of the response variable may be higher than supposed by the Poisson law. If the parameter is greater than 1.5, we can say that there is overdispersion and the standard errors of the coefficient estimates are biased.\nLet’s check the overdispersion in the model with a DHARMa non parametric dispersion test:\n\n# Overdisperion checking of the Poisson model\n# Scale parameter calculation\nE1 &lt;- resid(mod1, type = \"pearson\") # (Y - mu) / sqrt(mu)\nN  &lt;- nrow(para)\np  &lt;- length(coef(mod1))\nsum(E1^2) / (N - p)\n\n[1] 24.72358\n\n# Use simulations for parameter estimation (package DHARMa)\ntestDispersion(mod1)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 31.727, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe overdispersion index is largely over 1.5, which means that there is an overdispersion in the model. So, the Poisson model can’t be apply and analyse. There are some reasons of overdispersion like for example: the presence of outliers, a dependency, a non linear relationship, the use of the wrong link function…\nWe will change and use a Negative Binomial model and recalculate the overdispersion once we have found the model.\n\n# Model formulation\nmodNB&lt;-glm.nb(Number~ Sex + Area + Length + Area:Length ,data=para)\n# Then we check for significance\ndrop1(modNB,test=\"Chi\")\n\nSingle term deletions\n\nModel:\nNumber ~ Sex + Area + Length + Area:Length\n            Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;           581.89 4182.9                     \nSex          1   582.70 4181.7  0.814    0.3669    \nArea:Length  3   637.58 4232.6 55.698 4.874e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have a significant effect for the interaction between Area and Length but not for the main effect Sex. We need to keep the main factor Area and Length and delete the variable Sex, contrary to the Poisson model.\n\n\nSingle term deletions\n\nModel:\nNumber ~ Area + Length + Area:Length\n            Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;           581.90 4181.7                     \nArea:Length  3   636.77 4230.6 54.873 7.308e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, the interaction is still significant, with the two main effect. So we have found the candidate model. Now we need to test the overdispersion, the same way as before.\n\n\n[1] 1.345759\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.86643, p-value = 0.568\nalternative hypothesis: two.sided\n\n\nWe found 1.35, which is below/under 1.5. So we succeed to delete the overdispersion. The DHARMa test validates the absence of overdispersion.\nModel explanation\nNow we can examine the coefficients of the candidate model in order to understand how the number of parasites in fish is influenced.\n\n# Coefficients of the model\nsummary(modNB)\n\n\nCall:\nglm.nb(formula = Number ~ Area + Length + Area:Length, data = para, \n    init.theta = 1.326438087, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.440440   0.459468  -0.959   0.3378    \nAreaB         1.639419   0.652440   2.513   0.0120 *  \nAreaC        -0.613328   0.825456  -0.743   0.4575    \nAreaD        -5.421868   0.718160  -7.550 4.36e-14 ***\nLength        0.028189   0.004059   6.944 3.80e-12 ***\nAreaB:Length -0.010137   0.005593  -1.812   0.0699 .  \nAreaC:Length  0.002096   0.005866   0.357   0.7209    \nAreaD:Length  0.029865   0.005618   5.316 1.06e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.3264) family taken to be 1)\n\n    Null deviance: 1096.8  on 520  degrees of freedom\nResidual deviance:  581.9  on 513  degrees of freedom\nAIC: 4183.7\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.3264 \n          Std. Err.:  0.0866 \n\n 2 x log-likelihood:  -4165.6880 \n\n\nSo, the candidate model is: \\[ log(Number\\:of\\:Parasites) = -0.44 + (Area_{A} = 0 ;\\:Area_{B} = +1.64;\\:Area_{C} = -0.61;\\:Area_{D}  = -5.42)\\] \\[ +\\: 0.028.Length\\:+ (if\\:Area=B: - 0.010. Length;\\:if\\:Area=C: + 0.002. Length ;\\:if\\:Area=C: + 0.029.Length)  \\]\nBe careful here! As we saw previously, there is always a modality of the factor which is the baseline, and every modality of this factor is compared to the baseline. So if a modality of the factor is significant, it means that there is a difference between this modality and the baseline. So in order to test which modality is different from one another, we need to change the baseline.\nAs in every Generalized Linear Models, there is no R2. So we will calculate the pseudo R2 with the formula: \\[Pseudo\\:R^2=100\\:.\\:\\frac{Null\\:Deviance-Residual\\:Deviance}{Null\\:Deviance}\\]\nLet’s perform it on R:\n\n# Estimate of deviance explained\n(modNB$null.deviance-modNB$deviance)/modNB$null.deviance\n\n[1] 0.4694521\n\n# Some others estimates of deviance explained - package 'rcompanion'\nnagelkerke(modNB)\n\n$Models\n                                                                             \nModel: \"glm.nb, Number ~ Area + Length + Area:Length, para, 1.326438087, log\"\nNull:  \"glm.nb, Number ~ 1, para, 0.689574211, log\"                          \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                            0.0820131\nCox and Snell (ML)                  0.5104780\nNagelkerke (Cragg and Uhler)        0.5105620\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n      -7     -186.08 372.16 2.2094e-76\n\n$Number.of.observations\n          \nModel: 521\nNull:  521\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\nModel validation\nSee III/."
  },
  {
    "objectID": "linear_modelling.html#normality-of-residuals",
    "href": "linear_modelling.html#normality-of-residuals",
    "title": "Linear modelling in ecology",
    "section": "Normality of residuals",
    "text": "Normality of residuals\nThe assumption of normality can be checked by producing an histogram and a quantile plot of the residuals. The histogram of residuals should follow a normal distribution. If the points in the quantile plot lie mostly on the red line, the residuals are normally distributed.\n\npar(mfrow=c(1,2))\n# Histogram example 1\nhist(mod2$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality ex 1\")\n# Quantile-Quantile plot\nqqnorm(mod2$residuals,pch=16,col='blue',xlab='')\nqqline(mod2$residuals,col='red')\n\n\n\n# Histogram example 2\nhist(mod1$residuals,col='blue',xlab=\"residuals\",main=\"Check Normality ex 2\")\n# Quantile-Quantile plot\nqqnorm(mod1$residuals,pch=16,col='blue',xlab='')\nqqline(mod1$residuals,col='red')\n\n\n\n# Histogram example 3\nhist(resid,col='blue',xlab=\"residuals\",main=\"Check Normality ex 3\")\n# Quantile-Quantile plot\nqqnorm(resid,pch=16,col='blue',xlab='')\nqqline(resid,col='red')\n\n\n\n\nAs you can see on the first example, the residuals lie mostly on the red line so we can say the residuals are normally distributed.\nHowever, if a small deviation from normality is detected as on the second example, the Fisher test is considered fairly resistant to these deviations and you can still validate the model.\nIf the residuals don’t follow the red line like on the third example, you can’t conclude residuals are normally distributed and validate the model. Transformation can be done to solve the problem but the best is to make another hypothesis on the law of Y. Linear models lie on the assumption that Y is continuous and its distribution is close to follow a Gaussian law. If Y is semi quantitative, binary, or a proportion, the residuals are probably not going to follow a normal distribution, so you have to make another hypothesis on Y law. Therefore you will need to use a GLM."
  },
  {
    "objectID": "linear_modelling.html#homogeneity-of-variances",
    "href": "linear_modelling.html#homogeneity-of-variances",
    "title": "Linear modelling in ecology",
    "section": "Homogeneity of variances",
    "text": "Homogeneity of variances\nThe assumption of homogeneity of variances states that the variation in the residuals is approximately equal across the range of the predictor variables. It can be checked by plotting the residuals against the fitted values and the residuals against the significant main effects. To conclude that the variance is homogeneous, we need to check that it is stable and does not show any patterns.\n\npar(mfrow=c(2,3))\n# residuals vs fitted\nplot(residuals(mod2)~fitted(mod2)\n      , col='blue'\n      , pch=16, main=\"Graph1\")\nabline(h = 0)\n\n# residuals against Habitat factor\nboxplot(residuals(mod1)~ dataFly$Habitat, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Habitat\",\n          main=\"Graph2\")\n\n# residuals vs fitted\nplot(residuals(mod1)~fitted(mod1)\n      , col='blue'\n      , pch=16, main=\"Graph3\")\nabline(h = 0)\n\n# residuals against Castes factor\nboxplot(resid~ dataBombus$Castes, \n         varwidth = TRUE,\n         ylab = \"Residuals\",\n         xlab = \"Castes\",\n         main=\"Graph4\")\nabline(h = 0)\n\n# residuals vs fitted\nplot(resid~fitted(mod3)\n      , col='blue'\n      , pch=16, main=\"Graph5\")\nabline(h = 0)\n\n\n\n\n\nAs you can see in graph 1, the variation in the residuals is approximately equal across the range of the fitted values and there is no pattern, so we can conclude that the variance is homogeneous.\nThe variance of a qualitative variable is studied with a boxplot. For the second graph, we can see that the variance is similar, so we can conclude that it is homogeneous.\nIn the third graph, we can see that the variance is not stable, as we have a funnel shape. So we can’t conclude that it is homogeneous.\nOn the fourth graph, we can see that the variance is not stable, as the boxplots do not have the same height. It is therefore not possible to conclude that they are homogeneous.\nThe fifth graph shows a linear pattern so we cannot conclude positively on the homogeneity of the variance.\n\nStatistical tests are rather resistant to deviations from homogeneity, and can tolerate deviations of around 16 units. If the variance exceeds this threshold, is not stable or shows a pattern, it can be transformed to solve the problem."
  },
  {
    "objectID": "linear_modelling.html#independence-of-residuals",
    "href": "linear_modelling.html#independence-of-residuals",
    "title": "Linear modelling in ecology",
    "section": "Independence of residuals",
    "text": "Independence of residuals\nANCOVA (like other General Linear Models) lies on the assumption that all replicate measures (and so, residuals) are independent of each other. This issue needs to be considered as the design stage. Given the present design, all flies collected are independent as randomly sampled. This assumption is checked. If data are grouped/dependent in any way, then more complex designs are needed to account for additional factors. During the exploration stage, you can detect spatial dependencies with a Moran test on the response variable or spot time dependencies by plotting the autocorrelogram. Thus, you can include the dependency in your model by using mixed models (cf chapter mixte model). If the dependency is still present in the residual of the model, you can’t use general linear model and need to implement specific time (ARIMA …) and space (Spatial Lag model …) models.\nUnlike General Linear Models, Generalized Linear Models don’t require the conditions of homoscedasticity and normality of residuals, but it does require the independence of residuals. We can also check the presence of influential observations (these are the statistical units that have a too large contribution to the model)."
  }
]